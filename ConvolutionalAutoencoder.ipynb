{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.io\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"CinC2017Data/database.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def adaptive_gain_norm(x, w):\n",
    "    x_mean_sub = np.pad(x - x.mean(), int((w-1)/2), \"reflect\")\n",
    "    window = np.ones(w)\n",
    "    sigma_square = np.convolve(x_mean_sub**2, window, mode=\"valid\")/w\n",
    "    gain = 1/np.sqrt(sigma_square)\n",
    "\n",
    "    return x * gain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset[\"length\"] = dataset[\"data\"].map(lambda arr: arr.shape[-1])\n",
    "dataset[\"data\"] = dataset[\"data\"].map(lambda d: d[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# select only the 30s length records\n",
    "dataset = dataset[dataset[\"length\"] == 9000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset[\"data\"] = dataset[\"data\"].map(lambda d: adaptive_gain_norm(d, 501))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check cuda\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Now define a model\n",
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.conv_section1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Conv1d(16, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)\n",
    "        )\n",
    "\n",
    "        self.conv_section2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Conv1d(16, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)\n",
    "        )\n",
    "\n",
    "        self.conv_section3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "\n",
    "        self.conv_section4 = nn.Sequential(\n",
    "            nn.Conv1d(32, 48, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.Conv1d(48, 48, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(48)\n",
    "        )\n",
    "\n",
    "        self.conv_section5 = nn.Sequential(\n",
    "            nn.Conv1d(48, 64, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 64, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        self.conv_section6 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 64, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        self.conv_section7 = nn.Sequential(\n",
    "            nn.Conv1d(64, 80, 9, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.Conv1d(80, 80, 9, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(80)\n",
    "        )\n",
    "\n",
    "        self.encoder_linear = nn.Linear(5120, 120)\n",
    "        self.decoder_linear = nn.Linear(60, 5120)\n",
    "        self.decoder_batchnorm = nn.BatchNorm1d(5120)\n",
    "\n",
    "        self.transconv_section1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16, 1, 20, padding=9, stride=1),\n",
    "        )\n",
    "\n",
    "        self.transconv_section2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16, 16, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "\n",
    "        self.transconv_section3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32, 16, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "\n",
    "        self.transconv_section4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(48, 32, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "        )\n",
    "\n",
    "        self.transconv_section5 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 48, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(48),\n",
    "        )\n",
    "\n",
    "        self.transconv_section6 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 64, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        self.transconv_section7 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(80, 64, 10, padding=4, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # [1, 2048]\n",
    "        x = self.conv_section1(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [16, 1024]\n",
    "        x = self.conv_section2(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [32, 512]\n",
    "        x = self.conv_section3(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [48, 256]\n",
    "        x = self.conv_section4(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [64, 128]\n",
    "        x = self.conv_section5(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [64, 64]\n",
    "        x = self.conv_section6(x)\n",
    "\n",
    "        # [64, 64]\n",
    "        x = self.conv_section7(x)\n",
    "\n",
    "        # [80, 64]\n",
    "        x = torch.flatten(x, -2)\n",
    "\n",
    "        # [5120]\n",
    "        x = self.encoder_linear(x)\n",
    "\n",
    "        # Sample from the latent distribution\n",
    "        z = torch.normal(x[:, :60], torch.abs(x[:, 60:]))\n",
    "\n",
    "        # [60]\n",
    "        z = self.decoder_linear(z)\n",
    "        z = self.decoder_batchnorm(z)\n",
    "        z = torch.nn.functional.relu(z)\n",
    "\n",
    "        # [5120]\n",
    "        z = torch.reshape(z, (-1, 80, 64))\n",
    "        # [80, 64]\n",
    "        z = self.transconv_section7(z)\n",
    "        print(z.shape)\n",
    "        # [64, 64]\n",
    "        z = self.transconv_section6(z)\n",
    "        print(z.shape)\n",
    "        # [64, 64]\n",
    "        z = self.transconv_section5(z)\n",
    "        print(z.shape)\n",
    "        # [64, 128]\n",
    "        z = self.transconv_section4(z)\n",
    "        print(z.shape)\n",
    "        # [48, 256]\n",
    "        z = self.transconv_section3(z)\n",
    "        print(z.shape)\n",
    "        # [32, 512]\n",
    "        z = self.transconv_section2(z)\n",
    "        print(z.shape)\n",
    "        # [16, 1024]\n",
    "        z = self.transconv_section1(z)\n",
    "        print(z.shape)\n",
    "        # [1, 2048]\n",
    "\n",
    "        return z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Onehot encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def generate_onehot(c):\n",
    "    if c == \"N\":\n",
    "        return np.array([1, 0, 0, 0])\n",
    "    if c == \"O\":\n",
    "        return np.array([0, 1, 0, 0])\n",
    "    if c == \"A\":\n",
    "        return np.array([0, 0, 1, 0])\n",
    "    if c == \"~\":\n",
    "        return np.array([0, 0, 0, 1])\n",
    "\n",
    "def generate_index(c):\n",
    "    if c == \"N\":\n",
    "        return 0\n",
    "    if c == \"O\":\n",
    "        return 0\n",
    "    if c == \"A\":\n",
    "        return 0\n",
    "    if c == \"~\":\n",
    "        return 1\n",
    "\n",
    "# dataset[\"onehot\"] = dataset[\"class\"].map(generate_onehot)\n",
    "dataset[\"class_index\"] = dataset[\"class\"].map(generate_index)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"data\"]\n",
    "        y = row[\"class_index\"]\n",
    "\n",
    "        return X, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A00941    4.736952e-18\n",
      "A08370   -1.184238e-17\n",
      "A05125   -7.500173e-18\n",
      "A03387   -1.894781e-17\n",
      "A06901    1.026340e-17\n",
      "              ...     \n",
      "A07345    3.157968e-18\n",
      "A06524   -3.157968e-17\n",
      "A03281    6.315935e-18\n",
      "A00856   -1.736882e-17\n",
      "A07043    3.947460e-18\n",
      "Name: data, Length: 5080, dtype: float64\n",
      "A00941    1.0\n",
      "A08370    1.0\n",
      "A05125    1.0\n",
      "A03387    1.0\n",
      "A06901    1.0\n",
      "         ... \n",
      "A07345    1.0\n",
      "A06524    1.0\n",
      "A03281    1.0\n",
      "A00856    1.0\n",
      "A07043    1.0\n",
      "Name: data, Length: 5080, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.15, stratify=dataset[\"class_index\"])\n",
    "\n",
    "# Normalise the data\n",
    "train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "print(train_dataset[\"data\"].map(lambda x: x.mean()))\n",
    "print(train_dataset[\"data\"].map(lambda x: x.std()))\n",
    "\n",
    "def split_to_segments(dataset, new_len, orig_len, overlap=0):\n",
    "    sections = []\n",
    "\n",
    "    num_sections = orig_len // new_len\n",
    "    for _, series in dataset.iterrows():\n",
    "        for i in range(num_sections):\n",
    "            data = series[\"data\"][i*new_len: (i+1)*new_len]\n",
    "            label = series[\"class_index\"]\n",
    "            sections.append({\"data\": data, \"class_index\": label})\n",
    "\n",
    "    return pd.DataFrame(sections)\n",
    "\n",
    "split_to_segments(test_dataset, 2048, 9000, 0)\n",
    "\n",
    "torch_dataset_train = Dataset(split_to_segments(train_dataset, 2048, 9000, 0))\n",
    "torch_dataset_test = Dataset(split_to_segments(test_dataset, 2048, 9000, 0))\n",
    "\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cuda\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = CVAE().to(device)\n",
    "\n",
    "# Use weightings to avoid\n",
    "\n",
    "# class_counts = torch.tensor(dataset[\"class_index\"].value_counts().values.astype(np.float32))\n",
    "# class_weights = torch.nn.functional.normalize(1.0/class_counts, dim=0)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.8)\n",
    "num_batches = len(train_dataloader)\n",
    "num_test_batches = len(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0 ...\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([32, 64, 130])\n",
      "torch.Size([32, 48, 260])\n",
      "torch.Size([32, 32, 520])\n",
      "torch.Size([32, 16, 1040])\n",
      "torch.Size([32, 16, 2080])\n",
      "torch.Size([32, 1, 2081])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([32, 1, 2048])) that is different to the input size (torch.Size([32, 1, 2081])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2081) must match the size of tensor b (2048) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [45], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     15\u001B[0m output \u001B[38;5;241m=\u001B[39m model(signals)\n\u001B[1;32m---> 16\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(output, signals)\n\u001B[0;32m     17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    529\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3279\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3277\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3279\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3280\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:73\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[1;34m(*tensors)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[1;32m---> 73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (2081) must match the size of tensor b (2048) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"starting epoch {epoch} ...\")\n",
    "    # Train\n",
    "    model.train()\n",
    "    for i, (signals, _) in enumerate(train_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(signals)\n",
    "        loss = loss_func(output, signals)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "    print(\"Testing ...\")\n",
    "    # Test\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (signals, _) in enumerate(test_dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "            # fft = torch.abs(torch.fft.fft(signals))\n",
    "            # signals = torch.cat([signals, fft], dim=1)\n",
    "            # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "            output = model(signals)\n",
    "            loss = loss_func(output, signals)\n",
    "            test_loss += float(loss)\n",
    "\n",
    "    print(f\"Average test loss: {test_loss/num_test_batches}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
