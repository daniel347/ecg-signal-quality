{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import DataHandlers.CinCDataset as CinCDataset\n",
    "import DataHandlers.SAFERDataset as SAFERDataset\n",
    "\n",
    "import importlib\n",
    "importlib.reload(SAFERDataset)\n",
    "importlib.reload(CinCDataset)\n",
    "\n",
    "from DataHandlers.DiagEnum import DiagEnum, feas1DiagToEnum\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"text.usetex\"] = False\n",
    "\n",
    "# A fudge because I moved the files\n",
    "sys.modules[\"SAFERDataset\"] = SAFERDataset\n",
    "sys.modules[\"CinCDataset\"] = CinCDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "feas2_pt_data, feas2_ecg_data = SAFERDataset.load_feas_dataset(2, \"dataframe\")\n",
    "feas2_ecg_data = feas2_ecg_data[feas2_ecg_data[\"length\"] == 9120]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas1_pt_data, feas1_ecg_data = SAFERDataset.load_feas_dataset(1, ecg_meas_diag=[d for d in DiagEnum if d != DiagEnum.Undecided])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load specially cleaned data\n",
    "\n",
    "feas2_ecg_data = pd.read_pickle(r\"C:\\Users\\daniel\\Documents\\2022_23_DSiromani\\Feas2\\ECGs\\filtered_dataframe.pk\")\n",
    "feas2_pt_data = pd.read_csv(r\"C:\\Users\\daniel\\Documents\\2022_23_DSiromani\\Feas2\\pt_data_anon.csv\")\n",
    "feas2_pt_data[\"ptID\"] += 10000\n",
    "feas2_ecg_data[\"ptID\"] += 10000\n",
    "\n",
    "feas1_ecg_data_clean = pd.read_pickle(r\"C:\\Users\\daniel\\Documents\\2022_23_DSiromani\\Feas1\\ECGs\\clean_ecg_dataset.pk\")\n",
    "feas1_pt_data = pd.read_csv(r\"C:\\Users\\daniel\\Documents\\2022_23_DSiromani\\Feas1\\pt_data_anon.csv\")\n",
    "print(len(feas1_ecg_data_clean.index))\n",
    "\n",
    "feas2_ecg_data_clean = pd.read_pickle(r\"C:\\Users\\daniel\\Documents\\2022_23_DSiromani\\Feas2\\ECGs\\clean_ecg_dataset.pk\")\n",
    "feas2_ecg_data_clean[\"ptID\"] += 10000\n",
    "print(len(feas2_ecg_data_clean.index))\n",
    "\n",
    "all_clean_data = pd.concat([feas2_ecg_data_clean, feas1_ecg_data_clean], ignore_index=True)\n",
    "all_clean_pt = pd.concat([feas2_pt_data[feas2_pt_data[\"ptID\"].isin(feas2_ecg_data_clean[\"ptID\"])], feas1_pt_data[feas1_pt_data[\"ptID\"].isin(feas1_ecg_data_clean[\"ptID\"])]])\n",
    "\n",
    "all_clean_pt.set_index(\"ptID\", drop=False, inplace=True)\n",
    "all_clean_pt[\"noRecs\"] = all_clean_data[\"ptID\"].value_counts()\n",
    "all_clean_pt[\"noHQrecs\"] = all_clean_pt[\"noRecs\"]\n",
    "all_clean_pt.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.conv_section1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Conv1d(16, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)\n",
    "        )\n",
    "\n",
    "        self.conv_section2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Conv1d(16, 16, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)\n",
    "        )\n",
    "\n",
    "        self.conv_section3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, 9, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, 9, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "\n",
    "        self.conv_section4 = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, 9, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, 9, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "\n",
    "        self.conv_section5 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, 7, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 64, 7, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        self.conv_section6 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 64, 19, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        self.conv_section7 = nn.Sequential(\n",
    "            nn.Conv1d(64, 80, 7, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.Conv1d(80, 80, 7, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(80)\n",
    "        )\n",
    "\n",
    "        self.encoder_linear = nn.Linear(5120, z_dim*2)\n",
    "        self.decoder_linear = nn.Linear(z_dim, 5120)\n",
    "\n",
    "        self.decoder_batchnorm = nn.BatchNorm1d(5120)\n",
    "\n",
    "        self.transconv_section1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16, 1, 19, padding=9, stride=1),\n",
    "        )\n",
    "\n",
    "        self.transconv_section2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16, 16, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "\n",
    "        self.transconv_section3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32, 16, 20, padding=9, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "\n",
    "        self.transconv_section4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(48, 32, 10, padding=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "        )\n",
    "\n",
    "        self.transconv_section5 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 48, 10, padding=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(48),\n",
    "        )\n",
    "\n",
    "        self.transconv_section6 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 64, 8, padding=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        self.transconv_section7 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(80, 64, 7, padding=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # [1, 2048]\n",
    "        x = self.conv_section1(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [16, 1024]\n",
    "        x = self.conv_section2(x) + x\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [32, 512]\n",
    "        x = self.conv_section3(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [32, 256]\n",
    "        x = self.conv_section4(x) + x\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [64, 128]\n",
    "        x = self.conv_section5(x)\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "\n",
    "        # [64, 64]\n",
    "        # x = self.conv_section6(x)\n",
    "\n",
    "        # [64, 64]\n",
    "        x = self.conv_section7(x)\n",
    "\n",
    "        # [80, 64]\n",
    "        x = torch.flatten(x, -2)\n",
    "\n",
    "        # [5120]\n",
    "        x = self.encoder_linear(x)\n",
    "\n",
    "        # Apply exp activation function to variances\n",
    "        x[:, self.z_dim:] = torch.exp(x[:, self.z_dim:])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        # [z_dim]\n",
    "        z = self.decoder_linear(z)\n",
    "        z = self.decoder_batchnorm(z)\n",
    "        z = torch.nn.functional.relu(z)\n",
    "\n",
    "        # [5120]\n",
    "        z = torch.reshape(z, (-1, 80, 64))\n",
    "        # [80, 64]\n",
    "        z = self.transconv_section7(z)\n",
    "        # print(z.shape)\n",
    "        # [64, 64]\n",
    "        z = self.transconv_section6(z)\n",
    "        # print(z.shape)\n",
    "        # [64, 64]\n",
    "        z = self.transconv_section5(z)\n",
    "        # print(z.shape)\n",
    "        # [64, 128]\n",
    "        z = self.transconv_section4(z)\n",
    "        # print(z.shape)\n",
    "        # [48, 256]\n",
    "        z = self.transconv_section3(z)\n",
    "        # print(z.shape)\n",
    "        # [32, 512]\n",
    "        z = self.transconv_section2(z)\n",
    "        # print(z.shape)\n",
    "        # [16, 1024]\n",
    "        z = self.transconv_section1(z)\n",
    "        # print(z.shape)\n",
    "        # [1, 2048]\n",
    "        return z\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        z_dist = self.encode(x)\n",
    "\n",
    "        z = torch.randn((batch_size, self.z_dim)).to(device) * torch.abs(z_dist[:, self.z_dim:]) + z_dist[:, :self.z_dim]\n",
    "\n",
    "        # x = self.decode(z)\n",
    "\n",
    "        return z_dist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Onehot encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"data\"]\n",
    "        y = row[\"class_index\"]\n",
    "        ind = row.name\n",
    "\n",
    "        return X, y, ind\n",
    "\n",
    "class IDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        ids = pd.unique(self.dataset[\"ptID\"])\n",
    "        for id in ids:\n",
    "            num_samples = pd.value_counts(self.dataset[\"ptID\"] == id)[True]\n",
    "            pos = self.dataset[self.dataset[\"ptID\"] == id].sample(num_samples)\n",
    "            neg = self.dataset[self.dataset[\"ptID\"] != id].sample(num_samples)\n",
    "\n",
    "            self.dataset.loc[self.dataset[\"ptID\"] == id, \"pos_sample\"] = pos.index.values\n",
    "            self.dataset.loc[self.dataset[\"ptID\"] == id, \"neg_sample\"] = neg.index.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        anchor = self.dataset.iloc[index]\n",
    "        Xa = anchor[\"data\"]\n",
    "        ya = anchor[\"ptID\"]\n",
    "        ind = anchor.name\n",
    "\n",
    "        pos = self.dataset.loc[anchor[\"pos_sample\"]]\n",
    "        Xp = pos[\"data\"]\n",
    "\n",
    "        neg = self.dataset.loc[anchor[\"neg_sample\"]]\n",
    "        Xn = neg[\"data\"]\n",
    "        yn = neg[\"ptID\"]\n",
    "\n",
    "        return Xa, Xp, Xn, ya, yn, ind"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def split_to_segments(dataset, new_len, orig_len, overlap=0):\n",
    "    sections = []\n",
    "\n",
    "    step = int(round(new_len * (1 - overlap)))\n",
    "    num_sections = (orig_len - (new_len - step)) // step\n",
    "    for _, series in dataset.iterrows():\n",
    "        for i in range(num_sections):\n",
    "            section_series = series.copy()\n",
    "            section_series[\"data\"] = section_series[\"data\"][i*step: i*step + new_len]\n",
    "            section_series[\"rec_ind\"] = series.name\n",
    "            section_series[\"rec_pos\"] = i\n",
    "            # Keep all other data (ptid, measDiag etc the same for each section as the source ECG)\n",
    "            sections.append(section_series)\n",
    "\n",
    "    return pd.DataFrame(sections).reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test high quality: 4650 low quality: 101 \n",
      "Train high quality: 18029 low quality: 479 \n",
      "Empty DataFrame\n",
      "Columns: [index]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3799\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [112], line 50\u001B[0m\n\u001B[0;32m     45\u001B[0m         test_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(torch_dataset_test, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m train_dataloader, test_dataloader, train_dataset, test_dataset\n\u001B[1;32m---> 50\u001B[0m train_dataloader, test_dataloader, train_dataset, test_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mmake_SAFER_dataloaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeas2_pt_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeas2_ecg_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_clean_training\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [112], line 32\u001B[0m, in \u001B[0;36mmake_SAFER_dataloaders\u001B[1;34m(pt_data, ecg_data, test_frac, only_clean_training)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_dataset\u001B[38;5;241m.\u001B[39mhead())\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Normalise\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m train_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (\u001B[43mtrain_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m-\u001B[39m train_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mmean()))\u001B[38;5;241m/\u001B[39mtrain_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mstd())\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m only_clean_training:\n\u001B[0;32m     35\u001B[0m     torch_dataset_train \u001B[38;5;241m=\u001B[39m Dataset(train_dataset[train_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3804\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3805\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3807\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3804\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, only_clean_training=True):\n",
    "    pt_data[\"noLQrecs\"] = pt_data[\"noRecs\"] - pt_data[\"noHQrecs\"]  # for Feas1 this might include stuff flagged by zenicor as noisy?\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "\n",
    "    for val, df in pt_data.groupby(\"noLQrecs\"):\n",
    "        # print(f\"processing {val}\")\n",
    "        # print(f\"number of patients {len(df.index)}\")\n",
    "        test = df.sample(frac=test_frac)\n",
    "        test_patients.append(test)\n",
    "        train_patients.append(df[~df[\"ptID\"].isin(test[\"ptID\"])])\n",
    "\n",
    "    train_pt_df = pd.concat(train_patients)\n",
    "    test_pt_df = pd.concat(test_patients)\n",
    "\n",
    "    print(f\"Test high quality: {test_pt_df['noHQrecs'].sum()} low quality: {test_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Train high quality: {train_pt_df['noHQrecs'].sum()} low quality: {train_pt_df['noLQrecs'].sum()} \")\n",
    "\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        # get ECG datasets\n",
    "        train_dataset = split_to_segments(ecg_data[ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])], 2048, 9120, 0.5)\n",
    "        # Normalise\n",
    "        train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "        if only_clean_training:\n",
    "            torch_dataset_train = Dataset(train_dataset[train_dataset[\"class_index\"] == 0])\n",
    "        else:\n",
    "            torch_dataset_train = Dataset(train_dataset)\n",
    "\n",
    "        train_dataloader = DataLoader(torch_dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = split_to_segments(ecg_data[ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])], 2048, 9120, 0.5)\n",
    "        test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_test = Dataset(test_dataset)\n",
    "        test_dataloader = DataLoader(torch_dataset_test, batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, train_dataset, test_dataset\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, train_dataset, test_dataset = make_SAFER_dataloaders(feas2_pt_data, feas2_ecg_data, test_frac=0.2, only_clean_training=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_safer_id_dataloaders(pt_data, ecg_data, test_frac, only_clean_training=True):\n",
    "    test_patients = pt_data.sample(frac=0.2)\n",
    "\n",
    "    if only_clean_training:\n",
    "        ecg_segments = split_to_segments(ecg_data[(ecg_data[\"class_index\"] == 0) & (~ecg_data[\"ptID\"].isin(test_patients[\"ptID\"]))], 2048, 9120, 0.5)\n",
    "    else:\n",
    "        ecg_segments = split_to_segments(ecg_data[(~ecg_data[\"ptID\"].isin(test_patients[\"ptID\"]))], 2048, 9120, 0.5)\n",
    "\n",
    "    train_dataset, test_dataset = train_test_split(ecg_segments, stratify=ecg_segments[\"ptID\"], test_size=test_frac)\n",
    "\n",
    "    if not train_dataset.empty:\n",
    "        train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_train = IDDataset(train_dataset)\n",
    "        train_dataloader = DataLoader(torch_dataset_train, batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not test_dataset.empty:\n",
    "        test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_test = IDDataset(test_dataset)\n",
    "        test_dataloader = DataLoader(torch_dataset_test, batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, train_dataset, test_dataset\n",
    "\n",
    "train_dataloader, test_dataloader, train_dataset, test_dataset = make_safer_id_dataloaders(feas2_pt_data, feas2_ecg_data, 0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If we want noisy and clean test_data for evaluation, after training and testing on only clean data in training loop\n",
    "_, noisy_test_dataloader, _, noisy_test_dataset = make_SAFER_dataloaders(feas2_pt_data, feas2_ecg_data[~feas2_ecg_data[\"measID\"].isin(train_dataset[\"measID\"])], test_frac=1, only_clean_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup dataloaders for only the clean data\n",
    "train_dataloader, test_dataloader, train_dataset, test_dataset = make_SAFER_dataloaders(all_clean_pt, all_clean_data, test_frac=0.2, only_clean_training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up the losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    return torch.sqrt(torch.sum((x - y) ** 2, dim=-1))\n",
    "\n",
    "triplet_margin = 0.1\n",
    "\n",
    "def triplet_latent_loss(za, zp, zn):\n",
    "    print(za)\n",
    "    print(zp)\n",
    "    print(zn)\n",
    "    loss = dist(za, zp) - dist(za, zn) + triplet_margin\n",
    "    print(loss)\n",
    "    return torch.mean(torch.where(loss > 0, loss, 0))\n",
    "\n",
    "def kl_latent_loss(z_mean, z_std):\n",
    "    # The regularization loss based on kl divergence of the latent distribution from N(0, 1)\n",
    "    vars = z_std ** 2\n",
    "    means = z_mean\n",
    "\n",
    "    return 1/50 * torch.mean( - torch.log(vars) + vars + means ** 2 - 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "z_dim = 128\n",
    "\n",
    "model = CVAE(z_dim).to(device)\n",
    "\n",
    "loss_func = lambda z: kl_latent_loss(z[:, :z_dim], z[:, z_dim:])\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.8)\n",
    "num_batches = len(train_dataloader)\n",
    "num_test_batches = len(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train on feas 1 and 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# warning: changing these chunk sizes may reload feas1 data from scratch, which will take ages\n",
    "chunk_size = 20000\n",
    "num_chunks = math.ceil(162515 / chunk_size )\n",
    "\n",
    "def get_feas1_dataloader(chunk_num):\n",
    "    feas1_pt_data, feas1_ecg_data = SAFERDataset.load_feas_dataset(1, f\"dataframe_{chunk_num}.pk\", ecg_range=[chunk_size * chunk_num, chunk_size * (chunk_num + 1)])\n",
    "    train_dataset = split_to_segments(feas1_ecg_data, 2048, 9120, 0.5)\n",
    "    train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "    torch_dataset_train = Dataset(train_dataset)\n",
    "    train_dataloader = DataLoader(torch_dataset_train, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0 ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [82], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[0;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ds_ind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mnum_chunks\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining on dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mds_ind\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ds_ind \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'num_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "import copy\n",
    "\n",
    "best_test_loss = 100\n",
    "best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"starting epoch {epoch} ...\")\n",
    "    # Train\n",
    "    model.train()\n",
    "\n",
    "    for ds_ind in range(num_chunks + 1):\n",
    "        print(f\"training on dataset: {ds_ind}\")\n",
    "        if ds_ind == 0:\n",
    "            train_dataloader_part = train_dataloader\n",
    "        else:\n",
    "            train_dataloader_part = get_feas1_dataloader(ds_ind-1)\n",
    "\n",
    "        for i, (signals, _, _) in enumerate(train_dataloader_part):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, latents = model(signals)\n",
    "            loss = loss_func(output, signals, latents)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss)\n",
    "\n",
    "        print(f\"Total loss {total_loss/num_batches}\")\n",
    "\n",
    "    print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "    print(\"Testing ...\")\n",
    "    # Test\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (signals, _, _)in enumerate(test_dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "\n",
    "            output, latents = model (signals)\n",
    "            loss = loss_func(output, signals, latents)\n",
    "            test_loss += float(loss)\n",
    "\n",
    "    print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "\n",
    "    if test_loss/num_test_batches < best_test_loss:\n",
    "        best_model = copy.deepcopy(model).cpu()\n",
    "        best_test_loss = test_loss/num_test_batches\n",
    "\n",
    "model = best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train only using feas2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0 ...\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 2.2585e-01,  2.9277e-01, -6.2979e-02,  ...,  1.3004e+00,\n",
      "          3.5853e+00,  2.3914e-01],\n",
      "        [ 6.5220e-01,  9.3021e-02, -3.6025e-01,  ...,  1.3581e+00,\n",
      "          1.0197e+00,  1.3384e+00],\n",
      "        [ 3.4962e-01,  1.6490e-01, -6.1851e-01,  ...,  4.4539e-01,\n",
      "          9.0657e-01,  7.2466e-01],\n",
      "        ...,\n",
      "        [-4.3428e-01,  1.5238e-01,  7.6054e-01,  ...,  1.3600e+00,\n",
      "          6.4126e-01,  4.0571e-01],\n",
      "        [ 4.0193e-01,  4.8054e-01,  3.0405e-01,  ...,  5.2683e-01,\n",
      "          1.3961e-01,  1.9934e+00],\n",
      "        [-2.0811e-03, -6.6920e-01,  1.1409e+00,  ...,  4.7770e-01,\n",
      "          5.6657e-01,  1.1697e-01]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 0.2259,  0.2928, -0.0630,  ..., -0.0912, -0.1720, -0.3667],\n",
      "        [ 0.6522,  0.0930, -0.3603,  ...,  0.4173,  0.3177, -0.0026],\n",
      "        [ 0.3496,  0.1649, -0.6185,  ..., -0.5375,  0.0275,  0.4648],\n",
      "        ...,\n",
      "        [ 0.6566,  0.1543,  0.1675,  ...,  0.0217, -0.8543,  0.2211],\n",
      "        [-0.4797, -0.1501,  0.3954,  ..., -0.0503,  0.0623, -0.0750],\n",
      "        [-0.2669,  0.7348, -0.3160,  ..., -0.4943, -0.0123, -0.8327]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-1.6576,  0.5115, -0.0928,  ...,  0.0455,  0.4200, -0.4408],\n",
      "        [-0.1375, -0.4605, -0.4452,  ..., -0.3219, -0.1066,  0.0328],\n",
      "        [ 0.1057,  0.0048,  0.5826,  ...,  0.5816, -0.2820, -0.1267],\n",
      "        ...,\n",
      "        [ 0.1833, -0.2231,  0.2645,  ...,  1.2518, -0.0566,  0.0573],\n",
      "        [ 0.8686,  0.7116, -0.2690,  ...,  0.1566,  1.0911, -0.0323],\n",
      "        [-0.4624, -0.5942,  0.5553,  ...,  0.3291,  0.5229, -0.3900]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.2092, -0.3368,  0.1050,  ..., -0.7140,  0.3618,  0.1504],\n",
      "        [ 0.5228,  0.1402, -1.1664,  ...,  0.0029, -0.0055, -0.2442],\n",
      "        [ 0.0364,  0.3243,  0.4427,  ...,  0.8275, -0.5458,  0.7614],\n",
      "        ...,\n",
      "        [-0.4343,  0.1524,  0.7605,  ...,  0.5808,  0.0926,  0.0588],\n",
      "        [ 0.4019,  0.4805,  0.3041,  ..., -0.7890,  0.0988, -0.6451],\n",
      "        [-0.0021, -0.6692,  1.1409,  ...,  0.3305,  0.1061, -0.3572]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.8832, -1.3671,  1.6153, -1.1243,  0.2832,  0.5358,  0.2534,  0.5574,\n",
      "        -1.1860, -4.0462, -0.2207,  2.0177, -0.6593, -1.2500, -0.0419, -0.1473,\n",
      "         1.0076, -1.0004, -0.2034, -0.0571, -1.7381, -0.3042,  1.2945,  0.1396,\n",
      "         0.0301, -0.2198, -0.8341,  0.3901,  0.6173,  0.4842, -0.9371, -0.1166,\n",
      "         0.5732,  0.2051, -0.2085,  0.7192, -2.7398,  2.0410,  0.4961,  1.5044,\n",
      "         0.8188,  0.6536,  0.3227,  1.4282,  0.3880, -1.1242,  0.4312,  0.0774,\n",
      "        -1.1504,  1.0239,  0.9076, -0.2875,  0.6278,  0.2106,  1.3127,  1.0261,\n",
      "        -0.5521, -0.7274, -1.0277,  0.0621, -0.2548,  0.0932,  0.6192, -0.6126],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.027, triplet_loss: 0.416\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 0.0722, -0.2581,  0.5856,  ...,  0.6679,  0.8216,  0.3952],\n",
      "        [-0.6950, -1.2112,  0.1714,  ...,  1.0459,  1.6260,  1.0971],\n",
      "        [-0.3438,  0.5801,  0.1277,  ...,  1.2887,  1.7946,  0.3410],\n",
      "        ...,\n",
      "        [ 0.3201,  0.2646,  0.5750,  ...,  0.1471,  0.7531,  1.6792],\n",
      "        [-0.0345, -0.2225, -0.6442,  ...,  0.8953,  0.8279,  1.0262],\n",
      "        [-0.8054,  0.4004, -0.3816,  ...,  1.4538,  1.8621,  1.7759]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0722, -0.2581,  0.5856,  ...,  0.2915, -0.0576,  0.7992],\n",
      "        [-0.6950, -1.2112,  0.1714,  ..., -0.1029,  0.4549, -0.8793],\n",
      "        [-0.3438,  0.5801,  0.1277,  ..., -0.2832,  0.8233,  0.4414],\n",
      "        ...,\n",
      "        [-0.4231, -0.7562,  0.5431,  ...,  0.9475,  0.1783, -0.2922],\n",
      "        [-0.2699, -0.6492, -0.2251,  ...,  0.3584,  0.3467,  0.5011],\n",
      "        [-0.3091,  0.2559, -0.0480,  ..., -0.0173, -0.2495, -0.0262]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-1.2359,  0.6504,  0.9486,  ...,  0.0042,  0.0039,  0.9662],\n",
      "        [-0.0700,  0.5270,  0.0265,  ...,  0.9143,  0.1084, -0.0275],\n",
      "        [-0.2897,  0.4403,  0.3129,  ..., -0.4976,  0.2845,  0.1677],\n",
      "        ...,\n",
      "        [-1.0337, -0.3178, -0.0380,  ...,  1.3377,  0.4615, -0.2482],\n",
      "        [ 0.1226,  0.6052,  0.2620,  ..., -0.2182, -1.4829, -1.6838],\n",
      "        [ 0.0134,  1.2029,  0.3929,  ..., -0.9288,  0.0924, -0.0487]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5498,  0.6710, -0.3678,  ...,  0.4391,  0.5542,  0.7940],\n",
      "        [-0.2409, -0.3749,  0.4539,  ...,  0.6549, -0.1862,  0.1583],\n",
      "        [ 0.4461, -0.3626, -0.8626,  ...,  0.0726,  0.4578, -0.0494],\n",
      "        ...,\n",
      "        [ 0.3201,  0.2646,  0.5750,  ...,  1.9203,  0.0493,  0.1288],\n",
      "        [-0.0345, -0.2225, -0.6442,  ..., -0.4136, -0.3283, -0.2049],\n",
      "        [-0.8054,  0.4004, -0.3816,  ..., -0.7925,  0.3100, -0.4795]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 0.1052, -1.6996,  1.9331,  1.7125, -1.2060, -1.1648,  0.5491, -0.5838,\n",
      "        -0.1455, -0.5590, -0.3049,  1.2834, -1.5369, -0.4753,  0.1946, -0.9812,\n",
      "        -0.6577,  1.4461,  0.0204, -1.2807,  0.3395, -0.1666,  0.3170, -1.1547,\n",
      "         1.0113, -0.6648, -0.2087, -0.5843, -2.1622,  0.0201, -0.2983,  0.1603,\n",
      "        -0.1833, -0.9039, -0.4889, -1.4154,  1.3574, -0.7740, -0.1540, -0.4078,\n",
      "         0.2112, -0.3473, -0.1140, -0.5453, -0.1092,  0.3741, -0.3537,  0.5953,\n",
      "        -0.2903, -0.2858,  1.3449, -0.5804,  0.5849,  1.2329,  0.5836, -1.1392,\n",
      "         0.5515,  0.3569,  0.2639,  0.0635, -0.3592,  0.0407,  1.1580, -0.3062],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.025, triplet_loss: 0.278\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 1.3220,  0.2921,  0.1475,  ...,  0.5012,  0.1975,  0.6742],\n",
      "        [ 0.1206,  0.7780, -0.7014,  ...,  1.0596,  1.2804,  1.6498],\n",
      "        [-0.5139, -0.2195,  0.2276,  ...,  0.9508,  0.5128,  0.9607],\n",
      "        ...,\n",
      "        [ 0.1715,  0.0940,  0.2651,  ...,  0.7675,  0.6830,  0.4781],\n",
      "        [-0.4069, -0.2713, -1.1849,  ...,  0.2620,  0.8672,  0.6568],\n",
      "        [ 0.1829, -0.2229, -0.8383,  ...,  0.8459,  1.9621,  1.3457]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 1.3220,  0.2921,  0.1475,  ..., -0.1397,  0.3452, -0.3143],\n",
      "        [ 0.1206,  0.7780, -0.7014,  ..., -0.1943, -0.8865,  0.1333],\n",
      "        [-0.5139, -0.2195,  0.2276,  ...,  0.3594,  1.3424,  0.1206],\n",
      "        ...,\n",
      "        [-0.4091,  0.8100, -0.1160,  ...,  0.0111, -0.1179,  0.7854],\n",
      "        [ 0.0746,  0.5106, -0.0520,  ..., -0.6469,  0.2322, -0.3538],\n",
      "        [ 0.2083,  0.0475, -0.0715,  ...,  0.9441,  0.0342, -0.1165]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.1431,  0.7980, -0.7669,  ...,  0.9444,  0.4292,  0.5355],\n",
      "        [-0.4153, -0.4620, -0.1361,  ..., -0.4599, -0.9177, -0.0752],\n",
      "        [ 0.5986,  0.2841,  0.9039,  ...,  0.5032,  0.5470,  0.2171],\n",
      "        ...,\n",
      "        [ 0.5899,  0.2911, -0.4343,  ..., -0.0061, -0.6301, -0.0459],\n",
      "        [-0.6194, -0.5258, -0.3881,  ..., -0.0013,  0.2469, -0.1935],\n",
      "        [-0.2886, -0.5359, -0.0354,  ...,  0.2619, -0.2769,  0.4861]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 9.5785e-01, -1.7826e-01, -5.1649e-01,  ...,  1.2653e-03,\n",
      "          3.6885e-02,  7.5318e-01],\n",
      "        [-7.7553e-01, -2.5045e-01, -1.1739e+00,  ...,  6.5811e-01,\n",
      "          8.7516e-01,  1.3753e-01],\n",
      "        [ 2.5793e-02, -2.2712e-01,  8.9542e-01,  ..., -4.5086e-01,\n",
      "          3.6759e-01, -1.2705e+00],\n",
      "        ...,\n",
      "        [ 1.7148e-01,  9.3971e-02,  2.6508e-01,  ..., -2.8428e-01,\n",
      "         -8.7029e-01,  3.3428e-01],\n",
      "        [-4.0689e-01, -2.7134e-01, -1.1849e+00,  ..., -5.1834e-01,\n",
      "         -8.6662e-01,  2.6007e-01],\n",
      "        [ 1.8290e-01, -2.2290e-01, -8.3833e-01,  ..., -4.9401e-01,\n",
      "          3.9809e-01, -2.1246e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 0.4961, -2.5546,  0.6849,  0.9185,  0.5335, -0.0584, -0.2323, -0.2085,\n",
      "        -0.4829, -0.2110,  0.3906, -0.4656,  0.5984, -1.1929, -0.3325,  0.1927,\n",
      "         0.8538, -0.2872, -0.6089, -0.5643, -1.7287,  0.8959, -0.3251,  1.3149,\n",
      "         0.1291,  0.9114, -0.9837,  0.8103,  0.2028, -0.4038,  0.5343, -0.2910,\n",
      "        -1.8018, -0.3657, -0.5216,  0.6014,  0.9074, -0.2324,  1.2038,  0.2937,\n",
      "         0.4840,  1.0476,  1.0445,  0.2068, -0.4228,  0.5476,  0.0558, -0.7684,\n",
      "        -0.8776,  0.5746,  0.0757,  0.4791,  0.1997, -0.2173,  0.4695,  0.1323,\n",
      "         0.9968, -1.0498, -0.1052, -0.2274,  0.2931,  0.0651, -1.1943, -0.4485],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.299\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-0.6167,  0.2279,  0.0457,  ...,  1.1225,  0.7628,  0.9981],\n",
      "        [ 0.1909,  0.5286,  0.0982,  ...,  0.7661,  1.6329,  0.6065],\n",
      "        [-0.2163, -0.0406,  0.2836,  ...,  0.4263,  2.0378,  1.1838],\n",
      "        ...,\n",
      "        [-0.2656, -0.8372,  0.1780,  ...,  0.7648,  0.2648,  0.6636],\n",
      "        [ 0.5975, -0.1680,  0.8940,  ...,  2.8662,  1.4199,  1.3532],\n",
      "        [-0.7169, -0.1230, -0.2306,  ...,  1.2247,  0.6061,  0.8037]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-0.6167,  0.2279,  0.0457,  ..., -0.3488,  0.0128,  0.9387],\n",
      "        [ 0.1909,  0.5286,  0.0982,  ...,  0.7493, -0.3132, -0.5283],\n",
      "        [-0.2163, -0.0406,  0.2836,  ..., -0.0441, -1.0287, -0.0878],\n",
      "        ...,\n",
      "        [-0.6005,  1.0844,  0.5075,  ...,  0.6341,  0.8363, -0.0044],\n",
      "        [-0.1809,  0.5188,  0.2350,  ...,  0.2850,  0.6264, -1.8755],\n",
      "        [-1.1897,  0.2462, -0.2930,  ...,  0.1755,  0.0448, -0.7243]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.1224, -0.1995,  0.1003,  ...,  0.2171, -0.5742,  0.1384],\n",
      "        [ 0.0421,  0.8316,  0.4931,  ...,  0.4380, -0.7051,  0.0219],\n",
      "        [-0.1252,  1.1552,  0.4930,  ...,  0.4260,  0.3167, -0.7153],\n",
      "        ...,\n",
      "        [-0.3479,  0.3428,  0.2970,  ...,  0.3034, -0.1840,  0.0255],\n",
      "        [ 0.3322,  0.6352, -0.7606,  ...,  0.3812,  0.5591, -0.4815],\n",
      "        [ 0.3978,  0.3176,  0.3415,  ...,  0.0325,  0.6979, -0.1881]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.1504,  0.8794, -0.6805,  ...,  0.0539,  0.1412,  0.5553],\n",
      "        [-0.1509,  0.1447, -0.1240,  ...,  1.1629,  0.2846, -0.3670],\n",
      "        [ 1.0325, -0.2748,  0.8606,  ...,  0.0477,  0.4040, -0.4652],\n",
      "        ...,\n",
      "        [-0.2656, -0.8372,  0.1780,  ...,  0.2075, -0.2806,  0.4201],\n",
      "        [ 0.5975, -0.1680,  0.8940,  ...,  0.6149, -1.0639,  0.3616],\n",
      "        [-0.7169, -0.1230, -0.2306,  ..., -0.2956, -0.1503, -0.4900]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.6197, -1.5964, -0.0849, -0.0155, -0.1472,  0.2168, -0.1439,  0.5022,\n",
      "        -0.9998,  0.5916,  0.2713, -0.5050,  0.4519, -0.4783,  0.4689,  0.1680,\n",
      "         0.0089, -1.0589,  1.8214,  0.1247,  0.3578, -1.8122, -1.0561,  0.1449,\n",
      "         0.3512,  1.2169,  0.2503,  0.6215, -0.1226, -0.3188,  0.4969,  0.0417,\n",
      "         0.3656,  0.7032, -1.8868,  2.4441,  0.4597, -0.0697,  0.0552,  0.6256,\n",
      "        -0.4102, -0.6379, -0.0176, -0.8108, -0.4996,  0.8143,  0.9122, -0.4345,\n",
      "         0.0783,  1.5258, -0.8222,  0.8735,  1.8325, -1.3592, -0.2420,  0.0361,\n",
      "         0.7248,  0.1571,  0.9864, -1.2722, -1.1290,  0.9464, -0.9040,  0.7975],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.351\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 0.1730,  0.5245, -0.4444,  ...,  0.5691,  2.7215,  1.5994],\n",
      "        [-0.6017,  0.1812, -0.2952,  ...,  2.7330,  2.5153,  1.6283],\n",
      "        [-0.9005,  0.5399, -0.5727,  ...,  0.4072,  0.5525,  0.1712],\n",
      "        ...,\n",
      "        [ 0.1268,  0.7996,  1.5064,  ...,  0.5073,  1.4969,  1.8970],\n",
      "        [ 0.7798, -0.3584,  0.1136,  ...,  0.4079,  1.6498,  2.2063],\n",
      "        [ 0.4745,  0.1830,  0.1265,  ...,  1.3129,  0.8339,  0.8273]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 0.1730,  0.5245, -0.4444,  ..., -0.2311,  0.1700,  0.3622],\n",
      "        [-0.6017,  0.1812, -0.2952,  ...,  1.6531,  0.3806,  0.5042],\n",
      "        [-0.9005,  0.5399, -0.5727,  ...,  0.5428, -0.1077, -0.1685],\n",
      "        ...,\n",
      "        [ 0.3815,  0.7111,  0.5569,  ..., -0.6084, -0.3249,  0.0676],\n",
      "        [ 0.8484,  0.9600, -0.5418,  ..., -1.0761, -0.2996,  1.1820],\n",
      "        [ 0.0286,  0.6937, -0.0623,  ..., -0.3394, -1.3442, -1.0225]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.2667,  0.1228, -0.3886,  ...,  0.2988,  0.4428,  0.2654],\n",
      "        [ 0.3629, -0.6665,  0.7849,  ...,  0.4178,  0.5102, -0.8721],\n",
      "        [-0.3161,  0.9985, -0.8152,  ...,  0.1545, -0.3013, -1.0232],\n",
      "        ...,\n",
      "        [ 0.7360, -0.0722,  1.1023,  ...,  0.4525, -0.0382,  0.0437],\n",
      "        [ 0.0186, -0.0964, -0.0644,  ..., -0.4639,  0.1184, -0.8151],\n",
      "        [-0.6674,  0.5542, -0.2040,  ..., -0.6037, -0.4770,  0.8368]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5287,  0.4941,  1.2090,  ..., -0.1753, -0.0123, -0.9154],\n",
      "        [ 0.4328, -0.2611, -0.3236,  ..., -0.1534, -0.2141,  0.8381],\n",
      "        [ 0.4632, -0.1839, -0.5236,  ...,  0.0218,  0.4435,  0.3134],\n",
      "        ...,\n",
      "        [ 0.1268,  0.7996,  1.5064,  ...,  0.2287, -0.1249, -0.3642],\n",
      "        [ 0.7798, -0.3584,  0.1136,  ..., -0.1308,  0.8573, -0.7615],\n",
      "        [ 0.4745,  0.1830,  0.1265,  ...,  0.3079, -0.6816, -0.6937]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.3017,  0.6978, -0.7468, -0.2264,  0.5527,  0.7253,  1.7581,  0.2618,\n",
      "        -0.4812,  0.8078,  0.1511, -0.8438, -0.5917,  0.5343,  0.2775,  2.1754,\n",
      "         0.6156, -0.8921,  0.6735,  0.7507, -0.1660,  0.1117,  0.4214,  0.2475,\n",
      "         0.3319, -0.3798,  0.5977, -0.3055,  0.5866, -0.7150,  1.1599, -0.5434,\n",
      "        -0.5365,  0.8083,  1.6070, -0.1506, -1.5415, -1.1022, -0.0132,  0.5642,\n",
      "        -0.6749,  1.0834,  1.0426, -0.7482, -3.2434, -0.5190,  0.5211, -0.6882,\n",
      "         1.3441,  0.0354, -1.2862, -0.1796,  0.5846, -0.0042,  0.1091,  0.1330,\n",
      "         0.9290,  0.2648,  1.4146,  0.4172, -0.1459,  0.4485,  0.4757,  1.4128],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.416\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-2.0345e-01, -1.0340e+00,  1.0427e-01,  ...,  5.5390e+00,\n",
      "          2.2790e+00,  9.4199e-01],\n",
      "        [-2.8925e-01, -1.4244e-01, -1.6458e-01,  ...,  1.2980e+00,\n",
      "          7.0003e-01,  6.2581e-01],\n",
      "        [ 7.3756e-01, -4.1010e-01,  2.7981e-01,  ...,  6.5803e-01,\n",
      "          1.4057e+00,  1.0576e+00],\n",
      "        ...,\n",
      "        [-2.0490e-01, -1.6369e-03,  1.0883e+00,  ...,  9.8172e-01,\n",
      "          8.2489e-01,  6.3543e-01],\n",
      "        [-2.5059e-02,  1.3358e-01, -1.8173e-01,  ...,  4.5009e-01,\n",
      "          8.3333e-01,  3.7350e-01],\n",
      "        [ 8.4346e-01,  2.2124e-01, -1.5260e-01,  ...,  1.0286e+00,\n",
      "          1.4224e+00,  1.0978e+00]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-0.2035, -1.0340,  0.1043,  ...,  0.2698,  0.8732, -0.1192],\n",
      "        [-0.2893, -0.1424, -0.1646,  ...,  0.0183, -0.4239,  0.4553],\n",
      "        [ 0.7376, -0.4101,  0.2798,  ..., -0.9099, -0.1832, -0.1678],\n",
      "        ...,\n",
      "        [ 0.5877,  0.1666,  0.4950,  ..., -0.3613, -0.3823, -0.4132],\n",
      "        [ 0.3440,  0.6285, -0.5170,  ...,  0.2509, -0.2948, -0.9758],\n",
      "        [ 0.0914, -0.4061, -0.8294,  ...,  0.3463, -0.1849,  0.3532]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.6565, -0.2090, -0.4664,  ..., -0.3279, -0.2307,  0.3407],\n",
      "        [ 0.4920,  0.6592, -0.1447,  ..., -0.1673,  0.6868, -0.3762],\n",
      "        [-0.4574,  0.1552,  0.3863,  ..., -0.7540,  0.5243, -0.7801],\n",
      "        ...,\n",
      "        [ 0.2026,  0.4556,  0.4598,  ...,  0.0019, -0.6273, -0.5089],\n",
      "        [-0.3733, -0.7403,  0.3899,  ..., -0.2183, -0.1862, -0.4078],\n",
      "        [ 0.4334, -0.4084, -0.1196,  ...,  0.1468,  0.3988,  0.5940]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.2026, -0.0846, -0.3684,  ..., -0.9415, -0.6623, -0.7700],\n",
      "        [ 0.1581, -0.1339,  0.6221,  ...,  0.2215,  0.1828,  0.7268],\n",
      "        [-0.3131,  0.4449,  0.4859,  ...,  0.6799, -1.5100, -0.3793],\n",
      "        ...,\n",
      "        [-0.2049, -0.0016,  1.0883,  ...,  0.0689,  0.5852,  0.4766],\n",
      "        [-0.0251,  0.1336, -0.1817,  ..., -0.7358, -0.3401, -0.9138],\n",
      "        [ 0.8435,  0.2212, -0.1526,  ..., -0.3675,  0.7524, -0.1393]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.1171, -1.2963,  0.3285, -0.1449,  0.2796, -0.0046,  0.7862, -0.1254,\n",
      "         0.7233,  0.1006,  0.4247,  0.7805, -0.6726, -0.1920, -0.0450,  0.4413,\n",
      "        -0.7251,  1.0747,  0.2085,  0.0100,  0.9108,  0.9860,  0.1206,  0.4684,\n",
      "         1.0774, -0.5418,  0.1773, -1.9036, -0.0793,  0.2188,  1.0267,  1.2914,\n",
      "        -1.2883,  0.0696, -0.3859, -1.2824,  0.1684, -0.4431,  1.0280, -0.1507,\n",
      "        -0.3459,  0.5862,  0.0933, -0.1407, -0.9654,  0.6299, -0.1714,  0.6803,\n",
      "        -0.8007,  0.9127, -2.7170, -0.7204, -0.6399,  0.0581,  0.1680,  0.2577,\n",
      "         0.3037, -1.2544, -0.4988,  0.6097, -0.2964, -1.4791,  0.2983, -1.1957],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.025, triplet_loss: 0.270\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 0.2735, -0.3030,  0.8503,  ...,  0.6314,  0.7903,  0.9387],\n",
      "        [ 1.0126,  0.6012, -0.3674,  ...,  0.5472,  2.2078,  0.6647],\n",
      "        [-0.9709, -0.5973, -0.0278,  ...,  0.8245,  1.3316,  0.6758],\n",
      "        ...,\n",
      "        [-1.0432, -0.6532, -0.3030,  ...,  0.9374,  0.6608,  1.2520],\n",
      "        [ 0.0895, -0.3252, -0.9591,  ...,  0.5599,  0.7888,  0.9432],\n",
      "        [ 0.1670, -0.5433, -0.8592,  ...,  0.3414,  2.2883,  1.2074]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 0.2735, -0.3030,  0.8503,  ...,  0.3301, -1.1875,  0.3503],\n",
      "        [ 1.0126,  0.6012, -0.3674,  ..., -0.2234, -1.0294, -0.7517],\n",
      "        [-0.9709, -0.5973, -0.0278,  ...,  0.4581, -0.1550, -0.8127],\n",
      "        ...,\n",
      "        [ 0.3070, -0.2418, -0.7204,  ...,  0.5306, -0.6639, -0.4474],\n",
      "        [ 0.9058,  0.5996, -0.6571,  ...,  0.3097,  0.4773,  0.1979],\n",
      "        [-0.3686, -0.0343, -0.3946,  ..., -0.4017, -1.0340,  1.0853]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.2221, -0.1614,  0.2957,  ..., -0.2530, -0.8510, -1.0065],\n",
      "        [-0.5195,  0.2024,  0.4253,  ..., -0.4452,  0.2644, -0.5077],\n",
      "        [-0.2060, -0.6900,  0.6516,  ..., -0.6759,  0.2916, -0.7136],\n",
      "        ...,\n",
      "        [ 1.2006,  1.2355, -0.4458,  ..., -0.2484,  0.0370, -0.0274],\n",
      "        [-0.3881, -0.2402, -0.1689,  ..., -0.0656,  0.4606,  0.1417],\n",
      "        [ 0.3156,  1.6974, -0.0611,  ...,  0.5871, -0.3655,  0.1872]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-1.4476,  0.6846, -0.6627,  ...,  0.8020,  1.4379,  0.1408],\n",
      "        [-0.1575,  1.0779,  0.4667,  ...,  0.4614,  0.2900,  0.6402],\n",
      "        [-0.4747,  0.1945,  1.1862,  ..., -0.1925,  0.1679,  0.1285],\n",
      "        ...,\n",
      "        [-1.0432, -0.6532, -0.3030,  ..., -0.5855, -0.3311,  0.1650],\n",
      "        [ 0.0895, -0.3252, -0.9591,  ...,  0.0671, -0.3349,  0.0912],\n",
      "        [ 0.1670, -0.5433, -0.8592,  ..., -1.6474, -0.6127, -0.7401]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-2.0951, -1.0551,  0.9842, -0.8753, -0.4347,  0.0039,  0.0072, -0.0354,\n",
      "         0.4793, -0.5237,  1.0870,  0.6193, -1.6297, -0.3611, -2.0659,  1.2689,\n",
      "        -0.5740,  1.0522, -1.7038, -0.2131, -0.4404,  0.1091,  0.1472, -0.4494,\n",
      "         0.3667, -0.3461,  0.3292,  0.8738,  1.0570, -1.2253,  0.6528, -0.4961,\n",
      "        -0.9123,  0.9307,  0.8306,  0.5368, -0.0621,  0.5405, -0.7433,  2.5116,\n",
      "        -0.8512, -0.7733, -0.4245,  0.8980, -0.5534, -0.6478, -0.3882,  0.4603,\n",
      "        -0.3211,  1.6348,  0.3707, -1.3736, -0.9916, -0.1569,  1.0192, -0.1952,\n",
      "         1.4450, -0.9121,  0.4915,  1.9270, -0.7057,  1.0731, -0.0649, -0.1704],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.027, triplet_loss: 0.370\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-1.5063, -0.7181,  0.1402,  ...,  0.7762,  1.0842,  1.4770],\n",
      "        [ 0.3860, -0.7606,  0.5135,  ...,  1.8825,  1.9609,  0.3979],\n",
      "        [ 0.9795,  0.6316,  1.1261,  ...,  1.1558,  1.4151,  2.0884],\n",
      "        ...,\n",
      "        [-0.2218, -0.5041,  0.4234,  ...,  0.4418,  3.6851,  4.0732],\n",
      "        [-0.2362, -0.7034, -0.6074,  ...,  3.0205,  2.2211,  0.7959],\n",
      "        [-0.2041,  0.1752,  0.0504,  ...,  1.2423,  0.3568,  1.3950]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-1.5063, -0.7181,  0.1402,  ..., -0.3779, -0.3176,  0.1553],\n",
      "        [ 0.3860, -0.7606,  0.5135,  ..., -1.0371, -0.8255, -0.4279],\n",
      "        [ 0.9795,  0.6316,  1.1261,  ...,  0.4603, -1.1323, -1.8088],\n",
      "        ...,\n",
      "        [ 0.3347,  0.2803, -0.5422,  ..., -0.0549,  0.3912, -0.0831],\n",
      "        [ 0.3252, -0.1327, -0.4288,  ...,  0.4486,  0.0050, -0.0243],\n",
      "        [ 0.0645,  0.2359,  0.5530,  ..., -0.3577, -0.7535, -1.0248]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.9402,  0.9289, -0.2446,  ..., -0.3005,  0.6554, -0.2093],\n",
      "        [ 0.2626,  0.8401, -0.0806,  ...,  0.1340,  0.6771,  0.2966],\n",
      "        [-0.3983,  0.5349,  0.0872,  ...,  0.0923,  0.8672, -0.7417],\n",
      "        ...,\n",
      "        [ 1.0202, -0.6675,  0.0505,  ..., -0.4802,  0.2100,  0.0936],\n",
      "        [-0.3881,  0.3479,  0.1227,  ..., -0.4914, -0.5028, -0.7740],\n",
      "        [ 0.4718,  0.0203,  0.0796,  ..., -0.2942,  0.5896,  1.0280]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0228,  0.6991, -0.1405,  ...,  0.6336,  0.1288, -0.0470],\n",
      "        [-0.0552,  0.0647, -0.0437,  ...,  0.3733,  0.8303,  1.0032],\n",
      "        [-0.2467,  0.3701, -0.4215,  ...,  0.1778, -0.2608,  0.7343],\n",
      "        ...,\n",
      "        [-0.2218, -0.5041,  0.4234,  ...,  0.3051,  0.1887, -0.4835],\n",
      "        [-0.2362, -0.7034, -0.6074,  ...,  0.8047,  0.0883, -1.6534],\n",
      "        [-0.2041,  0.1752,  0.0504,  ..., -0.6349, -0.5659, -0.4444]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 0.8339, -0.4930, -1.1654, -0.0773,  1.3009, -0.8494, -0.5459, -0.8743,\n",
      "        -0.0204,  0.1365, -1.1834, -0.6542, -0.7304, -0.3240, -0.3513, -0.2242,\n",
      "        -0.1220,  2.3156, -0.5013, -0.4858,  0.1742, -0.6947, -0.6001, -0.3883,\n",
      "         0.3430, -1.2879, -0.1340,  0.4747, -0.5080,  0.0993, -0.5504, -0.4277,\n",
      "         0.1793,  1.5862,  0.3632,  0.4818,  0.3067,  0.9837,  0.1578,  0.3062,\n",
      "         0.3625,  0.1450, -0.4208, -0.3124, -0.2689,  1.3655,  1.5017, -0.5091,\n",
      "         0.2362, -1.9764,  0.6591,  0.7368,  0.5706,  1.8673, -0.6714,  1.2273,\n",
      "         1.9056, -1.0526, -0.6447, -1.8686, -0.3230, -1.0434, -0.9424,  0.9297],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.337\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-0.3808,  0.4644, -0.2398,  ...,  0.7215,  0.9217,  1.4571],\n",
      "        [-0.3127, -0.3012, -0.6662,  ...,  0.8522,  0.9226,  0.5835],\n",
      "        [ 0.3180,  0.3101, -0.0264,  ...,  0.5413,  1.5208,  0.6836],\n",
      "        ...,\n",
      "        [ 0.5972, -0.2394,  0.5079,  ...,  0.3142,  2.3479,  0.6588],\n",
      "        [-0.3315,  0.1478, -0.8505,  ...,  0.6023,  2.6073,  1.6848],\n",
      "        [-1.0792, -0.4335, -0.4565,  ...,  0.6018,  0.8820,  0.5725]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-0.3808,  0.4644, -0.2398,  ...,  0.7544, -0.1088,  0.7005],\n",
      "        [-0.3127, -0.3012, -0.6662,  ...,  0.6210,  0.2136, -1.0231],\n",
      "        [ 0.3180,  0.3101, -0.0264,  ..., -0.7359, -0.1555,  0.1326],\n",
      "        ...,\n",
      "        [-0.2561, -0.2923,  0.3165,  ...,  1.4810,  0.7675,  1.0781],\n",
      "        [ 0.3383, -0.5679, -0.1727,  ...,  0.4849,  0.9437,  0.1283],\n",
      "        [ 0.3645,  0.2676,  0.8205,  ..., -0.9590,  0.1484, -0.1177]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-3.6026e-01, -1.1193e+00, -8.2100e-01,  ..., -4.9609e-01,\n",
      "          1.3782e-03, -2.1744e-01],\n",
      "        [ 1.5071e-01, -4.4354e-01,  2.2938e-01,  ...,  1.7159e-01,\n",
      "          3.4546e-01,  9.1069e-02],\n",
      "        [ 1.1528e+00,  2.8876e-01,  1.0016e+00,  ...,  1.0690e+00,\n",
      "          5.3087e-01, -8.3782e-01],\n",
      "        ...,\n",
      "        [ 3.2691e-01,  7.9857e-02, -4.8448e-02,  ...,  1.6415e-01,\n",
      "          5.3278e-01,  3.0181e-01],\n",
      "        [ 5.8509e-01,  1.0416e-01,  3.3891e-02,  ..., -1.6278e-01,\n",
      "          2.5393e-01, -7.0633e-01],\n",
      "        [ 9.9082e-01, -1.6748e+00, -1.3557e-01,  ...,  1.6529e-01,\n",
      "          2.6204e-01,  7.7232e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4707, -0.2473,  0.0089,  ...,  1.0030, -0.1619, -0.6718],\n",
      "        [-0.5426, -0.3429,  0.1991,  ...,  0.1841,  0.2920,  0.2106],\n",
      "        [ 0.0901,  0.0693, -0.1086,  ..., -0.3526, -0.3915,  0.1463],\n",
      "        ...,\n",
      "        [ 0.5972, -0.2394,  0.5079,  ...,  0.7182,  0.2894,  0.1986],\n",
      "        [-0.3315,  0.1478, -0.8505,  ..., -0.1455,  0.7101,  0.3783],\n",
      "        [-1.0792, -0.4335, -0.4565,  ..., -0.5025,  0.6700,  0.2787]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.6959,  1.1328,  0.7306,  0.0222,  1.9204,  0.5411,  0.0086, -0.4385,\n",
      "        -2.2701,  0.7321, -1.5994,  0.0915,  1.0491, -0.7060,  0.2940,  0.0280,\n",
      "         0.2539,  0.5945,  0.2733, -1.0333, -0.5715,  0.0731, -1.0654,  0.2321,\n",
      "         1.5192, -0.0838,  0.2005,  1.5282,  0.0236,  0.8873,  0.0164,  1.5096,\n",
      "         0.0202,  1.3592,  0.1166,  0.1582,  0.1728,  0.7585,  1.2145,  0.1160,\n",
      "        -0.8340, -0.0788,  0.1248,  1.6750, -1.1283,  1.2085,  0.8775, -2.3515,\n",
      "         0.8834,  0.0533, -0.1553,  0.0747,  0.9089,  0.0636, -1.6261,  0.9614,\n",
      "         1.3324, -0.4610, -0.8419,  1.3331,  0.7215, -1.2865,  0.6280, -0.9927],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.025, triplet_loss: 0.444\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 0.2166, -0.0555,  0.4267,  ...,  1.1439,  1.9219,  0.5166],\n",
      "        [ 1.0939,  0.5750, -0.6827,  ...,  0.7056,  1.2070,  1.1343],\n",
      "        [-0.3147, -0.2547, -0.2645,  ...,  0.7063,  0.7397,  1.4129],\n",
      "        ...,\n",
      "        [-0.2645,  0.6892, -0.2480,  ...,  1.2577,  0.9183,  2.2254],\n",
      "        [ 0.6460,  0.4420,  0.1431,  ...,  0.6718,  0.9142,  1.0699],\n",
      "        [ 0.1883, -0.4410,  1.0104,  ...,  1.0257,  3.0647,  0.5989]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 0.2166, -0.0555,  0.4267,  ..., -0.0509,  0.7194, -1.0014],\n",
      "        [ 1.0939,  0.5750, -0.6827,  ...,  0.7930,  0.0155,  0.0707],\n",
      "        [-0.3147, -0.2547, -0.2645,  ...,  0.5379,  0.1169,  0.3478],\n",
      "        ...,\n",
      "        [-0.6675, -0.0845, -0.0089,  ...,  0.1226,  0.2009,  0.0333],\n",
      "        [-0.7554, -0.0271, -1.6387,  ...,  0.4656, -0.1026,  0.2134],\n",
      "        [ 0.3392,  0.6236, -0.6623,  ...,  0.6463,  0.3069, -0.2790]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.2455, -0.0286, -0.5194,  ..., -0.1918, -0.1378,  1.0525],\n",
      "        [-0.0701,  0.5046,  0.0644,  ...,  0.3957,  0.8157, -0.1652],\n",
      "        [ 0.2746, -0.1316, -0.7515,  ...,  0.1225,  0.4623,  0.6249],\n",
      "        ...,\n",
      "        [ 0.4497, -0.0022,  0.4733,  ..., -0.6378, -0.8673,  0.0789],\n",
      "        [ 0.4208, -0.1082, -0.6413,  ...,  0.8588, -1.0833, -0.1348],\n",
      "        [ 0.0706,  0.0590, -0.8830,  ..., -0.7684, -0.3754,  0.5246]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.2780,  0.4510, -0.0888,  ...,  0.1866, -0.1874, -0.5205],\n",
      "        [ 0.3131,  0.0783,  0.5372,  ...,  0.1121,  0.0155,  0.2161],\n",
      "        [ 0.1855,  0.2759, -0.0978,  ..., -0.0533,  0.8000,  0.3507],\n",
      "        ...,\n",
      "        [-0.2645,  0.6892, -0.2480,  ..., -0.0343, -0.1639,  0.5385],\n",
      "        [ 0.6460,  0.4420,  0.1431,  ...,  0.4258,  0.8340,  0.2611],\n",
      "        [ 0.1883, -0.4410,  1.0104,  ...,  0.3129, -0.1272,  0.1462]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.3687,  1.3810, -1.0333, -0.2168,  1.2804,  0.5758,  0.8235, -0.2888,\n",
      "        -0.4468, -0.1096,  0.5132, -0.3938, -1.1688,  0.5663, -0.4043, -0.3789,\n",
      "         1.0826,  0.7843, -0.1088,  1.2579, -1.0355,  0.6576, -0.0022,  0.3093,\n",
      "        -0.1057,  0.2610, -0.7379,  0.7392,  0.3740, -0.6595,  0.5131,  0.5090,\n",
      "         0.7861,  0.7539,  0.4704,  1.7347,  0.7277,  0.2821,  0.8798, -1.2022,\n",
      "         0.6373,  0.0102,  1.0005, -0.6779,  0.9066,  0.6184, -1.2967,  1.6181,\n",
      "         0.1397,  0.4445, -0.2333, -1.0912, -0.8006, -0.1249,  0.0605,  1.5656,\n",
      "         0.5971,  0.1529,  0.3068, -1.0259,  0.4606,  1.8449, -0.5945,  0.2612],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.457\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 0.5584,  0.6770,  0.3708,  ...,  0.2940,  0.2862,  0.9428],\n",
      "        [-0.3009, -0.3531,  0.0520,  ...,  1.1058,  1.1330,  0.2482],\n",
      "        [-0.2297, -1.1913,  0.4124,  ...,  0.6242,  1.9840,  0.8129],\n",
      "        ...,\n",
      "        [ 0.6951,  0.6106, -0.4717,  ...,  1.5601,  1.1304,  1.5422],\n",
      "        [ 0.5387,  0.3929, -0.4025,  ...,  2.0496,  0.5767,  1.2702],\n",
      "        [-0.0871, -0.4940,  0.3141,  ...,  0.3575,  0.6448,  0.5875]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 5.5838e-01,  6.7701e-01,  3.7077e-01,  ..., -5.7477e-02,\n",
      "         -1.4250e-01, -2.9344e-01],\n",
      "        [-3.0095e-01, -3.5310e-01,  5.1963e-02,  ...,  2.2656e-01,\n",
      "         -2.3989e-01,  1.1123e-01],\n",
      "        [-2.2971e-01, -1.1913e+00,  4.1240e-01,  ...,  2.8637e-01,\n",
      "          2.3096e-01, -3.4782e-01],\n",
      "        ...,\n",
      "        [ 1.2455e-01,  6.4254e-01, -5.8569e-01,  ...,  6.1981e-01,\n",
      "         -1.6799e-01,  2.7716e-01],\n",
      "        [-8.0158e-01,  1.7241e-01, -1.6124e-04,  ...,  4.5878e-01,\n",
      "         -3.7631e-01, -9.4986e-02],\n",
      "        [-8.1287e-01,  7.0021e-03, -1.0012e-01,  ...,  2.2666e-01,\n",
      "         -4.8859e-01, -5.6514e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0616, -0.9150,  0.0879,  ...,  0.1400, -0.5928, -0.4796],\n",
      "        [-0.5308,  0.6803, -0.4317,  ...,  0.5553, -1.5623, -0.6271],\n",
      "        [ 1.2188, -0.3074, -0.0587,  ..., -0.0461, -0.2422,  0.5366],\n",
      "        ...,\n",
      "        [-0.2111, -0.3598, -0.0631,  ...,  0.0885,  0.0770,  0.4413],\n",
      "        [ 0.8255, -0.6156,  0.3879,  ..., -0.2986, -1.1487, -0.6168],\n",
      "        [ 0.8335,  0.3953, -0.3063,  ..., -0.8328,  0.2429,  0.0261]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.1292, -0.4346,  0.0842,  ...,  1.0521, -1.0729,  0.2201],\n",
      "        [ 1.5180,  0.8438, -0.2551,  ...,  0.3770,  0.2121,  0.3254],\n",
      "        [ 0.2535,  0.2849,  0.6795,  ...,  0.0572, -0.2901, -0.3304],\n",
      "        ...,\n",
      "        [ 0.6951,  0.6106, -0.4717,  ..., -0.1204,  0.2967, -0.3690],\n",
      "        [ 0.5387,  0.3929, -0.4025,  ...,  0.6207, -1.0037,  1.3106],\n",
      "        [-0.0871, -0.4940,  0.3141,  ..., -0.8936,  0.7620, -0.0278]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.3793,  0.2556,  0.1831, -0.5205, -1.7050, -1.1675,  1.2138,  1.6637,\n",
      "         0.2734, -1.3934,  0.1297, -1.0456,  0.3726, -0.1500, -0.3162,  0.4858,\n",
      "        -1.1518,  0.8821, -0.8898, -1.1094, -1.3926,  0.2513,  1.3392,  0.3679,\n",
      "         0.2199,  0.4646, -0.1907, -1.1043,  0.7277, -0.0749,  0.3603, -0.3727,\n",
      "        -0.0815, -3.0365, -1.1405, -0.5868, -0.1107, -0.4515,  0.0285,  0.1831,\n",
      "        -0.6026,  0.1824,  0.0108,  0.4649, -0.4703, -0.0784, -0.5319,  0.8928,\n",
      "        -1.1927,  0.7644, -0.1158, -0.6718,  1.6779,  1.0245,  0.8119, -0.1325,\n",
      "         0.2672,  0.4742,  0.1385, -0.0525,  0.0693, -0.6745, -0.5341, -0.1979],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.025, triplet_loss: 0.253\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-0.4589,  0.4543,  0.1856,  ...,  2.2921,  0.7994,  1.9654],\n",
      "        [ 0.9248,  0.4160, -0.0852,  ...,  1.0117,  1.4891,  0.6995],\n",
      "        [-0.1154,  1.2150,  0.8909,  ...,  0.8267,  1.4187,  1.4859],\n",
      "        ...,\n",
      "        [ 0.8820,  0.1501,  0.3156,  ...,  2.4120,  0.9511,  0.6233],\n",
      "        [ 0.8727,  0.5614, -0.1032,  ...,  1.6873,  0.7558,  2.4737],\n",
      "        [ 1.0616,  0.9660,  0.7340,  ...,  0.6306,  0.7945,  1.2838]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-0.4589,  0.4543,  0.1856,  ..., -0.5012,  0.2245, -0.8794],\n",
      "        [ 0.9248,  0.4160, -0.0852,  ...,  0.0815,  0.4161, -0.0327],\n",
      "        [-0.1154,  1.2150,  0.8909,  ..., -0.9335,  1.4533, -0.2848],\n",
      "        ...,\n",
      "        [-0.5279,  0.0427, -0.0257,  ...,  0.3676,  0.3133,  0.2510],\n",
      "        [ 0.9402, -0.6091,  0.1585,  ...,  0.5636, -0.1041,  0.2180],\n",
      "        [ 0.7051, -0.7533,  0.9313,  ...,  0.2712,  0.1233,  0.1683]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.3832, -0.2581,  0.2035,  ..., -0.7575, -0.6060, -0.5188],\n",
      "        [ 0.4915, -1.1297, -0.1620,  ...,  0.0847, -0.5066,  0.2810],\n",
      "        [-0.3963, -0.0316, -0.4585,  ...,  0.6441, -0.0957,  0.9673],\n",
      "        ...,\n",
      "        [ 0.0632,  0.1057, -0.3368,  ...,  0.1455, -0.7807, -0.0892],\n",
      "        [ 0.3582,  0.3855,  0.0136,  ...,  0.2725, -0.3598,  0.1358],\n",
      "        [-0.0981,  0.1670, -0.3805,  ...,  0.4655, -0.3216, -0.0635]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.3593, -0.7773,  0.1596,  ..., -0.1156,  0.1829,  1.3159],\n",
      "        [ 0.3541,  0.8085, -0.8249,  ...,  0.4717, -0.0715, -0.2638],\n",
      "        [ 1.0021,  0.2665, -0.1943,  ..., -0.8121, -0.5971,  0.8141],\n",
      "        ...,\n",
      "        [ 0.8820,  0.1501,  0.3156,  ..., -0.9455,  1.0734,  0.1716],\n",
      "        [ 0.8727,  0.5614, -0.1032,  ..., -0.5160,  0.1773, -0.1276],\n",
      "        [ 1.0616,  0.9660,  0.7340,  ...,  0.0889,  0.7704,  0.1839]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-5.2470e-01,  2.2444e-01,  1.0267e+00,  1.0132e-02,  5.1941e-01,\n",
      "        -1.9515e+00,  4.8874e-01, -1.1355e+00, -1.2863e+00,  6.7818e-01,\n",
      "         9.3325e-01,  8.5974e-01, -4.6588e+00, -1.3655e-01,  3.1128e-01,\n",
      "        -3.8858e-01, -1.6785e+00,  2.8917e-01, -7.1468e-01, -2.7707e-02,\n",
      "        -4.5880e-01,  1.1411e-01,  1.5891e-01, -3.6831e-01,  1.4512e-01,\n",
      "        -1.0984e+00,  4.6077e-02, -9.2357e-01,  3.4299e-02, -4.9980e-01,\n",
      "         6.0508e-02,  6.4837e-01, -9.7740e-01, -5.0340e-01, -5.0448e-01,\n",
      "        -6.4550e-01, -1.0056e+00, -5.9544e-02,  7.9654e-02,  9.6111e-02,\n",
      "         2.0226e+00,  6.9101e-01,  2.5141e-01, -5.5285e-01,  5.6531e-02,\n",
      "         4.7721e-01, -1.2069e+00,  1.6695e-01, -1.3992e+00,  6.9786e-01,\n",
      "        -2.7200e-01,  5.6785e-01, -1.1153e+00,  1.5853e+00,  3.2756e-01,\n",
      "         1.1251e-01, -2.2874e-01,  1.1147e+00,  5.9891e-01, -1.0313e-03,\n",
      "        -5.9803e-01, -5.4609e-01, -4.1583e-01,  1.1845e+00], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.259\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-0.0302, -1.0640,  0.5633,  ...,  1.0707,  1.7623,  1.3248],\n",
      "        [-0.6350, -0.6164, -0.1002,  ...,  2.0599,  1.7697,  1.1791],\n",
      "        [ 0.3868,  0.4768,  0.5592,  ...,  0.6590,  0.7562,  2.3176],\n",
      "        ...,\n",
      "        [-0.6996, -0.7208, -0.4381,  ...,  0.8698,  1.1436,  1.0160],\n",
      "        [ 0.0696, -0.8550, -0.9436,  ...,  1.1013,  1.4342,  1.5529],\n",
      "        [ 0.8881, -0.4399,  0.4796,  ...,  1.6433,  1.6455,  1.4910]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-0.0302, -1.0640,  0.5633,  ..., -0.3301, -1.5652,  0.6194],\n",
      "        [-0.6350, -0.6164, -0.1002,  ..., -0.6175,  0.2489, -0.7873],\n",
      "        [ 0.3868,  0.4768,  0.5592,  ..., -0.2246, -0.8503, -0.2791],\n",
      "        ...,\n",
      "        [ 0.3910, -0.3435, -0.3047,  ..., -0.1295,  0.2163, -0.6082],\n",
      "        [ 0.2753, -0.0875,  0.2846,  ..., -0.6264,  0.2582, -0.1663],\n",
      "        [-0.4295, -0.8076, -0.1908,  ...,  0.4802,  0.8118,  0.5755]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0030, -0.6243, -0.3039,  ..., -0.7565, -0.5953, -0.0853],\n",
      "        [-0.9061,  0.1411, -0.8111,  ...,  0.0940, -0.4748,  0.5276],\n",
      "        [-0.3271,  0.3735, -0.8388,  ...,  0.9408,  0.6235,  0.4381],\n",
      "        ...,\n",
      "        [ 0.8845, -0.3721,  0.6464,  ..., -0.1461, -0.8467,  0.2401],\n",
      "        [ 1.1046, -1.0997,  0.4817,  ...,  0.7831,  0.6016, -0.7763],\n",
      "        [-0.6856, -0.7384,  0.6654,  ...,  0.9430, -0.1760, -1.0714]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.3385, -0.2388, -0.3568,  ...,  0.4403, -0.6892, -0.6703],\n",
      "        [ 0.3728, -0.5151, -1.2059,  ..., -0.6722,  0.8465,  0.2338],\n",
      "        [ 0.3074,  1.2005, -0.8194,  ...,  1.0937,  0.1804,  0.1322],\n",
      "        ...,\n",
      "        [-0.6996, -0.7208, -0.4381,  ...,  0.0479,  0.0638, -0.0098],\n",
      "        [ 0.0696, -0.8550, -0.9436,  ...,  0.8664,  0.2097,  0.0575],\n",
      "        [ 0.8881, -0.4399,  0.4796,  ...,  0.1323,  0.4281, -0.3156]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.5183e-01,  8.9030e-01, -3.0788e-01, -7.8650e-01, -6.0826e-01,\n",
      "         2.2360e-03,  2.1438e+00,  9.9536e-01, -2.5246e+00,  1.8202e-01,\n",
      "         1.9901e-01, -7.0068e-01, -1.3072e+00,  1.8806e-02,  1.2623e+00,\n",
      "        -6.3084e-01, -4.6498e-01, -3.1397e+00, -5.7652e-01, -1.2446e+00,\n",
      "        -1.7355e+00,  7.8284e-02, -9.7377e-01, -4.6924e-01,  1.1777e+00,\n",
      "        -3.5140e-01,  1.0365e-01, -4.4776e-01,  1.5864e+00, -1.2048e+00,\n",
      "        -8.9276e-01,  9.3030e-01,  3.8811e-03,  4.5792e-01,  5.8704e-01,\n",
      "        -6.0002e-01, -5.0820e-01, -4.0706e-01, -2.2005e+00,  6.4505e-01,\n",
      "        -1.7169e-01,  1.4193e+00,  1.2778e-01,  8.7457e-02,  4.7965e-02,\n",
      "         1.5542e+00,  1.7455e+00,  5.2817e-01, -1.5379e+00,  4.7613e-01,\n",
      "        -8.4642e-02, -3.6120e-01, -1.9257e-01,  9.9857e-01, -9.3416e-01,\n",
      "         1.8840e-01,  7.4965e-01,  3.9519e-01, -1.5520e-03,  7.0152e-02,\n",
      "        -1.6666e-01,  5.6032e-01, -6.1394e-01,  1.3116e+00], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.339\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[-0.5494,  0.6790, -0.3493,  ...,  0.8762,  1.1474,  0.3783],\n",
      "        [ 0.7162, -0.4880,  0.6318,  ...,  0.8097,  0.7914,  0.8091],\n",
      "        [-0.3914, -0.7706, -0.0624,  ...,  0.5251,  0.8857,  0.9493],\n",
      "        ...,\n",
      "        [ 0.1315, -0.6695, -0.0724,  ...,  0.6579,  0.5986,  0.5477],\n",
      "        [ 0.4195, -0.2239,  0.0788,  ...,  1.1350,  0.5108,  1.4808],\n",
      "        [ 0.4281,  0.3109,  0.1451,  ...,  1.1963,  0.8172,  0.6368]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[-0.5494,  0.6790, -0.3493,  ..., -0.5592,  0.3359,  0.2161],\n",
      "        [ 0.7162, -0.4880,  0.6318,  ..., -0.4897, -0.7575,  0.3085],\n",
      "        [-0.3914, -0.7706, -0.0624,  ...,  0.2098, -1.0650, -0.0151],\n",
      "        ...,\n",
      "        [-0.8515, -0.7725,  0.5845,  ...,  0.1495,  0.8825, -0.0668],\n",
      "        [-1.3100,  0.3051, -0.5528,  ...,  0.2208, -0.7611, -1.1868],\n",
      "        [ 0.1475,  0.4953,  0.2381,  ..., -0.5984, -0.7188, -0.0225]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0977, -0.2210, -0.7036,  ..., -0.3929, -0.4690, -0.2941],\n",
      "        [ 0.4013,  0.3791, -0.1131,  ..., -0.2716,  0.9289, -0.4200],\n",
      "        [ 0.0197, -1.0773,  1.0599,  ...,  0.6164,  0.2558, -0.4517],\n",
      "        ...,\n",
      "        [-0.1198, -0.1434, -0.6873,  ..., -0.7228,  0.1163, -0.8642],\n",
      "        [ 0.0559, -0.7170,  0.1565,  ..., -0.4462,  0.6729,  0.0869],\n",
      "        [ 0.0060,  0.0847, -0.1090,  ..., -0.0984, -0.4158, -0.6147]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.6122, -0.3632,  1.2444,  ...,  0.2773, -0.5624, -0.1658],\n",
      "        [-0.1850,  0.5096, -0.9239,  ..., -0.6393,  0.3120, -0.4653],\n",
      "        [-0.6725, -1.2741,  0.8556,  ...,  0.2221,  0.1363, -0.1544],\n",
      "        ...,\n",
      "        [ 0.1315, -0.6695, -0.0724,  ..., -0.3518,  0.8242, -0.7268],\n",
      "        [ 0.4195, -0.2239,  0.0788,  ..., -0.3861,  0.3025,  0.5117],\n",
      "        [ 0.4281,  0.3109,  0.1451,  ...,  0.0138,  0.3968, -0.2562]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.5085,  0.7405,  0.6827, -1.1633, -0.1758,  0.4074, -0.6221,  0.5267,\n",
      "        -0.7820,  0.7235,  1.7727,  0.3770, -0.3498, -1.5953,  0.4265,  0.9698,\n",
      "         1.5860,  0.7864,  0.2057, -0.0876, -0.0121, -0.4453,  0.5102, -0.2960,\n",
      "        -0.6175, -0.3343, -0.4283, -0.8221, -0.2943,  0.3827, -0.0840,  1.4060,\n",
      "         1.1296,  0.6602, -1.4680, -1.0480,  0.8223, -0.1608,  0.7654,  0.4485,\n",
      "         1.0138, -0.0882, -0.3569, -0.0424, -1.4918,  0.9310,  0.5323, -1.0887,\n",
      "         0.9117,  1.0024,  0.1105,  0.0116,  0.8839, -0.3468,  1.4511,  0.3635,\n",
      "        -1.1784, -0.2753, -0.2553, -1.3548,  0.0815, -2.2486,  0.6284,  0.1904],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.366\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[ 1.2429, -0.1388,  0.4382,  ...,  0.4061,  1.4396,  1.5633],\n",
      "        [ 0.5925, -0.3893,  0.0520,  ...,  0.5883,  1.2121,  0.8536],\n",
      "        [ 0.1101, -0.5563,  0.4039,  ...,  1.1059,  1.3158,  0.7346],\n",
      "        ...,\n",
      "        [-0.1630, -0.4737,  0.5237,  ...,  0.9417,  1.4927,  1.2169],\n",
      "        [-0.1651,  0.6747,  0.4958,  ...,  1.2808,  0.5995,  0.7871],\n",
      "        [ 0.5686,  0.3822,  0.0566,  ...,  1.7909,  1.0656,  1.0768]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[ 1.2429, -0.1388,  0.4382,  ..., -1.3451, -1.1955,  0.6851],\n",
      "        [ 0.5925, -0.3893,  0.0520,  ..., -0.5135, -0.7922, -0.0809],\n",
      "        [ 0.1101, -0.5563,  0.4039,  ...,  0.2891, -0.6858,  0.3968],\n",
      "        ...,\n",
      "        [-0.3733,  0.0232, -0.0618,  ..., -0.8340, -0.4538, -0.2208],\n",
      "        [-0.2648,  0.6338, -0.3089,  ...,  0.2797, -0.2870, -0.4810],\n",
      "        [-0.4285,  0.0074,  0.2236,  ...,  0.3450,  0.5745,  0.8240]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.2597, -0.2474,  0.4575,  ..., -0.1815,  0.9479,  0.6006],\n",
      "        [-0.4497,  0.4500, -0.0539,  ..., -0.0594,  0.3179, -0.0976],\n",
      "        [-0.2768, -0.4229,  0.4416,  ...,  0.0800,  0.3475,  0.0705],\n",
      "        ...,\n",
      "        [-0.2332, -0.2371, -0.3074,  ...,  0.9016,  0.8743,  0.3246],\n",
      "        [-0.4008,  0.7466,  0.9322,  ...,  0.4155, -0.1066, -0.0553],\n",
      "        [-0.4469, -0.7651, -0.0966,  ..., -0.2617, -0.0657, -0.3639]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.2621, -0.3733,  0.0215,  ...,  0.7106,  0.1393,  0.3476],\n",
      "        [-0.0390,  0.2791, -0.1421,  ..., -0.1578,  0.1893,  0.0296],\n",
      "        [ 0.1998, -0.2233,  0.2000,  ...,  0.1163,  0.4921, -0.8216],\n",
      "        ...,\n",
      "        [-0.1630, -0.4737,  0.5237,  ...,  0.6665, -0.0979, -0.8656],\n",
      "        [-0.1651,  0.6747,  0.4958,  ...,  0.0915,  0.1819, -0.6775],\n",
      "        [ 0.5686,  0.3822,  0.0566,  ..., -0.0713,  0.9289,  0.3943]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 0.7401,  0.8108, -0.7805, -0.0274,  0.3579, -2.6347, -0.1419,  0.6264,\n",
      "         0.0182,  0.7761,  0.9048, -0.5745,  0.6438, -0.4499,  0.2593,  0.7197,\n",
      "        -0.1580, -0.6669, -0.7360, -0.2809,  0.3582,  0.1000,  0.1458, -0.8139,\n",
      "        -0.1146, -0.0541,  0.6805, -0.6339,  0.0747, -8.2775,  1.4260, -0.7508,\n",
      "         0.1150, -2.0036,  0.4871, -0.2093,  0.2657, -0.0306,  0.9159, -0.7064,\n",
      "        -0.0823,  1.0709,  0.8359, -0.5312,  0.6129,  0.2288, -0.1798,  0.0960,\n",
      "         1.5373,  0.7102, -0.8181, -0.4007,  0.4199,  1.2653, -0.6154,  0.5322,\n",
      "         1.1696,  0.1386,  0.7315,  1.5002,  0.4411, -0.3342,  0.9599,  0.6889],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: 0.026, triplet_loss: 0.365\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "torch.Size([64, 2048])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss: nan, triplet_loss: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [83], line 51\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;66;03m# nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1, norm_type=2)\u001B[39;00m\n\u001B[0;32m     50\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 51\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcombined_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m finished with average loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_loss\u001B[38;5;241m/\u001B[39mnum_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesting ...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "import copy\n",
    "\n",
    "batch_size = 64  # NB this doesn't actually modify the batch size\n",
    "\n",
    "best_test_loss = 100\n",
    "best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"starting epoch {epoch} ...\")\n",
    "    # Train\n",
    "    model.train()\n",
    "\n",
    "    for (anchor_sig, pos_sig, neg_sig, anchor_label, neg_label, _) in train_dataloader:\n",
    "        print(anchor_sig.shape)\n",
    "        print(pos_sig.shape)\n",
    "        print(neg_sig.shape)\n",
    "\n",
    "        if anchor_sig.shape[0] != pos_sig.shape[0] or pos_sig.shape[0] != neg_sig.shape[0]:\n",
    "            # print(\"Not equal number of anchors, positive, and negatives\")\n",
    "            continue\n",
    "\n",
    "        if torch.any(torch.isnan(anchor_sig)) or torch.any(torch.isnan(pos_sig)) or torch.any(torch.isnan(neg_sig)):\n",
    "            print(\"Input is nan!\")\n",
    "            continue\n",
    "\n",
    "        batch_len = anchor_sig.shape[0]\n",
    "\n",
    "        all_signals = torch.concat([anchor_sig, pos_sig, neg_sig], dim=0)\n",
    "        all_signals = torch.unsqueeze(all_signals.to(device), 1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        latents = model(all_signals)\n",
    "\n",
    "        print(latents)\n",
    "\n",
    "        loss = loss_func(latents)\n",
    "        triplet_loss = triplet_latent_loss(latents[:batch_len, :z_dim],\n",
    "                                           latents[batch_len:2*batch_len, :z_dim],\n",
    "                                           latents[2*batch_len:, :z_dim])\n",
    "\n",
    "        print(f\"loss: {loss:0.3f}, triplet_loss: {triplet_loss:0.3f}\")\n",
    "\n",
    "        combined_loss = loss + 0.1 * triplet_loss\n",
    "        combined_loss.backward()\n",
    "\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1, norm_type=2)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += float(combined_loss.detach())\n",
    "\n",
    "    print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "    print(\"Testing ...\")\n",
    "    # Test\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (anchor_sig, pos_sig, neg_sig, anchor_label, neg_label, _) in enumerate(test_dataloader):\n",
    "            if anchor_sig.shape[0] != pos_sig.shape[0] or pos_sig.shape[0] != neg_sig.shape[0]:\n",
    "                print(\"Not equal number of anchors, positive, and negatives\")\n",
    "                continue\n",
    "\n",
    "            batch_len = anchor_sig.shape[0]\n",
    "\n",
    "            all_signals = torch.concat([anchor_sig, pos_sig, neg_sig], dim=0)\n",
    "            all_signals = torch.unsqueeze(all_signals.to(device), 1).float()\n",
    "\n",
    "            latents = model(all_signals)\n",
    "\n",
    "            loss = loss_func(output, all_signals, latents)\n",
    "            triplet_loss = triplet_latent_loss(latents[:batch_len, :z_dim],\n",
    "                                               latents[batch_len:2*batch_len, :z_dim],\n",
    "                                               latents[2*batch_len:, :z_dim])\n",
    "\n",
    "            total_loss = loss + triplet_loss\n",
    "            test_loss += float(total_loss)\n",
    "\n",
    "            print(f\"loss: {loss:0.3f}, triplet_loss: {triplet_loss:0.3f}\")\n",
    "\n",
    "    print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "\n",
    "    if test_loss/num_test_batches < best_test_loss:\n",
    "        best_model = copy.deepcopy(model).cpu()\n",
    "        best_test_loss = test_loss/num_test_batches\n",
    "\n",
    "model = best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        83.52%      50.130ms        99.54%      59.742ms       4.979ms      44.081ms        72.84%      59.961ms       4.997ms            12  \n",
      "                                       aten::pin_memory         1.16%     698.000us         7.38%       4.430ms     123.056us     810.000us         1.34%       2.570ms      71.389us            36  \n",
      "                                            aten::stack         2.05%       1.228ms         6.94%       4.168ms     347.333us     266.000us         0.44%     954.000us      79.500us            12  \n",
      "                                      aten::_pin_memory         1.61%     968.000us         5.87%       3.521ms      97.806us     810.000us         1.34%       1.485ms      41.250us            36  \n",
      "                                              aten::cat         4.57%       2.742ms         4.87%       2.923ms     243.583us     172.000us         0.28%     606.000us      50.500us            12  \n",
      "                                            aten::copy_         4.21%       2.529ms         4.21%       2.529ms      70.250us     435.000us         0.72%     435.000us      12.083us            36  \n",
      "                                         aten::randperm         1.02%     614.000us         2.54%       1.526ms     763.000us     593.000us         0.98%       1.602ms     801.000us             2  \n",
      "                                            aten::empty         0.65%     390.000us         0.65%     390.000us       7.647us     775.000us         1.28%     775.000us      15.196us            51  \n",
      "                                            aten::zeros         0.39%     237.000us         0.42%     250.000us      20.833us     354.000us         0.58%     521.000us      43.417us            12  \n",
      "                                        aten::is_pinned         0.35%     211.000us         0.35%     211.000us       5.861us     275.000us         0.45%     275.000us       7.639us            36  \n",
      "                                           aten::narrow         0.14%      84.000us         0.30%     181.000us      15.083us     181.000us         0.30%     434.000us      36.167us            12  \n",
      "                                            aten::slice         0.14%      84.000us         0.16%      97.000us       8.083us     173.000us         0.29%     253.000us      21.083us            12  \n",
      "                                             aten::set_         0.04%      24.000us         0.04%      24.000us       0.667us     240.000us         0.40%     240.000us       6.667us            36  \n",
      "                                       aten::lift_fresh         0.03%      18.000us         0.03%      18.000us       0.023us       5.564ms         9.19%       5.564ms       7.025us           792  \n",
      "                                             aten::view         0.03%      17.000us         0.03%      17.000us       1.417us      82.000us         0.14%      82.000us       6.833us            12  \n",
      "                                             aten::item         0.02%      14.000us         0.03%      16.000us       8.000us      29.000us         0.05%      42.000us      21.000us             2  \n",
      "                                          aten::random_         0.02%      13.000us         0.02%      13.000us       6.500us      13.000us         0.02%      13.000us       6.500us             2  \n",
      "                                       aten::as_strided         0.02%      13.000us         0.02%      13.000us       1.083us      80.000us         0.13%      80.000us       6.667us            12  \n",
      "                                    aten::scalar_tensor         0.00%       3.000us         0.00%       3.000us       3.000us       7.000us         0.01%       7.000us       7.000us             1  \n",
      "                              aten::_local_scalar_dense         0.00%       2.000us         0.00%       2.000us       1.000us      13.000us         0.02%      13.000us       6.500us             2  \n",
      "                                               aten::to         0.00%       1.000us         0.00%       1.000us       0.001us       5.457ms         9.02%       5.457ms       6.890us           792  \n",
      "                                            aten::zero_         0.00%       0.000us         0.00%       0.000us       0.000us      82.000us         0.14%      82.000us       6.833us            12  \n",
      "                                          aten::resize_         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us         0.02%      14.000us       7.000us             2  \n",
      "                                     aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us       7.000us         0.01%       7.000us       7.000us             1  \n",
      "                                      aten::resolve_neg         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.01%       6.000us       6.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 60.020ms\n",
      "Self CUDA time total: 60.519ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    for i, (_, _, _) in enumerate(train_dataloader): # , total=len(train_dataset)):\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=25))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train using the NST dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0 ...\n",
      "Epoch 0 finished with average loss 0.400227868819938\n",
      "Testing ...\n",
      "Average test loss: 0.37143253578859214\n",
      "starting epoch 1 ...\n",
      "Epoch 1 finished with average loss 0.40309759667691064\n",
      "Testing ...\n",
      "Average test loss: 0.373511717599981\n",
      "starting epoch 2 ...\n",
      "Epoch 2 finished with average loss 0.4044528840219273\n",
      "Testing ...\n",
      "Average test loss: 0.3732363511534298\n",
      "starting epoch 3 ...\n",
      "Epoch 3 finished with average loss 0.4039087159668698\n",
      "Testing ...\n",
      "Average test loss: 0.3708088310325847\n",
      "starting epoch 4 ...\n",
      "Epoch 4 finished with average loss 0.40508778261787753\n",
      "Testing ...\n",
      "Average test loss: 0.37695128426832314\n",
      "starting epoch 5 ...\n",
      "Epoch 5 finished with average loss 0.40080102489275093\n",
      "Testing ...\n",
      "Average test loss: 0.38048596767818227\n",
      "starting epoch 6 ...\n",
      "Epoch 6 finished with average loss 0.40437007563955646\n",
      "Testing ...\n",
      "Average test loss: 0.37527622019543366\n",
      "starting epoch 7 ...\n",
      "Epoch 7 finished with average loss 0.40649035835967345\n",
      "Testing ...\n",
      "Average test loss: 0.37587883191950183\n",
      "starting epoch 8 ...\n",
      "Epoch 8 finished with average loss 0.3997736972044496\n",
      "Testing ...\n",
      "Average test loss: 0.3760336392066058\n",
      "starting epoch 9 ...\n",
      "Epoch 9 finished with average loss 0.40399297852726546\n",
      "Testing ...\n",
      "Average test loss: 0.3765006819192101\n",
      "starting epoch 10 ...\n",
      "Epoch 10 finished with average loss 0.40172330553040786\n",
      "Testing ...\n",
      "Average test loss: 0.3761910813696244\n",
      "starting epoch 11 ...\n",
      "Epoch 11 finished with average loss 0.405391568208442\n",
      "Testing ...\n",
      "Average test loss: 0.37459660628262686\n",
      "starting epoch 12 ...\n",
      "Epoch 12 finished with average loss 0.399419804706293\n",
      "Testing ...\n",
      "Average test loss: 0.3698061722166398\n",
      "starting epoch 13 ...\n",
      "Epoch 13 finished with average loss 0.3997577463879305\n",
      "Testing ...\n",
      "Average test loss: 0.37847397081992207\n",
      "starting epoch 14 ...\n",
      "Epoch 14 finished with average loss 0.40443339505616355\n",
      "Testing ...\n",
      "Average test loss: 0.37334489997695475\n",
      "starting epoch 15 ...\n",
      "Epoch 15 finished with average loss 0.40264145065756407\n",
      "Testing ...\n",
      "Average test loss: 0.3758282924399656\n",
      "starting epoch 16 ...\n",
      "Epoch 16 finished with average loss 0.40285230910076814\n",
      "Testing ...\n",
      "Average test loss: 0.376090407371521\n",
      "starting epoch 17 ...\n",
      "Epoch 17 finished with average loss 0.40313855953076305\n",
      "Testing ...\n",
      "Average test loss: 0.37387256937868457\n",
      "starting epoch 18 ...\n",
      "Epoch 18 finished with average loss 0.4065669827601489\n",
      "Testing ...\n",
      "Average test loss: 0.3801198759499718\n",
      "starting epoch 19 ...\n",
      "Epoch 19 finished with average loss 0.4048439588616876\n",
      "Testing ...\n",
      "Average test loss: 0.37560179654289694\n",
      "starting epoch 20 ...\n",
      "Epoch 20 finished with average loss 0.39912191077190284\n",
      "Testing ...\n",
      "Average test loss: 0.3721800674410427\n",
      "starting epoch 21 ...\n",
      "Epoch 21 finished with average loss 0.40380948036909103\n",
      "Testing ...\n",
      "Average test loss: 0.37709520844852223\n",
      "starting epoch 22 ...\n",
      "Epoch 22 finished with average loss 0.4046939979581272\n",
      "Testing ...\n",
      "Average test loss: 0.37085288237122926\n",
      "starting epoch 23 ...\n",
      "Epoch 23 finished with average loss 0.40225333632791743\n",
      "Testing ...\n",
      "Average test loss: 0.375233811490676\n",
      "starting epoch 24 ...\n",
      "Epoch 24 finished with average loss 0.4037479661843356\n",
      "Testing ...\n",
      "Average test loss: 0.371685431284063\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "import copy\n",
    "\n",
    "best_test_loss = 100\n",
    "best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"starting epoch {epoch} ...\")\n",
    "    # Train\n",
    "    model.train()\n",
    "    for i, (signals, clean_signals, _) in enumerate(train_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        clean_signals = torch.unsqueeze(clean_signals.to(device), 1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, latents = model(signals)\n",
    "        loss = loss_func(output, clean_signals, latents.to(\"cpu\"))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "    print(\"Testing ...\")\n",
    "    # Test\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (signals, clean_signals, _)in enumerate(test_dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "            clean_signals = torch.unsqueeze(clean_signals.to(device), 1).float()\n",
    "\n",
    "            output, latents = model (signals)\n",
    "            loss = loss_func(output, clean_signals, latents.to(\"cpu\"))\n",
    "            test_loss += float(loss)\n",
    "\n",
    "    print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "\n",
    "    if test_loss/num_test_batches < best_test_loss:\n",
    "        best_model = copy.deepcopy(model).cpu()\n",
    "        best_test_loss = test_loss/num_test_batches\n",
    "\n",
    "model = best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "model = model.to(device)  # if train finished use this to put back on the GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "model = best_model.to(device)  # if train did not finish use this to take the best intermediate result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Save a model\n",
    "torch.save(model.state_dict(), \"TrainedModels/Autoencoder_ID_just_encode_10_epochs.pt\")\n",
    "# train_dataset.to_pickle(\"TrainedModels/Autoencoder_50_epochs_nst_train_set.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 128\n",
    "model = CVAE(z_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"TrainedModels/Autoencoder_new_6_epochs_all_feas1_feas2.pt\", map_location=device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Reload the training and dataset with the model so we don't test on stuff we trained on\n",
    "train_dataset = pd.read_pickle(\"TrainedModels/Autoencoder_new_6_epochs_all_feas1_feas2_train_set.pk\")\n",
    "\n",
    "train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_pt_df = feas2_pt_data[~feas2_pt_data[\"ptID\"].isin(train_dataset[\"ptID\"])]\n",
    "\n",
    "if not test_pt_df.empty:\n",
    "    test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "    test_dataset = split_to_segments(feas2_ecg_data[feas2_ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])], 2048, 9120, 0.5)\n",
    "    test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "    torch_dataset_test = Dataset(test_dataset)\n",
    "    test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the TSNE for some of the training patients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 101/427 [00:34<01:50,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ptID                                            feature\n",
      "0  tensor(57)  [tensor(-0.3639), tensor(0.7346), tensor(0.462...\n",
      "1  tensor(57)  [tensor(-0.0949), tensor(0.9462), tensor(0.262...\n",
      "2  tensor(24)  [tensor(0.6876), tensor(0.9295), tensor(0.4696...\n",
      "3  tensor(24)  [tensor(0.5577), tensor(0.6108), tensor(0.7485...\n",
      "4  tensor(10)  [tensor(0.2424), tensor(-0.1201), tensor(-0.45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (anchor_sig, pos_sig, neg_sig, anchor_label, neg_label, _) in tqdm(enumerate(train_dataloader), total = len(train_dataset)//64):\n",
    "        if i > 100:\n",
    "            break\n",
    "\n",
    "        if anchor_sig.shape[0] != pos_sig.shape[0] or pos_sig.shape[0] != neg_sig.shape[0]:\n",
    "            # print(\"Not equal number of anchors, positive, and negatives\")\n",
    "            continue\n",
    "\n",
    "        batch_len = anchor_sig.shape[0]\n",
    "\n",
    "        all_signals = torch.concat([anchor_sig, pos_sig, neg_sig], dim=0)\n",
    "        all_signals = torch.unsqueeze(all_signals.to(device), 1).float()\n",
    "\n",
    "        latents = model(all_signals)\n",
    "\n",
    "        for i, l in enumerate(anchor_label):\n",
    "            embeddings.append({\"ptID\": l.detach().cpu(), \"feature\": latents[i, :z_dim].detach().cpu()})\n",
    "            embeddings.append({\"ptID\": l.detach().cpu(), \"feature\": latents[batch_len + i, :z_dim].detach().cpu()})\n",
    "\n",
    "        for i, l in enumerate(neg_label):\n",
    "            embeddings.append({\"ptID\": l.detach().cpu(), \"feature\": latents[2*batch_len + i, :z_dim].detach().cpu()})\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "print(embedding_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting tsne\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "feature_matrix = np.array([np.array(v) for v in embedding_df[\"feature\"].values])\n",
    "pt_ids = [int(v) for v in embedding_df[\"ptID\"].values]\n",
    "\n",
    "patients = np.unique(pt_ids)\n",
    "\n",
    "print(\"starting tsne\")\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=30)\n",
    "\n",
    "X_embedded = tsne.fit_transform(feature_matrix)\n",
    "\n",
    "for p in patients:\n",
    "    plt.scatter(X_embedded[pt_ids == p, 0], X_embedded[pt_ids == p, 1], marker=\"x\", label=p)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now try classifying the test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier = classifier.fit(feature_matrix, pt_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 101/106 [00:13<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ptID                                            feature\n",
      "0   tensor(1)  [tensor(0.3270), tensor(0.0010), tensor(1.5780...\n",
      "1   tensor(1)  [tensor(-0.5078), tensor(0.1782), tensor(1.598...\n",
      "2  tensor(41)  [tensor(-0.1397), tensor(0.4097), tensor(1.332...\n",
      "3  tensor(41)  [tensor(0.5691), tensor(0.2803), tensor(1.3708...\n",
      "4   tensor(6)  [tensor(-0.1465), tensor(0.8696), tensor(0.393...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (anchor_sig, pos_sig, neg_sig, anchor_label, neg_label, _) in tqdm(enumerate(test_dataloader), total = len(test_dataset)//64):\n",
    "\n",
    "        if anchor_sig.shape[0] != pos_sig.shape[0] or pos_sig.shape[0] != neg_sig.shape[0]:\n",
    "            # print(\"Not equal number of anchors, positive, and negatives\")\n",
    "            continue\n",
    "\n",
    "        batch_len = anchor_sig.shape[0]\n",
    "\n",
    "        all_signals = torch.concat([anchor_sig, pos_sig, neg_sig], dim=0)\n",
    "        all_signals = torch.unsqueeze(all_signals.to(device), 1).float()\n",
    "\n",
    "        latents = model(all_signals)\n",
    "\n",
    "        for i, l in enumerate(anchor_label):\n",
    "            test_embeddings.append({\"ptID\": l.detach().cpu(), \"feature\": latents[i, :z_dim].detach().cpu()})\n",
    "            test_embeddings.append({\"ptID\": l.detach().cpu(), \"feature\": latents[batch_len + i, :z_dim].detach().cpu()})\n",
    "\n",
    "        for i, l in enumerate(neg_label):\n",
    "            test_embeddings.append({\"ptID\": l.detach().cpu(), \"feature\": latents[2*batch_len + i, :z_dim].detach().cpu()})\n",
    "\n",
    "test_embedding_df = pd.DataFrame(test_embeddings)\n",
    "print(test_embedding_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "test_matrix = np.array([np.array(v) for v in test_embedding_df[\"feature\"].values])\n",
    "targets = [int(v) for v in test_embedding_df[\"ptID\"].values]\n",
    "\n",
    "prediction = classifier.predict(test_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8509694719471947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(targets, prediction)\n",
    "\n",
    "accuracy = np.trace(conf_mat) / np.sum(conf_mat)\n",
    "accuracy_per_person = np.diag(conf_mat) / np.sum(conf_mat, axis=1)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy}\")\n",
    "\n",
    "plt.plot(accuracy_per_person)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185, 133], [73, 19001]]\n"
     ]
    }
   ],
   "source": [
    "best_patient = np.argmin(accuracy_per_person)\n",
    "best_patient_conf_mat = [[conf_mat[best_patient, best_patient], np.sum(conf_mat[best_patient, best_patient+1:]) + np.sum(conf_mat[best_patient, :best_patient])],\n",
    "                         [np.sum(conf_mat[best_patient+1:, best_patient]) + np.sum(conf_mat[:best_patient, best_patient]),\n",
    "                          np.sum(conf_mat[:best_patient, :best_patient]) + np.sum(conf_mat[:best_patient, best_patient+1:]) + np.sum(conf_mat[best_patient+1:, :best_patient]) + np.sum(conf_mat[best_patient+1:, best_patient+1:])]]\n",
    "\n",
    "print(best_patient_conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For comparison try with a nearest neighbour classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nn_classify = KNeighborsClassifier()\n",
    "\n",
    "nn_classify.fit(feature_matrix, pt_ids)\n",
    "prediction = nn_classify.predict(test_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7937809405940595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(targets, prediction)\n",
    "\n",
    "accuracy = np.trace(conf_mat) / np.sum(conf_mat)\n",
    "accuracy_per_person = np.diag(conf_mat) / np.sum(conf_mat, axis=1)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy}\")\n",
    "\n",
    "plt.plot(accuracy_per_person)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[172, 172], [110, 18938]]\n"
     ]
    }
   ],
   "source": [
    "best_patient = np.argmin(accuracy_per_person)\n",
    "best_patient_conf_mat = [[conf_mat[best_patient, best_patient], np.sum(conf_mat[best_patient, best_patient+1:]) + np.sum(conf_mat[best_patient, :best_patient])],\n",
    "                         [np.sum(conf_mat[best_patient+1:, best_patient]) + np.sum(conf_mat[:best_patient, best_patient]),\n",
    "                          np.sum(conf_mat[:best_patient, :best_patient]) + np.sum(conf_mat[:best_patient, best_patient+1:]) + np.sum(conf_mat[best_patient+1:, :best_patient]) + np.sum(conf_mat[best_patient+1:, best_patient+1:])]]\n",
    "\n",
    "print(best_patient_conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reconstruction for clean samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# Plot test data reconstruction\n",
    "test_dataset[\"reconstruction\"] = None\n",
    "mse_only_loss = lambda truth, pred: torch.mean((truth - pred) ** 2, dim=(1,2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    r_err = []\n",
    "    inds = []\n",
    "    reconstructions = []\n",
    "\n",
    "    for i, (signals, _, ind) in enumerate(test_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # signals_clean = torch.unsqueeze(signals_clean.to(device), 1).float()\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        output, latents  = model(-signals)\n",
    "        loss = mse_only_loss(output, signals).detach().cpu().numpy()\n",
    "\n",
    "        output = output.detach().cpu().numpy()\n",
    "\n",
    "        for i, o, l in zip(ind, output[:, 0, :], loss):\n",
    "            r_err.append(l)\n",
    "            reconstructions.append(o)\n",
    "            inds.append(int(i))\n",
    "\n",
    "\n",
    "test_dataset[\"r_err\"] = pd.Series(data=r_err, index=inds)\n",
    "test_dataset[\"reconstruction\"] = pd.Series(data=reconstructions, index=inds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "test_df = test_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "   index  ptID   age         ptDiag     ptDiagRev1          ptDiagRev2  \\\n0    590     8  72.5  DiagEnum.NoAF  DiagEnum.NoAF  DiagEnum.Undecided   \n1    590     8  72.5  DiagEnum.NoAF  DiagEnum.NoAF  DiagEnum.Undecided   \n2    590     8  72.5  DiagEnum.NoAF  DiagEnum.NoAF  DiagEnum.Undecided   \n3    590     8  72.5  DiagEnum.NoAF  DiagEnum.NoAF  DiagEnum.Undecided   \n4    590     8  72.5  DiagEnum.NoAF  DiagEnum.NoAF  DiagEnum.Undecided   \n\n           ptDiagRev3  cardRev            measDiag        measDiagRev1  ...  \\\n0  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided  ...   \n1  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided  ...   \n2  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided  ...   \n3  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided  ...   \n4  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided  ...   \n\n  perhapsAF  measID                                               data  \\\n0         0     591  [-0.49644204545732973, -0.6246603189506869, -0...   \n1         0     591  [0.6110765438273132, 0.6934087028050593, 0.796...   \n2         0     591  [4.487455908826463, 5.457356318373902, 6.04539...   \n3         0     591  [-0.19036774215761504, -0.13728679542462185, -...   \n4         0     591  [-0.2725175881175217, -0.3385749643142289, -0....   \n\n                    file_path  class_index  length  rec_ind  rec_pos  \\\n0  ECGs/000000/saferF2_000591            0    9120      590        0   \n1  ECGs/000000/saferF2_000591            0    9120      590        1   \n2  ECGs/000000/saferF2_000591            0    9120      590        2   \n3  ECGs/000000/saferF2_000591            0    9120      590        3   \n4  ECGs/000000/saferF2_000591            0    9120      590        4   \n\n                                      reconstruction     r_err  \n0  [0.27881688, 0.18085302, 0.30135077, 0.2448078...  1.414751  \n1  [0.2756654, 0.16722995, 0.3430887, 0.32528114,...  1.502876  \n2  [0.28695422, 0.1368554, 0.22762185, 0.20471534...  1.426323  \n3  [0.34878796, 0.11950919, 0.31936407, 0.1697063...  1.354892  \n4  [0.373186, 0.29026645, 0.4235611, 0.34632695, ...  1.427138  \n\n[5 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>ptID</th>\n      <th>age</th>\n      <th>ptDiag</th>\n      <th>ptDiagRev1</th>\n      <th>ptDiagRev2</th>\n      <th>ptDiagRev3</th>\n      <th>cardRev</th>\n      <th>measDiag</th>\n      <th>measDiagRev1</th>\n      <th>...</th>\n      <th>perhapsAF</th>\n      <th>measID</th>\n      <th>data</th>\n      <th>file_path</th>\n      <th>class_index</th>\n      <th>length</th>\n      <th>rec_ind</th>\n      <th>rec_pos</th>\n      <th>reconstruction</th>\n      <th>r_err</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>590</td>\n      <td>8</td>\n      <td>72.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>591</td>\n      <td>[-0.49644204545732973, -0.6246603189506869, -0...</td>\n      <td>ECGs/000000/saferF2_000591</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>590</td>\n      <td>0</td>\n      <td>[0.27881688, 0.18085302, 0.30135077, 0.2448078...</td>\n      <td>1.414751</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>590</td>\n      <td>8</td>\n      <td>72.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>591</td>\n      <td>[0.6110765438273132, 0.6934087028050593, 0.796...</td>\n      <td>ECGs/000000/saferF2_000591</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>590</td>\n      <td>1</td>\n      <td>[0.2756654, 0.16722995, 0.3430887, 0.32528114,...</td>\n      <td>1.502876</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>590</td>\n      <td>8</td>\n      <td>72.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>591</td>\n      <td>[4.487455908826463, 5.457356318373902, 6.04539...</td>\n      <td>ECGs/000000/saferF2_000591</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>590</td>\n      <td>2</td>\n      <td>[0.28695422, 0.1368554, 0.22762185, 0.20471534...</td>\n      <td>1.426323</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>590</td>\n      <td>8</td>\n      <td>72.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>591</td>\n      <td>[-0.19036774215761504, -0.13728679542462185, -...</td>\n      <td>ECGs/000000/saferF2_000591</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>590</td>\n      <td>3</td>\n      <td>[0.34878796, 0.11950919, 0.31936407, 0.1697063...</td>\n      <td>1.354892</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>590</td>\n      <td>8</td>\n      <td>72.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>591</td>\n      <td>[-0.2725175881175217, -0.3385749643142289, -0....</td>\n      <td>ECGs/000000/saferF2_000591</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>590</td>\n      <td>4</td>\n      <td>[0.373186, 0.29026645, 0.4235611, 0.34632695, ...</td>\n      <td>1.427138</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  43 columns</p>\n</div>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "nan"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"reconstruction\"].iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.565233\n",
      "Name: 637, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.475245\n",
      "Name: 638, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.915962\n",
      "Name: 639, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.953902\n",
      "Name: 640, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.354805\n",
      "Name: 641, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                          1.55388\n",
      "Name: 642, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.441283\n",
      "Name: 643, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.345248\n",
      "Name: 1750, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.557865\n",
      "Name: 1751, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.727465\n",
      "Name: 1752, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                         1.311485\n",
      "Name: 1753, dtype: object\n",
      "ptDiag                   DiagEnum.NoAF\n",
      "measDiag                 DiagEnum.NoAF\n",
      "tag_orig_Poor_Quality                0\n",
      "poss_AF_tag                          1\n",
      "r_err                          1.25387\n",
      "Name: 1754, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [65], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, ecg \u001B[38;5;129;01min\u001B[39;00m test_df[test_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasDiag\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m DiagEnum\u001B[38;5;241m.\u001B[39mNoAF]\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;66;03m# print(ecg)\u001B[39;00m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28mprint\u001B[39m(ecg[[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mptDiag\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasDiag\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtag_orig_Poor_Quality\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mposs_AF_tag\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr_err\u001B[39m\u001B[38;5;124m\"\u001B[39m]])\n\u001B[1;32m---> 41\u001B[0m     \u001B[43mplot_ecg_and_reconstruction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mecg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mecg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreconstruction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [65], line 36\u001B[0m, in \u001B[0;36mplot_ecg_and_reconstruction\u001B[1;34m(x, r, fs, n_split)\u001B[0m\n\u001B[0;32m     33\u001B[0m     ax[j][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mgrid(which\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmajor\u001B[39m\u001B[38;5;124m'\u001B[39m, linestyle\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m, linewidth\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0.5\u001B[39m\u001B[38;5;124m'\u001B[39m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblack\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     34\u001B[0m     ax[j][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mgrid(which\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminor\u001B[39m\u001B[38;5;124m'\u001B[39m, linestyle\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m, linewidth\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0.5\u001B[39m\u001B[38;5;124m'\u001B[39m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlightgray\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "def plot_ecg_and_reconstruction(x, r, fs=300, n_split=3):\n",
    "    sample_len = x.shape[0]\n",
    "    time_axis = np.arange(sample_len)/fs\n",
    "\n",
    "    cuts = np.round(np.linspace(0, sample_len-1, n_split+1)).astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots(n_split, 1, figsize=(16, 10), squeeze=False)\n",
    "    for j in range(n_split):\n",
    "        ax[j][0].plot(time_axis[cuts[j]:cuts[j+1]], x[cuts[j]:cuts[j+1]])\n",
    "        ax[j][0].plot(time_axis[cuts[j]:cuts[j+1]], r[cuts[j]:cuts[j+1]])\n",
    "        ax[j][0].set_xlabel(\"Time\")\n",
    "        ax[j][0].set_xlim((time_axis[cuts[j]], time_axis[cuts[j+1]]))\n",
    "\n",
    "        t_s = time_axis[cuts[j]]\n",
    "        t_f = time_axis[cuts[j+1]]\n",
    "        time_ticks = np.arange(t_s - t_s%0.2, t_f + (0.2 - t_f%0.2), 0.2)\n",
    "        decimal_labels = ~np.isclose(time_ticks, np.round(time_ticks))\n",
    "        time_labels = np.round(time_ticks).astype(int).astype(str)\n",
    "        time_labels[decimal_labels] = \"\"\n",
    "\n",
    "        ax[j][0].set_xticks(time_ticks, time_labels)\n",
    "\n",
    "        ax[j][0].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax[j][0].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "        ax[j][0].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax[j][0].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "        ax[j][0].grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "        ax[j][0].grid(which='minor', linestyle='-', linewidth='0.5', color='lightgray')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for _, ecg in test_df[test_df[\"measDiag\"] == DiagEnum.NoAF].iterrows():\n",
    "    # print(ecg)\n",
    "    print(ecg[[\"ptDiag\", \"measDiag\", \"tag_orig_Poor_Quality\", \"poss_AF_tag\", \"r_err\"]])\n",
    "    plot_ecg_and_reconstruction(ecg[\"data\"], -ecg[\"reconstruction\"], n_split=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def plot_ecg_and_reconstruction_for_classes(xs, rs, titles, fs=300):\n",
    "    fig, ax = plt.subplots(len(xs), 1, figsize=(6, 7))\n",
    "\n",
    "    for j, (x, r, t) in enumerate(zip(xs, rs, titles)):\n",
    "        sample_len = x.shape[0]\n",
    "        time_axis = np.arange(sample_len)/fs\n",
    "\n",
    "        ax[j].plot(time_axis, x)\n",
    "        ax[j].plot(time_axis, r)\n",
    "        ax[j].set_xlabel(\"Time\")\n",
    "        ax[j].set_xlim((time_axis[0], time_axis[-1]))\n",
    "\n",
    "        ax[j].set_xticks(np.arange(time_axis[0], time_axis[-1]+0.2,0.2))\n",
    "        ax[j].set_title(t)\n",
    "\n",
    "        ax[j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax[j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "        ax[j].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax[j].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "        ax[j].grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "        ax[j].grid(which='minor', linestyle='-', linewidth='0.5', color='lightgray')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig(\"TMRFigures/cvae_reconst_examples_large_dataset.png\")\n",
    "\n",
    "ecg_ind_list = [3195, 2916, 1563, 1561]  # 2192 # 441 # 315\n",
    "\n",
    "xs = test_df.loc[ecg_ind_list][\"data\"].tolist()\n",
    "rs = test_df.loc[ecg_ind_list][\"reconstruction\"].tolist()\n",
    "titles = test_df.loc[ecg_ind_list].apply(lambda x: f\"{x['measDiag'].name} e = {x['r_err']:.3f}\", axis=1)   # [\"measDiag\"].map(lambda x: x.name).tolist()\n",
    "print(len(titles))\n",
    "\n",
    "plot_ecg_and_reconstruction_for_classes(xs, rs, titles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Latent space exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 120)\n"
     ]
    }
   ],
   "source": [
    "# Try some latent space exploration\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (signals, _, _) in enumerate(test_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        latent_position = model.encode(signals)\n",
    "        latent_position = latent_position.detach().cpu().numpy()\n",
    "        signals_np = signals.detach().cpu().numpy()\n",
    "\n",
    "        print(latent_position.shape)\n",
    "\n",
    "        break\n",
    "\n",
    "index = 3\n",
    "latent_positions = np.zeros((10, *latent_position.shape), dtype=np.float32)\n",
    "for i in range(10):\n",
    "    latent_positions[i, :, :] += latent_position\n",
    "    latent_positions[i, :, index] = i * 4 - 2\n",
    "\n",
    "signals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for l in latent_positions:\n",
    "        latent = torch.from_numpy(l[:, :60]).to(device)\n",
    "        signal = model.decode(latent)\n",
    "        signals.append(signal.detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpolate between a noisy and clean ECG!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting\n"
     ]
    }
   ],
   "source": [
    "# noisy 441 # clean 315\n",
    "\n",
    "noisy_latent = test_dataset.loc[441][\"latent_encoding\"][:60]\n",
    "clean_latent = test_dataset.loc[315][\"latent_encoding\"][:60]\n",
    "\n",
    "latent_sequence = np.linspace(noisy_latent, clean_latent, 32)\n",
    "latent_sequence = torch.from_numpy(latent_sequence).to(device)\n",
    "\n",
    "ecgs = model.decode(latent_sequence).detach().cpu().numpy()\n",
    "print(\"plotting\")\n",
    "\n",
    "for ecg in np.flip(ecgs[:, 0, :], axis=0):\n",
    "    plt.plot(ecg)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find the reconstruction error for noisy and clean samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "            r_err            measDiag\nrec_ind                              \n590      1.461073  DiagEnum.Undecided\n591      1.734338  DiagEnum.Undecided\n592      1.272624  DiagEnum.Undecided\n593      1.588059  DiagEnum.Undecided\n594      1.382735  DiagEnum.Undecided\n...           ...                 ...\n23182    1.404204  DiagEnum.Undecided\n23183    1.498458  DiagEnum.Undecided\n23184    1.509076  DiagEnum.Undecided\n23185    1.468974  DiagEnum.Undecided\n23186    1.429129  DiagEnum.Undecided\n\n[4843 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r_err</th>\n      <th>measDiag</th>\n    </tr>\n    <tr>\n      <th>rec_ind</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>590</th>\n      <td>1.461073</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>1.734338</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>1.272624</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>1.588059</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>1.382735</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23182</th>\n      <td>1.404204</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>23183</th>\n      <td>1.498458</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>23184</th>\n      <td>1.509076</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>23185</th>\n      <td>1.468974</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n    <tr>\n      <th>23186</th>\n      <td>1.429129</td>\n      <td>DiagEnum.Undecided</td>\n    </tr>\n  </tbody>\n</table>\n<p>4843 rows  2 columns</p>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For NST data\n",
    "# test_whole_ecgs_rec_err = test_dataset.groupby(\"rec_ind\").agg({\"r_err\": \"mean\", \"noise_level\": lambda x: x.iloc[0]})\n",
    "# For safer data\n",
    "test_whole_ecgs_rec_err = test_dataset.groupby(\"rec_ind\").agg({\"r_err\": \"mean\" ,\"measDiag\": lambda x: x.iloc[0]})\n",
    "test_whole_ecgs_rec_err"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "# A little bit of faff to plot the results from small and large datasets on one axis, for the TMR, not sure it made it into the report in the end\n",
    "test_whole_ecgs_rec_err.to_pickle(\"TrainedModels/Autoecoder_small_dataset.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'measDiag'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3799\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'measDiag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [41], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m test_not_undecided \u001B[38;5;241m=\u001B[39m test_whole_ecgs_rec_err\n\u001B[0;32m      3\u001B[0m fig \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m4\u001B[39m), dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m300\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m plt\u001B[38;5;241m.\u001B[39mscatter(\u001B[43mtest_not_undecided\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmeasDiag\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mvalue), test_not_undecided[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr_err\u001B[39m\u001B[38;5;124m\"\u001B[39m], marker\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m+\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mxticks([e\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m pd\u001B[38;5;241m.\u001B[39munique(test_not_undecided[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasDiag\u001B[39m\u001B[38;5;124m\"\u001B[39m])], [e\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m pd\u001B[38;5;241m.\u001B[39munique(test_not_undecided[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasDiag\u001B[39m\u001B[38;5;124m\"\u001B[39m])])\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReconstruction error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3804\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3805\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3807\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3804\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'measDiag'"
     ]
    }
   ],
   "source": [
    "test_not_undecided = test_whole_ecgs_rec_err\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.scatter(test_not_undecided[\"measDiag\"].map(lambda x: x.value), test_not_undecided[\"r_err\"], marker='+')\n",
    "plt.xticks([e.value for e in pd.unique(test_not_undecided[\"measDiag\"])], [e.name for e in pd.unique(test_not_undecided[\"measDiag\"])])\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Measurement diagnosis\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"TMRFigures/cvae_reconst_err_large_dataset.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "DiagEnum.Undecided      4569\nDiagEnum.NoAF            172\nDiagEnum.PoorQuality      99\nDiagEnum.AF                3\nName: measDiag, dtype: int64"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_whole_ecgs_rec_err[\"measDiag\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Safer data\n",
    "\n",
    "test_not_undecided = test_whole_ecgs_rec_err\n",
    "# test_not_undecided_2 = pd.read_pickle(\"TrainedModels/Autoecoder_large_dataset.pk\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "\n",
    "enum_order = [DiagEnum.NoAF, DiagEnum.AF, DiagEnum.PoorQuality]\n",
    "data = [test_not_undecided[test_not_undecided[\"measDiag\"] == e][\"r_err\"] for e in enum_order]\n",
    "\n",
    "# print(test_not_undecided_2[\"measDiag\"].value_counts())\n",
    "# data_2 = [test_not_undecided_2[test_not_undecided_2[\"measDiag\"] == e][\"r_err\"] for e in enum_order]\n",
    "\n",
    "plt.violinplot(data)    # quantiles=[[0.25, 0.75]]*4, showmedians=True)\n",
    "# plt.violinplot(data_2)\n",
    "plt.xticks([1, 2, 3], [e.name for e in enum_order])\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Measurement diagnosis\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"TMRFigures/cvae_reconst_err_small_dataset.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '06' '12' '18' '_6']\n"
     ]
    }
   ],
   "source": [
    "# NST data\n",
    "\n",
    "test_not_undecided = test_whole_ecgs_rec_err\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "\n",
    "noise_levels = np.sort(pd.unique(test_not_undecided[\"noise_level\"]))\n",
    "print(noise_levels)\n",
    "data = [test_not_undecided[test_not_undecided[\"noise_level\"] == e][\"r_err\"] for e in noise_levels]\n",
    "\n",
    "\n",
    "plt.violinplot(data)    # quantiles=[[0.25, 0.75]]*4, showmedians=True)\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Measurement diagnosis\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"TMRFigures/cvae_reconst_err_small_dataset.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample cross validation code for SAFER (not yet applied to anything)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification from the latent space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    inds = []\n",
    "\n",
    "    for i, (signals, _, ind) in enumerate(train_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        latent_position = model.encode(signals)\n",
    "        latent_position = latent_position.detach().cpu().numpy()\n",
    "        signals_np = signals.detach().cpu().numpy()\n",
    "\n",
    "        for i, l in zip(ind, latent_position):\n",
    "            latents.append(l)\n",
    "            inds.append(i)\n",
    "\n",
    "train_dataset[\"latent_encoding\"] = pd.Series(data=latents, index=inds)\n",
    "svc_train_df = train_dataset.dropna(subset=[\"latent_encoding\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "        index  ptID   age         ptDiag          ptDiagRev1     ptDiagRev2  \\\n0           0     1  79.0  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n1           0     1  79.0  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n2           0     1  79.0  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n3           0     1  79.0  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n4           0     1  79.0  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n...       ...   ...   ...            ...                 ...            ...   \n128907  23258   288  70.5  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n128908  23258   288  70.5  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n128909  23258   288  70.5  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n128910  23258   288  70.5  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n128911  23258   288  70.5  DiagEnum.NoAF  DiagEnum.Undecided  DiagEnum.NoAF   \n\n                ptDiagRev3  cardRev            measDiag        measDiagRev1  \\\n0       DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n1       DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n2       DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n3       DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n4       DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n...                    ...      ...                 ...                 ...   \n128907  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n128908  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n128909  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n128910  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n128911  DiagEnum.Undecided        1  DiagEnum.Undecided  DiagEnum.Undecided   \n\n        ... unlikelyAF  perhapsAF measID  \\\n0       ...          0          0      1   \n1       ...          0          0      1   \n2       ...          0          0      1   \n3       ...          0          0      1   \n4       ...          0          0      1   \n...     ...        ...        ...    ...   \n128907  ...          0          0  23259   \n128908  ...          0          0  23259   \n128909  ...          0          0  23259   \n128910  ...          0          0  23259   \n128911  ...          0          0  23259   \n\n                                                     data  \\\n0       [-0.25368181611599944, -0.45841503541331713, -...   \n1       [0.2026567814899965, 0.184902954707474, 0.1763...   \n2       [0.0664598504863543, 0.0779387026676995, 0.087...   \n3       [1.3574057452010926, 1.0019610647315154, 0.645...   \n4       [-0.025626811413227674, -0.038046522783888745,...   \n...                                                   ...   \n128907  [-0.801575410206255, -0.7448302284482392, -0.6...   \n128908  [-0.9279949051780753, -0.9494823052111254, -0....   \n128909  [0.4556109704687446, 0.3097769511612076, 0.185...   \n128910  [0.21881221826302832, 0.4723944748082791, 0.78...   \n128911  [-0.09619865014660389, -0.12374135799916317, -...   \n\n                         file_path  class_index  length  rec_ind  rec_pos  \\\n0       ECGs/000000/saferF2_000001            0    9120        0        0   \n1       ECGs/000000/saferF2_000001            0    9120        0        1   \n2       ECGs/000000/saferF2_000001            0    9120        0        2   \n3       ECGs/000000/saferF2_000001            0    9120        0        3   \n4       ECGs/000000/saferF2_000001            0    9120        0        4   \n...                            ...          ...     ...      ...      ...   \n128907  ECGs/023000/saferF2_023259            0    9120    23258        2   \n128908  ECGs/023000/saferF2_023259            0    9120    23258        3   \n128909  ECGs/023000/saferF2_023259            0    9120    23258        4   \n128910  ECGs/023000/saferF2_023259            0    9120    23258        5   \n128911  ECGs/023000/saferF2_023259            0    9120    23258        6   \n\n        latent_encoding  \n0                   NaN  \n1                   NaN  \n2                   NaN  \n3                   NaN  \n4                   NaN  \n...                 ...  \n128907              NaN  \n128908              NaN  \n128909              NaN  \n128910              NaN  \n128911              NaN  \n\n[128912 rows x 42 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>ptID</th>\n      <th>age</th>\n      <th>ptDiag</th>\n      <th>ptDiagRev1</th>\n      <th>ptDiagRev2</th>\n      <th>ptDiagRev3</th>\n      <th>cardRev</th>\n      <th>measDiag</th>\n      <th>measDiagRev1</th>\n      <th>...</th>\n      <th>unlikelyAF</th>\n      <th>perhapsAF</th>\n      <th>measID</th>\n      <th>data</th>\n      <th>file_path</th>\n      <th>class_index</th>\n      <th>length</th>\n      <th>rec_ind</th>\n      <th>rec_pos</th>\n      <th>latent_encoding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>79.0</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[-0.25368181611599944, -0.45841503541331713, -...</td>\n      <td>ECGs/000000/saferF2_000001</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>79.0</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[0.2026567814899965, 0.184902954707474, 0.1763...</td>\n      <td>ECGs/000000/saferF2_000001</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>79.0</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[0.0664598504863543, 0.0779387026676995, 0.087...</td>\n      <td>ECGs/000000/saferF2_000001</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>0</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>79.0</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[1.3574057452010926, 1.0019610647315154, 0.645...</td>\n      <td>ECGs/000000/saferF2_000001</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>0</td>\n      <td>3</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>79.0</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[-0.025626811413227674, -0.038046522783888745,...</td>\n      <td>ECGs/000000/saferF2_000001</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>0</td>\n      <td>4</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>128907</th>\n      <td>23258</td>\n      <td>288</td>\n      <td>70.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23259</td>\n      <td>[-0.801575410206255, -0.7448302284482392, -0.6...</td>\n      <td>ECGs/023000/saferF2_023259</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>23258</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>128908</th>\n      <td>23258</td>\n      <td>288</td>\n      <td>70.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23259</td>\n      <td>[-0.9279949051780753, -0.9494823052111254, -0....</td>\n      <td>ECGs/023000/saferF2_023259</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>23258</td>\n      <td>3</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>128909</th>\n      <td>23258</td>\n      <td>288</td>\n      <td>70.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23259</td>\n      <td>[0.4556109704687446, 0.3097769511612076, 0.185...</td>\n      <td>ECGs/023000/saferF2_023259</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>23258</td>\n      <td>4</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>128910</th>\n      <td>23258</td>\n      <td>288</td>\n      <td>70.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23259</td>\n      <td>[0.21881221826302832, 0.4723944748082791, 0.78...</td>\n      <td>ECGs/023000/saferF2_023259</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>23258</td>\n      <td>5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>128911</th>\n      <td>23258</td>\n      <td>288</td>\n      <td>70.5</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.NoAF</td>\n      <td>DiagEnum.Undecided</td>\n      <td>1</td>\n      <td>DiagEnum.Undecided</td>\n      <td>DiagEnum.Undecided</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23259</td>\n      <td>[-0.09619865014660389, -0.12374135799916317, -...</td>\n      <td>ECGs/023000/saferF2_023259</td>\n      <td>0</td>\n      <td>9120</td>\n      <td>23258</td>\n      <td>6</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>128912 rows  42 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_train_df = svc_train_df[svc_train_df[\"measDiag\"] != DiagEnum.Undecided]\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualise the data with scatter plots and T-SNE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=60, step=1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [69], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m     plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatent mean \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m     plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatent mean 0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m     \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mx_clean = latent_matrix[:, latent_inds[0]]\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03my_clean = latent_matrix[:, latent_inds[1]]\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03mplt.show()\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "latent_list = list(svc_train_df[\"latent_encoding\"].map(lambda x: x[:60].tolist()).values)\n",
    "latent_df = pd.DataFrame(latent_list, index=svc_train_df.index)\n",
    "print(latent_df.columns)\n",
    "\n",
    "latent_ind = 0\n",
    "\n",
    "# scatter plot\n",
    "for i in range(60):\n",
    "    plt.figure(figsize=(6, 4), dpi=300)\n",
    "    for d in [DiagEnum.NoAF, DiagEnum.PoorQuality, DiagEnum.AF, DiagEnum.CannotExcludePathology]:\n",
    "        plt.scatter(latent_df[svc_train_df[\"measDiag\"] == d][0], latent_df[svc_train_df[\"measDiag\"] == d][i], marker=\"x\", label=d.name)\n",
    "    plt.legend()\n",
    "    plt.ylabel(f\"latent mean {i}\")\n",
    "    plt.xlabel(f\"latent mean 0\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting tsne\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [74], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstarting tsne\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m tsne \u001B[38;5;241m=\u001B[39m TSNE(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m, init\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom\u001B[39m\u001B[38;5;124m'\u001B[39m, perplexity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m X_embedded \u001B[38;5;241m=\u001B[39m \u001B[43mtsne\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_matrix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m [DiagEnum\u001B[38;5;241m.\u001B[39mNoAF, DiagEnum\u001B[38;5;241m.\u001B[39mPoorQuality, DiagEnum\u001B[38;5;241m.\u001B[39mAF, DiagEnum\u001B[38;5;241m.\u001B[39mCannotExcludePathology]:\n\u001B[0;32m     12\u001B[0m     plt\u001B[38;5;241m.\u001B[39mscatter(X_embedded[latent_classes \u001B[38;5;241m==\u001B[39m d, \u001B[38;5;241m0\u001B[39m], X_embedded[latent_classes \u001B[38;5;241m==\u001B[39m d, \u001B[38;5;241m1\u001B[39m], marker\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m, label\u001B[38;5;241m=\u001B[39md\u001B[38;5;241m.\u001B[39mname)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1122\u001B[0m, in \u001B[0;36mTSNE.fit_transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m \n\u001B[0;32m   1105\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1120\u001B[0m \u001B[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001B[39;00m\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1122\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_params_vs_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1123\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X)\n\u001B[0;32m   1124\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_ \u001B[38;5;241m=\u001B[39m embedding\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793\u001B[0m, in \u001B[0;36mTSNE._check_params_vs_input\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    791\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_params_vs_input\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperplexity \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m--> 793\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mperplexity must be less than n_samples\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "latent_matrix = np.array(list(svc_train_df[\"latent_encoding\"].map(lambda x: x[:60].tolist()).values))\n",
    "latent_classes = svc_train_df[\"measDiag\"].values\n",
    "\n",
    "print(\"starting tsne\")\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=30)\n",
    "\n",
    "X_embedded = tsne.fit_transform(latent_matrix)\n",
    "\n",
    "for d in [DiagEnum.NoAF, DiagEnum.PoorQuality, DiagEnum.AF, DiagEnum.CannotExcludePathology]:\n",
    "    plt.scatter(X_embedded[latent_classes == d, 0], X_embedded[latent_classes == d, 1], marker=\"x\", label=d.name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Group all the segments together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(420,)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_train_df = svc_train_df[svc_train_df[\"measDiag\"] != DiagEnum.Undecided]\n",
    "\n",
    "def concatenate_means(x):\n",
    "    mean_series = x.map(lambda x: x[:60])\n",
    "    return np.concatenate(mean_series.tolist())\n",
    "\n",
    "full_ecg_train_df = svc_train_df.groupby(\"rec_ind\").agg({\"latent_encoding\": concatenate_means, \"measDiag\": lambda x: x.iloc[0], \"class_index\": lambda x: x.iloc[0]})\n",
    "full_ecg_train_df.iloc[0][\"latent_encoding\"].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting tsne\n"
     ]
    }
   ],
   "source": [
    "# Try a T-SNE now all the segments are together\n",
    "\n",
    "latent_matrix = np.array(list(full_ecg_train_df[\"latent_encoding\"].map(lambda x: x.tolist()).values))\n",
    "latent_classes = full_ecg_train_df[\"measDiag\"].values\n",
    "\n",
    "print(\"starting tsne\")\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=30)\n",
    "\n",
    "X_embedded = tsne.fit_transform(latent_matrix)\n",
    "\n",
    "for d in [DiagEnum.NoAF, DiagEnum.PoorQuality, DiagEnum.AF, DiagEnum.CannotExcludePathology]:\n",
    "    plt.scatter(X_embedded[latent_classes == d, 0], X_embedded[latent_classes == d, 1], marker=\"x\", label=d.name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(655, 420)\n"
     ]
    }
   ],
   "source": [
    "train_matrix = np.vstack(full_ecg_train_df[\"latent_encoding\"].values)\n",
    "targets =  np.array(full_ecg_train_df[\"class_index\"].astype(int).values)\n",
    "\n",
    "print(train_matrix.shape)\n",
    "\n",
    "# class weightings?\n",
    "classifier = SVC()\n",
    "classifier = classifier.fit(train_matrix, targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "test_dataset[\"latent_encoding\"] = None\n",
    "inds = []\n",
    "latents = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (signals, _,  ind) in enumerate(test_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        latent_position = model.encode(signals)\n",
    "        latent_position = latent_position.detach().cpu().numpy()\n",
    "\n",
    "        for i, l in zip(ind, latent_position):\n",
    "            inds.append(int(i))\n",
    "            latents.append(l)\n",
    "\n",
    "test_dataset[\"latent_encoding\"] = pd.Series(data=latents, index=inds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 420)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_71904\\3497999537.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_ecg_no_undecided_test_df[\"prediction\"] = prediction\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "full_ecg_test_df = test_dataset.groupby(\"rec_ind\").agg({\"latent_encoding\": concatenate_means, \"measDiag\": lambda x: x.iloc[0], \"class_index\": lambda x: x.iloc[0], \"measID\": lambda x: x.iloc[0]})\n",
    "full_ecg_no_undecided_test_df = full_ecg_test_df[full_ecg_test_df[\"measDiag\"] != DiagEnum.Undecided]\n",
    "\n",
    "test_matrix = np.vstack(full_ecg_no_undecided_test_df[\"latent_encoding\"].values)\n",
    "targets =  np.array(full_ecg_no_undecided_test_df[\"class_index\"].astype(int).values)\n",
    "print(test_matrix.shape)\n",
    "\n",
    "prediction = classifier.predict(test_matrix)\n",
    "\n",
    "full_ecg_no_undecided_test_df[\"prediction\"] = prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280 142]\n",
      " [ 41 123]]\n",
      "Sensitivity: 0.75\n",
      "Specificity: 0.6635071090047393\n",
      "Normal F1: 0.7537012113055181\n",
      "Noisy F1: 0.5734265734265734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(full_ecg_no_undecided_test_df[\"class_index\"].astype(int), full_ecg_no_undecided_test_df[\"prediction\"].astype(int))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1])}\")\n",
    "print(f\"Specificity: {conf_mat[0, 0]/np.sum(conf_mat[0])}\")\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat, 1)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_156864\\2192664923.py:1: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for _, ecg in feas2_ecg_data[feas2_ecg_data[\"measID\"].isin(false_positives[\"measID\"])][\"data\"].iteritems():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [51], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, ecg \u001B[38;5;129;01min\u001B[39;00m feas2_ecg_data[feas2_ecg_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasID\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39misin(false_positives[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasID\u001B[39m\u001B[38;5;124m\"\u001B[39m])][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39miteritems():\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mplot_ecg_and_reconstruction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mecg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mecg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "Cell \u001B[1;32mIn [47], line 36\u001B[0m, in \u001B[0;36mplot_ecg_and_reconstruction\u001B[1;34m(x, r, fs, n_split)\u001B[0m\n\u001B[0;32m     33\u001B[0m     ax[j][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mgrid(which\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmajor\u001B[39m\u001B[38;5;124m'\u001B[39m, linestyle\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m, linewidth\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0.5\u001B[39m\u001B[38;5;124m'\u001B[39m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblack\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     34\u001B[0m     ax[j][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mgrid(which\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminor\u001B[39m\u001B[38;5;124m'\u001B[39m, linestyle\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m, linewidth\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0.5\u001B[39m\u001B[38;5;124m'\u001B[39m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlightgray\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "false_positives = full_ecg_no_undecided_test_df[(full_ecg_no_undecided_test_df[\"class_index\"] == 0) & (full_ecg_no_undecided_test_df[\"prediction\"] == 1)]\n",
    "\n",
    "for _, ecg in feas2_ecg_data[feas2_ecg_data[\"measID\"].isin(false_positives[\"measID\"])][\"data\"].iteritems():\n",
    "    plot_ecg_and_reconstruction(ecg, ecg)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
