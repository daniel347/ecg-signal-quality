{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T19:47:39.329182900Z",
     "start_time": "2023-06-11T19:46:18.713331700Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import scipy.signal\n",
    "from ecgdetectors import Detectors\n",
    "\n",
    "import math\n",
    "from DataHandlers.DiagEnum import DiagEnum\n",
    "import DataHandlers.DiagEnum\n",
    "import DataHandlers.SAFERDataset as SAFERDataset\n",
    "import DataHandlers.CinC2020Dataset as CinC2020Dataset\n",
    "import DataHandlers.CinC2020Enums\n",
    "import importlib\n",
    "import DataHandlers.CinCDataset as CinCDataset\n",
    "import DataHandlers.DataAugmentations as DataAugmentations\n",
    "from multiprocesspandas import applyparallel\n",
    "\n",
    "import DataHandlers.DataProcessUtilities\n",
    "importlib.reload(DataHandlers.DataProcessUtilities)\n",
    "from DataHandlers.DataProcessUtilities import *\n",
    "import Utilities.Plotting\n",
    "importlib.reload(Utilities.Plotting)\n",
    "from Utilities.Plotting import *\n",
    "\n",
    "# A fudge because I moved the files\n",
    "sys.modules[\"SAFERDataset\"] = SAFERDataset\n",
    "sys.modules[\"CinC2020Dataset\"] = CinC2020Dataset\n",
    "sys.modules[\"DiagEnum\"] = DataHandlers.DiagEnum\n",
    "sys.modules[\"CinC2020Enums\"] = DataHandlers.CinC2020Enums\n",
    "sys.modules[\"CinCDataset\"] = CinCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-11T19:47:39.329182900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "enable_cuda = True\n",
    "\n",
    "if torch.cuda.is_available() and enable_cuda:\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAFER data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data, feas2_ecg_data = SAFERDataset.load_feas_dataset(2, \"dataframe_reload\")\n",
    "feas2_ecg_data[\"measID\"] += 300000\n",
    "feas2_ecg_data[\"feas\"] = 2\n",
    "feas2_ecg_data.index = feas2_ecg_data[\"measID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_normals_all_other_af(pt_data, ecg_data):\n",
    "    accepted_meas_diags = [DiagEnum.AF, DiagEnum.NoAF, DiagEnum.HeartBlock]\n",
    "    ecg_data = ecg_data[(ecg_data[\"measDiag\"].isin(accepted_meas_diags)) | (ecg_data[\"measID\"] < 20000) | (ecg_data[\"not_tagged_ign_wide_qrs\"] == 0)]\n",
    "    pt_data = pt_data[pt_data[\"ptID\"].isin(ecg_data[\"ptID\"])]\n",
    "\n",
    "    return pt_data, ecg_data\n",
    "\n",
    "# warning: changing these chunk sizes may reload feas1 data from scratch, which will take ages\n",
    "chunk_size = 20000\n",
    "num_chunks = math.ceil(162515 / chunk_size )\n",
    "\n",
    "def load_feas1_chunk_range(chunk_range=(0, num_chunks)):\n",
    "    ecg_data = []\n",
    "    pt_data = []\n",
    "\n",
    "    for chunk_num in range(chunk_range[0], chunk_range[1]):\n",
    "        feas1_pt_data, feas1_ecg_data = SAFERDataset.load_feas_dataset(1, f\"dataframe_{chunk_num}.pk\")\n",
    "\n",
    "        ecg_data.append(feas1_ecg_data)\n",
    "        pt_data.append(feas1_pt_data)\n",
    "\n",
    "    feas1_ecg_data = pd.concat(ecg_data)\n",
    "    feas1_ecg_data[\"feas\"] = 1\n",
    "    feas1_ecg_data[\"rri_len\"] = feas1_ecg_data[\"rri_feature\"].map(lambda x: x[x > 0].shape[-1])\n",
    "    feas1_pt_data = pd.concat(pt_data).drop_duplicates()\n",
    "\n",
    "    return feas1_ecg_data, feas1_pt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_safer_data(pt_data, ecg_data):\n",
    "    if \"length\" in ecg_data:\n",
    "        ecg_data = ecg_data[ecg_data[\"length\"] == 9120]\n",
    "\n",
    "    ecg_data = ecg_data[ecg_data[\"measDiag\"] != DiagEnum.PoorQuality]\n",
    "    # ecg_data = ecg_data[ecg_data[\"tag_orig_Poor_Quality\"] == 0]\n",
    "\n",
    "    ecg_data = ecg_data[ecg_data[\"rri_len\"] > 5]\n",
    "\n",
    "\n",
    "    pt_data.index = pt_data[\"ptID\"]\n",
    "    ecg_data = SAFERDataset.generate_af_class_labels(ecg_data)\n",
    "    pt_data = SAFERDataset.add_ecg_class_counts(pt_data, ecg_data)\n",
    "\n",
    "    return pt_data, ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_data, feas1_pt_data = load_feas1_chunk_range((0, num_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use feas2\n",
    "safer_ecg_data = feas2_ecg_data\n",
    "safer_ecg_data[\"ffReview_sent\"] = -1\n",
    "safer_ecg_data[\"ffReview_remain\"] = -1\n",
    "safer_pt_data = feas2_pt_data\n",
    "\n",
    "safer_pt_data, safer_ecg_data = prepare_safer_data(safer_pt_data, safer_ecg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use feas1 to prepare test and validation datasets (The train is best handled with a DatasetSequenceIterator)\n",
    "feas1_pt_data, feas1_ecg_data = prepare_safer_data(feas1_pt_data, feas1_ecg_data)\n",
    "feas1_ecg_data[\"class_index\"].value_counts()\n",
    "\n",
    "feas1_ecg_data_test = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(test_pts[\"ptID\"])]\n",
    "feas1_ecg_data_val = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(val_pts[\"ptID\"])]\n",
    "\n",
    "print(feas1_ecg_data_test[\"class_index\"].value_counts())\n",
    "print(feas1_ecg_data_val[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample normals and produce a dataloader\n",
    "feas1_ecg_data_train_norm = feas1_ecg_data[(feas1_ecg_data[\"class_index\"] == 0) & (feas1_ecg_data[\"ptID\"].isin(train_pts[\"ptID\"]))].sample(frac=0.3)\n",
    "feas1_ecg_data_train_not_norm = feas1_ecg_data[(feas1_ecg_data[\"class_index\"] != 0) & (feas1_ecg_data[\"ptID\"].isin(train_pts[\"ptID\"]))]\n",
    "\n",
    "feas1_ecg_data_test_norm = feas1_ecg_data[(feas1_ecg_data[\"class_index\"] == 0) & (feas1_ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]))].sample(frac=0.3)\n",
    "feas1_ecg_data_test_not_norm = feas1_ecg_data[(feas1_ecg_data[\"class_index\"] != 0) & (feas1_ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]))]\n",
    "\n",
    "feas1_ecg_data_test_undersamp = pd.concat([feas1_ecg_data_test_norm, feas1_ecg_data_test_not_norm])\n",
    "feas1_ecg_data_train_undersamp = pd.concat([feas1_ecg_data_train_norm, feas1_ecg_data_train_not_norm])\n",
    "print(feas1_ecg_data_train_undersamp[\"class_index\"].value_counts())\n",
    "\n",
    "feas1_train_dataloader_undersamp = get_dataloaders(feas1_ecg_data_train_undersamp, 64)\n",
    "feas1_test_dataloader_undersamp = get_dataloaders(feas1_ecg_data_test_undersamp, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = r\"C:\\Users\\daniel\\Documents\"\n",
    "\n",
    "feas1_ecg_data_test.to_pickle(os.path.join(doc_path, \"feas1_test_27_mar.pk\"))\n",
    "feas1_ecg_data_val.to_pickle(os.path.join(doc_path, \"feas1_val_27_mar.pk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safer_ecg_data = pd.concat([feas2_ecg_data, feas1_ecg_data])\n",
    "safer_pt_data = pd.concat([feas2_pt_data, feas1_pt_data])\n",
    "\n",
    "safer_pt_data, safer_ecg_data = prepare_safer_data(safer_pt_data, safer_ecg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safer_ecg_data.groupby(\"feas\")[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heartrate histogram for AF and not AF\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=300)\n",
    "ax.hist(safer_ecg_data[\"heartrate\"][(safer_ecg_data[\"measDiag\"] != DiagEnum.AF) & (safer_ecg_data[\"feas\"] == 1)], alpha=0.7, density=True, label=\"Normal or Other Rhythm\")\n",
    "ax.hist(safer_ecg_data[\"heartrate\"][(safer_ecg_data[\"measDiag\"] == DiagEnum.AF) & (safer_ecg_data[\"feas\"] == 1)], alpha=0.7, density=True, label=\"AF\")\n",
    "ax.set_xlabel(\"Heartrate (bpm)\")\n",
    "ax.set_ylabel(\"Frequency proportion\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out high and low heartrates - I dont think this makes a difference so havent been doing it mostly\n",
    "safer_ecg_data = safer_ecg_data[(safer_ecg_data[\"heartrate\"] < 120) & (safer_ecg_data[\"heartrate\"] > 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for _, ecg in safer_ecg_data[safer_ecg_data[\"feas\"] == 1].sample(frac=1).iterrows():\n",
    "    print(ecg[[\"measDiag\", \"class_index\", \"heartrate\", \"r_peaks\"]])\n",
    "    plot_ecg(ecg[\"data\"], r_peaks=ecg[\"r_peaks\"], fs=300, n_split=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 1 feas2 AF example with high heartrate!\n",
    "\n",
    "plot_ecg(safer_ecg_data.loc[310209][\"data\"], r_peaks=safer_ecg_data.loc[310209][\"r_peaks\"], fs=300, n_split=3, figsize=(6, 5), export_quality=True)\n",
    "plot_ecg_spectrogram(safer_ecg_data.loc[310209][\"data\"], fs=300, n_split=3, figsize=(6, 5), export_quality=True, cut_range=(2, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CinC 2020 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataHandlers.CinC2020Dataset as CinC2020Dataset\n",
    "import importlib\n",
    "importlib.reload(CinC2020Dataset)\n",
    "\n",
    "df = CinC2020Dataset.load_dataset(save_name=\"dataframe_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment we only select data with length which can be truncated to 3000 samples (10s)\n",
    "def select_length(df):\n",
    "    df_within_range = df[(df[\"length\"] <= 5000) & (df[\"length\"] >= 3000)].copy()\n",
    "    df_within_range[\"data\"] = df_within_range[\"data\"].map(lambda x: x[:3000])\n",
    "    df_within_range[\"length\"] = df_within_range[\"data\"].map(lambda x: x.shape[0])\n",
    "    return df_within_range\n",
    "\n",
    "df = select_length(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heartrate histogram for AF and not AF\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=300)\n",
    "ax.hist(df[\"heartrate\"][(df[\"measDiag\"] != DiagEnum.AF)], alpha=0.7, density=True, label=\"Normal or Other Rhythm\")\n",
    "ax.hist(df[\"heartrate\"][(df[\"measDiag\"] == DiagEnum.AF)], alpha=0.7, density=True, label=\"AF\")\n",
    "ax.set_xlabel(\"Heartrate (bpm)\")\n",
    "ax.set_ylabel(\"Frequency proportion\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = noise_df.sample()[\"data\"].iloc[0] * np.random.normal(scale=1)\n",
    "\n",
    "for _, ecg in df.iterrows():\n",
    "    noise_scale = np.random.normal(scale=0.2)\n",
    "    noise = noise_df.sample()[\"data\"].iloc[0] * noise_scale\n",
    "    print(noise_scale)\n",
    "    plot_ecg(ecg[\"data\"], figsize=(5, 2), export_quality=True)\n",
    "    plot_ecg(ecg[\"data\"] + noise, figsize=(5, 2), export_quality=True)\n",
    "    plot_ecg(noise, figsize=(5, 2), export_quality=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _, ecg in df[df[\"class_index\"] == 0].iterrows():\n",
    "    plot_ecg(ecg[\"data\"][:1500], figsize=(5, 2.5), export_quality=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"dataset\")[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load noise from MIT database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "noises = [\"em\", \"ma\"]\n",
    "noise_dfs = []\n",
    "mit_dataset_path = \"Datasets/mit-bih-noise-stress-test-database\"\n",
    "\n",
    "f_low = 0.67\n",
    "f_high = 25\n",
    "\n",
    "def split_signal(data, split_len):\n",
    "    data_splits = []\n",
    "    splits = np.arange(0, data[\"data\"].shape[0], split_len)\n",
    "\n",
    "    for i, (start, end) in enumerate(zip(splits, splits[1:])):\n",
    "        data_split = data.copy()\n",
    "        data_split[\"data\"] = data[\"data\"][start:end]\n",
    "        data_split[\"data\"] = (data_split[\"data\"] - data_split[\"data\"].mean())/ data_split[\"data\"].std()\n",
    "\n",
    "        data_split.name = i\n",
    "        data_splits.append(data_split)\n",
    "\n",
    "    return data_splits\n",
    "\n",
    "\n",
    "for n_path in noises:\n",
    "    rec = wfdb.rdrecord(os.path.join(mit_dataset_path, n_path))\n",
    "    sig = np.concatenate([rec.p_signal[:, 0], rec.p_signal[:, 1]])\n",
    "\n",
    "    bandpass = signal.butter(3, [f_low, f_high], 'bandpass', fs=rec.fs, output='sos')\n",
    "    notch = signal.butter(3, [48, 52], 'bandstop', fs=rec.fs, output='sos')\n",
    "\n",
    "    sig = filter_and_norm(sig, bandpass)\n",
    "    sig = filter_and_norm(sig, notch)\n",
    "\n",
    "    sig = resample(sig, rec.fs, 300)\n",
    "    sig_series = pd.Series(data={\"data\": sig, \"fs\": 300, \"noise_type\": n_path})\n",
    "\n",
    "    split_signals = split_signal(sig_series, 3000)\n",
    "    split_signals = pd.DataFrame(split_signals)\n",
    "\n",
    "    noise_dfs.append(split_signals)\n",
    "\n",
    "noise_df = pd.concat(noise_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CinC2017 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(CinCDataset)\n",
    "import DataHandlers.DataProcessUtilities\n",
    "importlib.reload(DataHandlers.DataProcessUtilities)\n",
    "from DataHandlers.DataProcessUtilities import *\n",
    "\n",
    "cinc2017_df = CinCDataset.load_cinc_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinc2017_df = cinc2017_df[cinc2017_df[\"length\"] == 9000]\n",
    "cinc2017_df[\"measDiag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinc2017_df = cinc2017_df[cinc2017_df[\"length\"] == 9000]\n",
    "cinc2017_df = cinc2017_df[cinc2017_df[\"measDiag\"] != DiagEnum.PoorQuality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinc2017_df[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heartrate histogram for AF and not AF\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=300)\n",
    "ax.hist(cinc2017_df[\"heartrate\"][(cinc2017_df[\"class_index\"] != 1)], alpha=0.7, density=True, label=\"Normal or Other Rhythm\")\n",
    "ax.hist(cinc2017_df[\"heartrate\"][(cinc2017_df[\"class_index\"] == 1)], alpha=0.7, density=True, label=\"AF\")\n",
    "ax.set_xlabel(\"Heartrate (bpm)\")\n",
    "ax.set_ylabel(\"Frequency proportion\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgs = cinc2017_df[(cinc2017_df[\"class_index\"] == 2) & (cinc2017_df[\"heartrate\"] > 120)]\n",
    "\n",
    "for _, ecg in ecgs.iterrows():\n",
    "    plot_ecg(ecg[\"data\"][:3000], 300, n_split=1, r_peaks=ecg[\"r_peaks\"], figsize=(6, 2.5), export_quality=True)\n",
    "    plot_ecg_spectrogram(ecg[\"data\"][:3000], 300, n_split=1, cut_range=[2, 18], figsize=(6, 2.5), export_quality=True)\n",
    "    plot_ecg_poincare(ecg[\"rri_feature\"][:10], 10)# ecg[\"rri_len\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = cinc2017_df.loc[\"A02650\"]\n",
    "\n",
    "plot_ecg(ecg[\"data\"], 300, n_split=3, r_peaks=ecg[\"r_peaks\"], figsize=(6, 2.5), export_quality=True)\n",
    "plot_ecg_drr(ecg[\"rri_feature\"], ecg[\"rri_len\"], export_quality=True)# ecg[\"rri_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = CinC2020Dataset.CinC2020DiagMapper()\n",
    "num_unique_classes = len(mapper.diag_desc.index)\n",
    "\n",
    "# Note this only gets used for CinC data - the safer data labels were decided to have different meanings\n",
    "def class_index_map(diag):\n",
    "    if diag == DiagEnum.NoAF:\n",
    "        return 0\n",
    "    elif diag == DiagEnum.AF:\n",
    "        return 1\n",
    "    elif diag == DiagEnum.CannotExcludePathology:\n",
    "        return 2\n",
    "    elif diag == DiagEnum.Undecided:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinc2017_df[\"class_index\"] = cinc2017_df[\"measDiag\"].map(class_index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "        self.noise_prob = 0\n",
    "        self.temp_warp = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "    def set_noise_prob(self, prob, power_std, noise_df):\n",
    "        self.noise_prob = prob\n",
    "        self.noise_power_std = power_std\n",
    "        self.noise_df = noise_df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        data = row[\"data\"]\n",
    "        rri = row[\"rri_feature\"]\n",
    "        rri_len = row[\"rri_len\"]\n",
    "\n",
    "        warp = np.random.binomial(1, self.temp_warp)\n",
    "        if warp:\n",
    "            data, r_peaks = DataAugmentations.temporal_warp(data, row[\"r_peaks_hamilton\"])\n",
    "            rri = get_rri_feature(r_peaks, 20)\n",
    "\n",
    "        add_noise = np.random.binomial(1, self.noise_prob)\n",
    "        if add_noise:\n",
    "            noise = noise_df.sample()[\"data\"].iloc[0] * np.random.normal(scale=self.noise_power_std)\n",
    "            data += noise\n",
    "\n",
    "        X = (data, rri, rri_len)\n",
    "        y = row[\"class_index\"]\n",
    "        ind = row.name\n",
    "\n",
    "        return X, y, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "# Note this function stratifies for AF and non AF!\n",
    "def generate_patient_splits(pt_data, test_frac, val_frac):\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "    val_patients = []\n",
    "\n",
    "    test_val_frac = test_frac + val_frac\n",
    "    val_second_frac = val_frac/test_val_frac\n",
    "\n",
    "    for val, df in pt_data.groupby(\"noAFRecs\"):\n",
    "        print(f\"processing {val}\")\n",
    "        print(f\"number of patients {len(df.index)}\")\n",
    "\n",
    "\n",
    "\n",
    "        n = math.floor(len(df.index) * test_val_frac)\n",
    "        if  test_val_frac > 0:\n",
    "            res = ((len(df.index) * test_val_frac) - n)/test_val_frac\n",
    "        else:\n",
    "            res = 0\n",
    "        n += np.random.binomial(res, test_val_frac)\n",
    "        test_val = df.sample(n)\n",
    "\n",
    "        n = math.floor(len(test_val.index) * val_second_frac)\n",
    "        if  val_second_frac > 0:\n",
    "            res = ((len(test_val.index) * val_second_frac) - n)/val_second_frac\n",
    "        else:\n",
    "            res = 0\n",
    "        n += np.random.binomial(res, val_second_frac)\n",
    "        val = test_val.sample(n)\n",
    "        val_patients.append(val)\n",
    "\n",
    "        test_patients.append(test_val[~test_val[\"ptID\"].isin(val[\"ptID\"])])\n",
    "        train_patients.append(df[~df[\"ptID\"].isin(test_val[\"ptID\"])])\n",
    "\n",
    "    train_pt_df = pd.concat(train_patients)\n",
    "    test_pt_df = pd.concat(test_patients)\n",
    "    val_pt_df = pd.concat(val_patients)\n",
    "\n",
    "    return train_pt_df, test_pt_df, val_pt_df\n",
    "\n",
    "\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, val_frac, batch_size=128):\n",
    "    train_pt_df, test_pt_df, val_pt_df = generate_patient_splits(pt_data, test_frac, val_frac)\n",
    "\n",
    "    print(f\"Test AF: {test_pt_df['noAFRecs'].sum()} Normal: {test_pt_df['noNormalRecs'].sum()} Other: {test_pt_df['noOtherRecs'].sum()}\")\n",
    "    print(f\"Train AF: {train_pt_df['noAFRecs'].sum()} Normal: {train_pt_df['noNormalRecs'].sum()} Other: {train_pt_df['noOtherRecs'].sum()}\")\n",
    "    print(f\"Val AF: {val_pt_df['noAFRecs'].sum()} Normal: {val_pt_df['noNormalRecs'].sum()} Other: {val_pt_df['noOtherRecs'].sum()}\")\n",
    "\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "    val_dataloader = None\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "    val_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        # get ECG datasets\n",
    "        train_dataset = ecg_data[ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])]\n",
    "        # Normalise\n",
    "        train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_train = Dataset(train_dataset)\n",
    "        train_dataloader = DataLoader(torch_dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = ecg_data[(ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"]))]\n",
    "        test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_test = Dataset(test_dataset)\n",
    "        test_dataloader = DataLoader(torch_dataset_test, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not val_pt_df.empty:\n",
    "        val_dataset = ecg_data[(ecg_data[\"ptID\"].isin(val_pt_df[\"ptID\"]))]\n",
    "        val_dataset[\"data\"] = (val_dataset[\"data\"] - val_dataset[\"data\"].map(lambda x: x.mean()))/val_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_val = Dataset(val_dataset)\n",
    "        val_dataloader = DataLoader(torch_dataset_val, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader, train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_safer, test_dataloader_safer, val_dataloader_safer, train_dataset_safer, test_dataset_safer, val_dataset_safer = make_SAFER_dataloaders(safer_pt_data, safer_ecg_data, test_frac=0.15, val_frac=0.15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, batch_size=32):\n",
    "    torch_dataset = Dataset(dataset)\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on Feas2 and train/test on feas1\n",
    "val_dataset_safer = safer_ecg_data[safer_ecg_data[\"feas\"] == 2]\n",
    "val_dataloader_safer = get_dataloaders(val_dataset_safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_safer[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make dataloaders for CinC data - separate cpsc as the validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "val_dataset = df[df[\"dataset\"] == \"cpsc_2018\"]\n",
    "train_dataset, test_dataset = train_test_split(df[df[\"dataset\"] != \"cpsc_2018\"], test_size=0.15, stratify=df[df[\"dataset\"] != \"cpsc_2018\"][\"class_index\"])\n",
    "# test_dataset, val_dataset = train_test_split(test_dataset, test_size=0.5, stratify=test_dataset[\"class_index\"])\n",
    "\n",
    "test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]  # Should just remove any errors in loading the dataset\n",
    "val_dataset = val_dataset[val_dataset[\"measDiag\"] != DiagEnum.Undecided]  # Should just remove any errors in loading the dataset\n",
    "\n",
    "torch_dataset_test = Dataset(test_dataset)\n",
    "test_dataloader = DataLoader(torch_dataset_test, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset)\n",
    "val_dataloader = DataLoader(torch_dataset_val, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "# torch_dataset_train.temp_warp = 0.2\n",
    "# torch_dataset_train.set_noise_prob(0.1, 0.2, noise_df)\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=128, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the proportion of AF samples in the test data to that of the train data\n",
    "\n",
    "val_df_counts = val_dataset[\"class_index\"].value_counts()\n",
    "train_df_counts = train_dataset[\"class_index\"].value_counts()\n",
    "\n",
    "train_not_af = train_df_counts.loc[2] + train_df_counts.loc[0]\n",
    "val_not_af = val_df_counts.loc[2] + val_df_counts.loc[0]\n",
    "\n",
    "val_af_wanted = int(round((train_df_counts.loc[1]/train_not_af) * val_not_af))\n",
    "\n",
    "wanted_af_samples = val_dataset[val_dataset[\"class_index\"] == 1].sample(val_af_wanted)\n",
    "val_dataset = pd.concat([val_dataset[val_dataset[\"class_index\"] != 1], wanted_af_samples])\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset)\n",
    "val_dataloader = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CinC2017 data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "\n",
    "train_dataset_2017, test_val = train_test_split(cinc2017_df.dropna(subset=\"class_index\"), test_size=test_size + val_size, stratify=cinc2017_df[\"class_index\"].dropna())\n",
    "test_dataset_2017, val_dataset_2017 = train_test_split(test_val, test_size=val_size/(test_size + val_size), stratify=test_val[\"class_index\"])\n",
    "\n",
    "# test_dataset_2017 = test_dataset_2017[test_dataset_2017[\"measDiag\"] != DiagEnum.Undecided]  # Should just remove any errors in loading the dataset\n",
    "\n",
    "torch_dataset_test = Dataset(test_dataset_2017)\n",
    "test_dataloader_2017 = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset_2017)\n",
    "train_dataloader_2017 = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset_2017)\n",
    "val_dataloader_2017 = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_2017[\"class_index\"].value_counts())\n",
    "print(test_dataset_2017[\"class_index\"].value_counts())\n",
    "print(val_dataset_2017[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CinC2017 data splits for consistent results!\n",
    "train_dataset_2017.to_pickle(\"TrainedModels/19_May_cinc_2017_train.pk\")\n",
    "test_dataset_2017.to_pickle(\"TrainedModels/19_May_cinc_2017_test.pk\")\n",
    "val_dataset_2017.to_pickle(\"TrainedModels/19_May_cinc_2017_val.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_2017 = pd.read_pickle(\"TrainedModels/19_May_cinc_2017_train.pk\")\n",
    "test_dataset_2017 = pd.read_pickle(\"TrainedModels/19_May_cinc_2017_test.pk\")\n",
    "val_dataset_2017 = pd.read_pickle(\"TrainedModels/19_May_cinc_2017_val.pk\")\n",
    "\n",
    "train_dataloader_2017 = get_dataloaders(train_dataset_2017, 32)\n",
    "test_dataloader_2017 = get_dataloaders(test_dataset_2017, 32)\n",
    "val_dataloader_2017 = get_dataloaders(val_dataset_2017, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use whole CinC2017 as a test\n",
    "dataset_2017 = cinc2017_df[cinc2017_df[\"measDiag\"] != DiagEnum.Undecided].dropna(subset=\"class_index\")\n",
    "\n",
    "torch_dataset = Dataset(dataset_2017)\n",
    "dataloader_2017 = DataLoader(torch_dataset, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_2017[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the noise detector to filter the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter noisy things out of SAFER\n",
    "import Models.NoiseCNN\n",
    "import importlib\n",
    "\n",
    "importlib.reload(Models.NoiseCNN)\n",
    "from Models.NoiseCNN import CNN, hyperparameters\n",
    "\n",
    "noiseDetector = CNN(**hyperparameters).to(device)\n",
    "noiseDetector.load_state_dict(torch.load(\"TrainedModels/CNN_16_may_final_no_undecided.pt\", map_location=device))\n",
    "noiseDetector.eval()\n",
    "\n",
    "def add_noise_predictions(nd, dataloader, dataset):\n",
    "    noise_ps = []\n",
    "    inds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signal = signals[0].to(device).float()\n",
    "            noise_prob = nd(torch.unsqueeze(signal, 1)).detach().to(\"cpu\").numpy()\n",
    "\n",
    "            for i, n in zip(ind, noise_prob):\n",
    "                if type(i) == str:\n",
    "                    inds.append(i)\n",
    "                else:\n",
    "                    inds.append(i.item())\n",
    "                noise_ps.append(float(n))\n",
    "\n",
    "    if dataset is not None:\n",
    "        dataset[\"noise_probs\"] = pd.Series(data=noise_ps, index=inds)\n",
    "    else:\n",
    "        return pd.Series(data=noise_ps, index=inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise_predictions(noiseDetector, val_dataloader_safer, val_dataset_safer)\n",
    "# add_noise_predictions(noiseDetector, test_dataloader_safer, test_dataset_safer)\n",
    "# add_noise_predictions(noiseDetector, train_dataloader_safer, train_dataset_safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the noisy samples\n",
    "# train_dataset_safer_clean = train_dataset_safer[train_dataset_safer[\"noise_probs\"] < 0]\n",
    "# test_dataset_safer_clean = test_dataset_safer[test_dataset_safer[\"noise_probs\"] < 0]\n",
    "val_dataset_safer_clean = val_dataset_safer[val_dataset_safer[\"noise_probs\"] < 0]\n",
    "\n",
    "# print(len(train_dataset_safer_clean.index))\n",
    "# print(len(test_dataset_safer_clean.index))\n",
    "print(len(val_dataset_safer_clean.index))\n",
    "\n",
    "# train_dataloader_safer_clean = get_dataloaders(train_dataset_safer_clean)\n",
    "# test_dataloader_safer_clean = get_dataloaders(test_dataset_safer_clean)\n",
    "val_dataloader_safer_clean = get_dataloaders(val_dataset_safer_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise_predictions(noiseDetector, train_dataloader_2017, train_dataset_2017)\n",
    "add_noise_predictions(noiseDetector, test_dataloader_2017, test_dataset_2017)\n",
    "add_noise_predictions(noiseDetector, val_dataloader_2017, val_dataset_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_2017[\"class_index\"].value_counts())\n",
    "print(test_dataset_2017[\"class_index\"].value_counts())\n",
    "print(val_dataset_2017[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the noisy samples\n",
    "thresh = 0.3\n",
    "\n",
    "train_dataset_2017_clean = train_dataset_2017[train_dataset_2017[\"noise_probs\"] < 0]\n",
    "test_dataset_2017_clean = test_dataset_2017[test_dataset_2017[\"noise_probs\"] < 0]\n",
    "val_dataset_2017_clean = val_dataset_2017[val_dataset_2017[\"noise_probs\"] < 0]\n",
    "\n",
    "print(train_dataset_2017_clean[\"class_index\"].value_counts())\n",
    "print(test_dataset_2017_clean[\"class_index\"].value_counts())\n",
    "print(val_dataset_2017_clean[\"class_index\"].value_counts())\n",
    "\n",
    "\n",
    "train_dataloader_2017_clean = get_dataloaders(train_dataset_2017_clean)\n",
    "test_dataloader_2017_clean = get_dataloaders(test_dataset_2017_clean)\n",
    "val_dataset_2017_clean = val_dataset_2017[val_dataset_2017[\"noise_probs\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class DatasetSequenceIterator:\n",
    "\n",
    "    def __init__(self, data_loading_functions, batch_sizes, filter=lambda x:x):\n",
    "        self.dl_functions = data_loading_functions\n",
    "\n",
    "        self.dataset = None\n",
    "        self.next_dataset = None\n",
    "\n",
    "        self.dataloader_iterator = None\n",
    "        self.next_dataloader_iterator = None\n",
    "\n",
    "        self.next_dataset_loaded = False\n",
    "        self.dataloader_thread = None\n",
    "\n",
    "        self.filter = filter\n",
    "\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.dl_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.dl_index = -1\n",
    "        self.dataloader_thread = threading.Thread(target=self.load_next_dataset)\n",
    "        self.dataloader_thread.start()\n",
    "        self.dataloader_thread.join()\n",
    "        self.swap_to_next_dataset()\n",
    "        self.dl_index += 1\n",
    "        self.dataloader_thread = threading.Thread(target=self.load_next_dataset)\n",
    "        self.dataloader_thread.start()\n",
    "        print(self.dl_index)\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO make this return the right value\n",
    "        return 100\n",
    "\n",
    "    def swap_to_next_dataset(self):\n",
    "        self.dataset = self.next_dataset\n",
    "        self.dataloader_iterator = self.next_dataloader_iterator\n",
    "        self.next_dataset_loaded = False\n",
    "\n",
    "    def load_next_dataset(self):\n",
    "        if self.dl_index + 1 < len(self.dl_functions):\n",
    "            print(f\"Loading dataset {self.dl_index + 1}\")\n",
    "            self.next_dataset = self.dl_functions[self.dl_index + 1]()\n",
    "            self.next_dataset = self.filter(self.next_dataset)\n",
    "\n",
    "            torch_dataset = Dataset(self.next_dataset)\n",
    "            self.next_dataloader_iterator = iter(DataLoader(torch_dataset, batch_size=self.batch_sizes[self.dl_index], shuffle=True, pin_memory=True))\n",
    "            self.next_dataset_loaded = True\n",
    "        else:\n",
    "            print(\"Finished loading all datasets\")\n",
    "            self.next_dataset_loaded = False\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            ret = next(self.dataloader_iterator)\n",
    "        except StopIteration:\n",
    "            print(\"stop_iteration\")\n",
    "            if self.dl_index >= len(self.dl_functions):\n",
    "                # We have gone through all the datasets\n",
    "                print(\"Completed all datasets\")\n",
    "                raise StopIteration\n",
    "            else:\n",
    "\n",
    "                if not self.next_dataset_loaded:\n",
    "                    print(\"waiting_for_next_dataset\")\n",
    "                    self.dataloader_thread.join()\n",
    "\n",
    "                self.swap_to_next_dataset()\n",
    "                self.dl_index += 1\n",
    "                self.dataloader_thread = threading.Thread(target=self.load_next_dataset)\n",
    "                self.dataloader_thread.start()\n",
    "                ret = next(self.dataloader_iterator)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the DatasetSequenceIterator by dividing feas1 into two parts\n",
    "\n",
    "def load_feas1_first_half():\n",
    "    ecg_data, pt_data = load_feas1_chunk_range((0, 1))\n",
    "    return prepare_safer_data(pt_data, ecg_data)[1]\n",
    "\n",
    "def load_feas1_second_half():\n",
    "    ecg_data, pt_data = load_feas1_chunk_range((4, 5))\n",
    "    return prepare_safer_data(pt_data, ecg_data)[1]\n",
    "\n",
    "def load_feas1_nth_chuck(n):\n",
    "    ecg_data, pt_data = load_feas1_chunk_range((n, n+1))\n",
    "    ecg_data.index = ecg_data[\"measID\"]\n",
    "    pt_data.index = pt_data[\"ptID\"]\n",
    "    return prepare_safer_data(pt_data, ecg_data)[1]\n",
    "\n",
    "loading_functions = [lambda n=n: load_feas1_nth_chuck(n) for n in range(num_chunks)]\n",
    "\n",
    "feas1_dataloader_entire = DatasetSequenceIterator(loading_functions, [128 for n in range(num_chunks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ecgs = 0\n",
    "for i, (signals, labels, _) in enumerate(feas1_train_dataloader):\n",
    "    signal = signals[0].to(device).float()\n",
    "    rris = signals[1].to(device).float()\n",
    "    rri_len = signals[2].to(device).float()\n",
    "\n",
    "    num_ecgs += signal.shape[0]\n",
    "\n",
    "print(num_ecgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = SAFERDataset.load_pt_dataset(1)\n",
    "ecg_data = SAFERDataset.load_ecg_csv(1, pt_data, ecg_range=None, ecg_meas_diag=None, feas2_offset=10000, feas2_ecg_offset=200000)\n",
    "\n",
    "ecg_data[\"feas\"] = 1\n",
    "ecg_data[\"length\"] = 9120\n",
    "ecg_data[\"rri_len\"] = 20\n",
    "\n",
    "pt_data, ecg_data = prepare_safer_data(pt_data, ecg_data)\n",
    "# train_pts, test_pts, val_pts = generate_patient_splits(pt_data, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_noise_predictions = add_noise_predictions(noiseDetector, feas1_dataloader_entire, ecg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data[\"noise_prediction\"] = ecg_data[\"noise_probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data.dropna(subset=[\"noise_prediction\"])\n",
    "(ecg_data[ecg_data['poss_AF_tag'] == 1][\"noise_prediction\"] < 0).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of noisy ECGs: {(feas1_noise_predictions > 0).sum()}\")\n",
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "feas1_noise_predictions.to_pickle(os.path.join(feas1_path, \"ECGs/feas1_noise_predictions.pk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "feas1_noise_predictions = pd.read_pickle(os.path.join(feas1_path, \"ECGs/feas1_noise_predictions.pk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zenicor_conf_mat = confusion_matrix(feas1_ecg_data_test[\"class_index\"], feas1_ecg_data_test[\"poss_AF_tag\"])\n",
    "print_results(zenicor_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data[\"noise_prediction\"] = feas1_noise_predictions\n",
    "print(ecg_data[ecg_data[\"noise_prediction\"] < 0][\"class_index\"].value_counts())\n",
    "print(ecg_data[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_train_data = ecg_data[ecg_data[\"ptID\"].isin(train_pts[\"ptID\"])]\n",
    "feas1_train_data_clean = feas1_train_data[feas1_train_data[\"noise_prediction\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pts in [train_pts, test_pts, val_pts]:\n",
    "    print(pts[\"noNormalRecs\"].sum())\n",
    "    print(pts[\"noAFRecs\"].sum())\n",
    "    print(pts[\"noOtherRecs\"].sum())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "train_pts.to_pickle(os.path.join(feas1_path, \"all_feas1_train_pts_27_may.pk\"))\n",
    "test_pts.to_pickle(os.path.join(feas1_path, \"all_feas1_test_pts_27_may.pk\"))\n",
    "val_pts.to_pickle(os.path.join(feas1_path, \"all_feas1_val_pts_27_may.pk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "train_pts = pd.read_pickle(os.path.join(feas1_path, \"all_feas1_train_pts_27_may.pk\"))\n",
    "test_pts = pd.read_pickle(os.path.join(feas1_path, \"all_feas1_test_pts_27_may.pk\"))\n",
    "val_pts = pd.read_pickle(os.path.join(feas1_path, \"all_feas1_val_pts_27_may.pk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_data_test = pd.read_pickle(os.path.join(feas1_path, \"ECGs/feas1_test_27_mar.pk\"))\n",
    "feas1_ecg_data_test = feas1_ecg_data_test[feas1_ecg_data_test[\"rri_len\"] > 5]\n",
    "\n",
    "feas1_ecg_data_val = pd.read_pickle(os.path.join(feas1_path, \"ECGs/feas1_val_27_mar.pk\"))\n",
    "feas1_ecg_data_val = feas1_ecg_data_val[feas1_ecg_data_val[\"rri_len\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_data_test[\"noise_prediction\"] = feas1_noise_predictions\n",
    "feas1_ecg_data_test_clean = feas1_ecg_data_test[feas1_ecg_data_test[\"noise_prediction\"] < 0]\n",
    "\n",
    "feas1_ecg_data_val[\"noise_prediction\"] = feas1_noise_predictions\n",
    "feas1_ecg_data_val_clean = feas1_ecg_data_val[feas1_ecg_data_val[\"noise_prediction\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_data_val[\"class_index\"].value_counts() - feas1_ecg_data_val_clean[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data[(feas1_noise_predictions[ecg_data.index] < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some a filter function to select data from each partition\n",
    "def filter_train_pts(ecg_data):\n",
    "    print(f\"filtering {(feas1_noise_predictions[ecg_data.index] > 0).sum()} ECGs out\")\n",
    "    print(ecg_data.head())\n",
    "    print(feas1_noise_predictions.head())\n",
    "    return ecg_data[(ecg_data[\"ptID\"].isin(train_pts[\"ptID\"])) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "def filter_test_pts(ecg_data):\n",
    "    return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "def filter_val_pts(ecg_data):\n",
    "    return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "feas1_train_dataloader = DatasetSequenceIterator(loading_functions, [64 for n in range(num_chunks)], filter=filter_train_pts)\n",
    "feas1_test_dataloader = get_dataloaders(feas1_ecg_data_test)\n",
    "feas1_val_dataloader = get_dataloaders(feas1_ecg_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_test_dataloader_clean = get_dataloaders(feas1_ecg_data_test_clean)\n",
    "feas1_val_dataloader_clean = get_dataloaders(feas1_ecg_data_val_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feas1_ecg_data_test_clean[\"class_index\"].value_counts())\n",
    "print(feas1_ecg_data_val_clean[\"class_index\"].value_counts())\n",
    "\n",
    "print(\"  \")\n",
    "\n",
    "print(feas1_ecg_data_test[\"class_index\"].value_counts())\n",
    "print(feas1_ecg_data_val[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del feas1_train_dataloader\n",
    "del feas1_test_dataloader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Models.SpectrogramTransformer\n",
    "importlib.reload(Models.SpectrogramTransformer)\n",
    "# from Models.SpectrogramTransformer import TransformerModel\n",
    "import Models.SpectrogramTransformerAttentionPooling\n",
    "importlib.reload(Models.SpectrogramTransformerAttentionPooling)\n",
    "from Models.SpectrogramTransformerAttentionPooling import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR, LambdaLR, SequentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 4\n",
    "n_fft = 128\n",
    "embed_dim = 128 # int(n_fft/2)\n",
    "n_inp_rri = 64\n",
    "\n",
    "model = TransformerModel(3, embed_dim, n_head, 512, 6, n_fft, n_inp_rri, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class focal_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, weights, gamma=2, label_smoothing=0):\n",
    "        super(focal_loss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction=\"none\", label_smoothing=label_smoothing)\n",
    "        self.weights = weights\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "        ce = self.ce_loss(pred, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "\n",
    "        loss_sum = torch.sum(((1-pt) ** self.gamma) * ce * self.weights[targets])\n",
    "        norm_factor = torch.sum(self.weights[targets])\n",
    "        return loss_sum/norm_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.tensor(train_dataset[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "loss_func = focal_loss(class_weights, 2) # nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1) # focal_loss(class_weights, 2, 0.05) #\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "number_warmup_batches = 600\n",
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "# scheduler = SequentialLR(optimizer, [warmup_scheduler, scheduler], [number_warmup_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake scheduler before retraining on SAFER\n",
    "\n",
    "\"\"\"\n",
    "class_counts = torch.tensor(train_dataset_safer_clean[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# just approximate weights using feas2 rather than computing for feas 1 - these might be fundamentally different because in feas2 the cardiologist stopped labelling after the first AF from a patient therefore fewer AF.\n",
    "class_counts = torch.tensor(val_dataset_safer[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Use all of feas1 to compute the class counts - precomputed values for next time: tensor([0.0043, 0.7924, 0.2033])\n",
    "class_counts = torch.tensor(feas1_ecg_data[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "\"\"\"\n",
    "\n",
    "# class_counts = torch.tensor(ecg_data[ecg_data[\"noise_prediction\"] < 0][\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_counts = torch.tensor(feas1_ecg_data_train_undersamp[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "\n",
    "# class_weights = torch.tensor([0.0043, 0.7924, 0.2033])\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "loss_func = focal_loss(class_weights, gamma=2, label_smoothing=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "number_warmup_batches = 600\n",
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake scheduler before retraining on CinC2017\n",
    "\n",
    "class_counts = torch.tensor(train_dataset_2017[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weight=class_weights) # multiclass_cross_entropy_loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00004)\n",
    "\n",
    "number_warmup_batches = 600\n",
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model I stole\n",
    "\n",
    "import OtherModels.Prna.physionet2020_submission.model\n",
    "importlib.reload(OtherModels.Prna.physionet2020_submission.model)\n",
    "from OtherModels.Prna.physionet2020_submission.model import CTN\n",
    "import OtherModels.Prna.physionet2020_submission.optimizer\n",
    "importlib.reload(OtherModels.Prna.physionet2020_submission.optimizer)\n",
    "from OtherModels.Prna.physionet2020_submission.optimizer import NoamOpt\n",
    "\n",
    "# Train prna's transformer\n",
    "n_head = 8\n",
    "n_fft = 128\n",
    "embed_dim = 128 # int(n_fft/2)\n",
    "n_inp_rri = 64\n",
    "\n",
    "class_counts = torch.tensor(train_dataset[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "model = CTN(256, n_head, 2048, 4, 0.1, 64, 0, 0, 3).to(device)\n",
    "\n",
    "# Initialize parameters with Glorot / fan_avg.\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# optimizer = NoamOpt(256, 1, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "number_warmup_batches = 2\n",
    "def warmup(current_step: int):\n",
    "    return 1 / (10 ** (float(number_warmup_batches - current_step)))\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "scheduler = SequentialLR(optimizer, [warmup_scheduler, scheduler], [number_warmup_batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, tensorboard_trace_handler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "model = model.to(device)\n",
    "model.fix_transformer_params(fix_spec=False, fix_rri=False)\n",
    "num_epochs = 40\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader):\n",
    "    best_test_loss = 100\n",
    "    best_epoch = -1\n",
    "    best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"starting epoch {epoch} ...\")\n",
    "        # Train\n",
    "        num_batches = 0\n",
    "        model.train()\n",
    "        for i, (signals, labels, _) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            signal = signals[0].to(device).float()\n",
    "            rris = signals[1].to(device).float()\n",
    "            rri_len = signals[2].to(device).float()\n",
    "\n",
    "            if torch.any(torch.isnan(signal)):\n",
    "                print(\"Signals are nan\")\n",
    "                continue\n",
    "\n",
    "            if torch.any(torch.isnan(rris)):\n",
    "                print(\"Signals are nan\")\n",
    "                continue\n",
    "\n",
    "            labels = labels.long()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(signal, rris, rri_len).to(\"cpu\")\n",
    "\n",
    "            if torch.any(torch.isnan(output)):\n",
    "                print(signal)\n",
    "                print(rris)\n",
    "                print(rri_len)\n",
    "                print(output)\n",
    "                raise ValueError\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            num_batches += 1\n",
    "            total_loss += float(loss)\n",
    "\n",
    "        print(num_batches)\n",
    "\n",
    "        print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "        # writer.add_scalar(\"Loss/train\", total_loss/num_batches, epoch)\n",
    "        print(\"Testing ...\")\n",
    "        # Test\n",
    "        num_test_batches = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, (signals, labels, _) in enumerate(test_dataloader):\n",
    "                signal = signals[0].to(device).float()\n",
    "                rris = signals[1].to(device).float()\n",
    "                rri_len = signals[2].to(device).float()\n",
    "\n",
    "                if torch.any(torch.isnan(signal)):\n",
    "                    print(\"Signals are nan\")\n",
    "                    continue\n",
    "\n",
    "                labels = labels.long()\n",
    "                output = model(signal, rris, rri_len).to(\"cpu\")\n",
    "                loss = loss_func(output, labels)\n",
    "                test_loss += float(loss)\n",
    "                num_test_batches += 1\n",
    "\n",
    "        print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "        losses.append([total_loss/num_batches, test_loss/num_test_batches])\n",
    "        # writer.add_scalar(\"Loss/test\", test_loss/num_t est_batches, epoch)\n",
    "\n",
    "        if test_loss/num_test_batches < best_test_loss:\n",
    "            best_model = copy.deepcopy(model).cpu()\n",
    "            best_test_loss = test_loss/num_test_batches\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            if best_epoch + 5 <= epoch:\n",
    "                return best_model, losses\n",
    "\n",
    "    return best_model, losses\n",
    "\n",
    "model, losses = train(model, feas1_train_dataloader_undersamp, feas1_test_dataloader_undersamp)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.load(\"TrainedModels/Transformer_23_May_cinc_train_attention_pooling_no_augmentation_smoothing_training_curve.npy\")\n",
    "\n",
    "# \"C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\TrainedModels\\Transformer_23_May_feas1_train_attention_pooling_augmentation_smoothing_retrain.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training curve (1 axis only)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "train_l = ax.plot([l[0] for l in losses], label=\"training loss\")\n",
    "val_l = ax.plot([l[1] for l in losses], label=\"validation loss\", color=\"#ff7f0e\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_np = np.array(losses)\n",
    "np.save(\"TrainedModels/Transformer_24_May_cinc_2017_train_attention_pooling_augmentation_smoothing\", losses_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "torch.save(model.state_dict(), \"TrainedModels/Transformer_24_May_cinc_2017_train_attention_pooling_augmentation_smoothing.pt\")\n",
    "\n",
    "# train_dataset_safer.to_pickle(\"TrainedModels/Transformer_15_Mar_train.pk\")\n",
    "# test_dataset_safer.to_pickle(\"TrainedModels/Transformer_15_Mar_test.pk\")\n",
    "# val_dataset_safer.to_pickle(\"TrainedModels/Transformer_15_Mar_val.pk\")\n",
    "# train_pt_df.to_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_train.pk\")\n",
    "# val_pt_df.to_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_val.pk\")\n",
    "# test_pt_df.to_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_test.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_pickle(\"TrainedModels/Transformer_13_May_cinc_trained_initial_train.pk\")\n",
    "test_dataset.to_pickle(\"TrainedModels/Transformer_13_May_cinc_trained_initial_test.pk\")\n",
    "val_dataset.to_pickle(\"TrainedModels/Transformer_13_May_cinc_trained_initial_val.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_safer = pd.read_pickle(\"TrainedModels/Transformer_13_Mar_train.pk\")\n",
    "test_dataset_safer = pd.read_pickle(\"TrainedModels/Transformer_13_Mar_test.pk\")\n",
    "val_dataset_safer = pd.read_pickle(\"TrainedModels/Transformer_13_Mar_val.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_safer = get_dataloaders(train_dataset_safer)\n",
    "test_dataloader_safer = get_dataloaders(test_dataset_safer)\n",
    "val_dataloader_safer = get_dataloaders(val_dataset_safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_2017.to_pickle(\"TrainedModels/Transformer_13_May_cinc_2017_trained_train.pk\")\n",
    "test_dataset_2017.to_pickle(\"TrainedModels/Transformer_13_May_cinc_2017_trained_test.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this for safer cross validation later\n",
    "cinc_model_path = \"TrainedModels/Transformer_15_Mar_cinc_trained_noise_augmentation.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "# model = TransformerModel(2, embed_dim, n_head, 1024, 4, 47, n_fft).to(device)\n",
    "model.load_state_dict(torch.load(\"TrainedModels/Transformer_27_may_feas1_train_attention_pooling_augmentation_smoothing_nk_beats_retrain.pt\", map_location=device))\n",
    "\n",
    "# train_pt_df = pd.read_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_train.pk\")\n",
    "# val_pt_df = pd.read_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_val.pk\")\n",
    "# test_pt_df = pd.read_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_test.pk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_pt_df = pd.read_pickle(\"TrainedModels/Transformer_13_May_cinc_2017_trained_train.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Tonights training schedule\n",
    "\n",
    "# Cross Validation dataset construction for SAFER data\n",
    "# Split train and test data according to each patient\n",
    "\n",
    "\n",
    "def get_train_and_val_feas1_sets(test_pts, val_pts):\n",
    "    # Have to load all the data!\n",
    "    feas1_ecg_data, feas1_pt_data = load_feas1_chunk_range((0, num_chunks))\n",
    "\n",
    "    # Just use feas1 to prepare test and validation datasets (The train is best handled with a DatasetSequenceIterator)\n",
    "    feas1_pt_data, feas1_ecg_data = prepare_safer_data(feas1_pt_data, feas1_ecg_data)\n",
    "    feas1_ecg_data[\"class_index\"].value_counts()\n",
    "\n",
    "    feas1_ecg_data_test = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(test_pts[\"ptID\"])]\n",
    "    feas1_ecg_data_val = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(val_pts[\"ptID\"])]\n",
    "\n",
    "    print(feas1_ecg_data_test[\"class_index\"].value_counts())\n",
    "    print(feas1_ecg_data_val[\"class_index\"].value_counts())\n",
    "\n",
    "    del feas1_ecg_data\n",
    "    del feas1_pt_data\n",
    "\n",
    "    return feas1_ecg_data_test, feas1_ecg_data_val\n",
    "\n",
    "\n",
    "def split_patients_2_groups(pt_data, frac):\n",
    "    split_ratios = np.array([frac, 1-frac])\n",
    "\n",
    "    splits = [[], []]\n",
    "\n",
    "    af_counts = np.zeros(2, dtype=int)\n",
    "    total_counts = np.zeros(2, dtype=int)\n",
    "\n",
    "    total_total_count = 0\n",
    "    total_af_count = 0\n",
    "\n",
    "    for _, pt in pt_data.sample(frac=1).iterrows():\n",
    "        total_total_count += pt[\"noHQrecs\"]\n",
    "        total_af_count += pt[\"noAFRecs\"]\n",
    "\n",
    "        exp_total_counts = total_total_count * split_ratios\n",
    "        exp_af_counts = total_af_count * split_ratios\n",
    "\n",
    "        af_rec_mat = np.diag(np.array([pt[\"noAFRecs\"] for _ in range(2)]))\n",
    "        hq_rec_mat = np.diag(np.array([pt[\"noHQrecs\"] for _ in range(2)]))\n",
    "\n",
    "        loss =  np.sum(np.abs(af_counts[None, :] + af_rec_mat - exp_af_counts[None, :]) + np.abs(total_counts[None, :] + hq_rec_mat - exp_total_counts[None, :]), axis=-1)\n",
    "        best_fold = np.argmin(loss)\n",
    "\n",
    "        splits[best_fold].append(pt)\n",
    "        af_counts[best_fold] += pt[\"noAFRecs\"]\n",
    "        total_counts[best_fold] += pt[\"noHQrecs\"]\n",
    "\n",
    "    return pd.DataFrame(splits[0]), pd.DataFrame(splits[1])\n",
    "\n",
    "\n",
    "pt_data = SAFERDataset.load_pt_dataset(1)\n",
    "ecg_data = SAFERDataset.load_ecg_csv(1, pt_data, ecg_range=None, ecg_meas_diag=None, feas2_offset=10000, feas2_ecg_offset=200000)\n",
    "\n",
    "ecg_data[\"feas\"] = 1\n",
    "ecg_data[\"length\"] = 9120\n",
    "ecg_data[\"rri_len\"] = 20\n",
    "\n",
    "pt_data, ecg_data = prepare_safer_data(pt_data, ecg_data)\n",
    "\n",
    "ecg_data[\"noise_prediction\"] = feas1_noise_predictions\n",
    "ecg_data = ecg_data[ecg_data[\"noise_prediction\"] < 0]  # Remove noisy samples here itself\n",
    "\n",
    "# now set the counts in pt\n",
    "pt_data[\"noHQrecs\"] = ecg_data[\"ptID\"].value_counts()\n",
    "pt_data[\"noAFRecs\"] = ecg_data[ecg_data[\"class_index\"] == 1][\"ptID\"].value_counts()\n",
    "pt_data[\"noHQrecs\"].fillna(0, inplace=True)\n",
    "pt_data[\"noAFRecs\"].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "num_cv = 5\n",
    "num_folds = num_cv  # We'll produce a validation set from the test set in each fold!\n",
    "pt_folds = [[] for _ in range(num_folds)]\n",
    "\n",
    "af_counts = np.zeros(num_folds, dtype=int)\n",
    "total_counts = np.zeros(num_folds, dtype=int)\n",
    "\n",
    "total_total_count = 0\n",
    "total_af_count = 0\n",
    "\n",
    "\n",
    "# Go around the folds and assign patients to each\n",
    "for _, pt in pt_data.sample(frac=1).iterrows():\n",
    "    total_total_count += pt[\"noHQrecs\"]\n",
    "    total_af_count += pt[\"noAFRecs\"]\n",
    "\n",
    "    exp_total_counts = total_total_count * 1.0/num_folds\n",
    "    exp_af_counts = total_af_count * 1.0/num_folds\n",
    "\n",
    "    af_rec_mat = np.diag(np.array([pt[\"noAFRecs\"] for _ in range(num_folds)]))\n",
    "    hq_rec_mat = np.diag(np.array([pt[\"noHQrecs\"] for _ in range(num_folds)]))\n",
    "\n",
    "    loss =  np.sum(np.abs(af_counts[None, :] + af_rec_mat - exp_af_counts) + np.abs(total_counts[None, :] + hq_rec_mat - exp_total_counts), axis=-1)\n",
    "    best_fold = np.argmin(loss)\n",
    "\n",
    "    pt_folds[best_fold].append(pt)\n",
    "    af_counts[best_fold] += pt[\"noAFRecs\"]\n",
    "    total_counts[best_fold] += pt[\"noHQrecs\"]\n",
    "\n",
    "\n",
    "test_pt_folds = [pd.DataFrame(fold) for fold in pt_folds]\n",
    "val_pt_folds = []\n",
    "train_pt_folds = []\n",
    "\n",
    "for test_pt_f in test_pt_folds:\n",
    "    val_pt_f, train_pt_f = split_patients_2_groups(pt_data[~pt_data[\"ptID\"].isin(test_pt_f[\"ptID\"])], 0.125)\n",
    "    val_pt_folds.append(val_pt_f)\n",
    "    train_pt_folds.append(train_pt_f)\n",
    "\n",
    "\n",
    "for f in test_pt_folds:\n",
    "    print(f[\"noAFRecs\"].sum(), f[\"noHQrecs\"].sum())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for f in val_pt_folds:\n",
    "    print(f[\"noAFRecs\"].sum(), f[\"noHQrecs\"].sum())\n",
    "\n",
    "conf_mats = []\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for i, (train_pt_df, test_pt_df, val_pt_df) in enumerate(zip(train_pt_folds, test_pt_folds, val_pt_folds)):\n",
    "    print(\"======\")\n",
    "    print(f\"Fold {i}\")\n",
    "    # Create some a filter function to select data from each partition\n",
    "    def filter_train_pts(ecg_data):\n",
    "        return ecg_data[(ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "    feas1_train_dataloader = DatasetSequenceIterator(loading_functions, [64 for n in range(num_chunks)], filter=filter_train_pts)\n",
    "    feas1_ecg_data_test, feas1_ecg_data_val = get_train_and_val_feas1_sets(test_pt_df, val_pt_df)\n",
    "\n",
    "    feas1_test_dataloader = get_dataloaders(feas1_ecg_data_test, 64)\n",
    "    feas1_val_dataloader = get_dataloaders(feas1_ecg_data_val, 64)\n",
    "\n",
    "\n",
    "    class_counts = torch.tensor(ecg_data[ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])][\"class_index\"].value_counts().values.astype(np.float32))\n",
    "    class_weights = (1/class_counts)\n",
    "    class_weights /= torch.sum(class_weights)\n",
    "    print(class_weights)\n",
    "\n",
    "    print(feas1_ecg_data_test[\"class_index\"].value_counts())\n",
    "    print(feas1_ecg_data_val[\"class_index\"].value_counts())\n",
    "\n",
    "    #  now get the model\n",
    "    model = TransformerModel(3, embed_dim, n_head, 512, 6, n_fft, n_inp_rri, device=device).to(device)\n",
    "    model.load_state_dict(torch.load(\"TrainedModels/Transformer_27_May_feas1_train_attention_pooling_augmentation_smoothing_no_noisy_nk_beats.pt\", map_location=device))\n",
    "\n",
    "    loss_func = focal_loss(class_weights, gamma=2, label_smoothing=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "    number_warmup_batches = 600\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "    model, losses = train(model, feas1_train_dataloader, feas1_val_dataloader)\n",
    "    model = model.to(device)\n",
    "\n",
    "    losses_np = np.array(losses)\n",
    "    np.save(f\"TrainedModels/Transformer_29_May_final_cross_validate_{i}_training_curve\", losses_np)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"TrainedModels/Transformer_29_May_final_cross_validate_{i}_training_curve.pt\")\n",
    "    feas1_ecg_data_test.to_pickle(f\"C:/Users/daniel/Documents/feas1_train_set_cross_validate_{i}.pk\")\n",
    "\n",
    "    predictions, true_labels = get_predictions(model, feas1_test_dataloader, feas1_ecg_data_test)\n",
    "    conf_mat = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "    print(conf_mat)\n",
    "    print(losses_np)\n",
    "\n",
    "    conf_mats.append(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "def get_predictions(model, dataloader, dataset):\n",
    "\n",
    "    attentions = []\n",
    "\n",
    "    \"\"\"\n",
    "    def hook(module, x, y):\n",
    "        for a in y[1]:\n",
    "            attentions.append(a.detach().cpu().numpy())\n",
    "\n",
    "    attention_hook = model.attention_pooling.attn.register_forward_hook(hook)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    outputs = []\n",
    "    inds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signal = signals[0].to(device).float()\n",
    "            rris = signals[1].to(device).float()\n",
    "            rri_len = signals[2].to(device).float()\n",
    "\n",
    "            labels = labels.long().detach().numpy()\n",
    "            true_labels.append(labels)\n",
    "\n",
    "            output = model(signal, rris, rri_len).detach().to(\"cpu\").numpy() # rris).detach().to(\"cpu\").numpy()\n",
    "\n",
    "            prediction = output # np.argmax(output, axis=-1)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            for i, o in zip(ind, output):\n",
    "                outputs.append(o)\n",
    "                if isinstance(i, str):\n",
    "                    inds.append(i)\n",
    "                else:\n",
    "                    inds.append(i.item())\n",
    "\n",
    "    dataset[\"prediction\"] = pd.Series(data=outputs, index=inds)\n",
    "    # dataset[\"attention\"] = pd.Series(data=attentions, index=inds)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    true_labels = np.concatenate(true_labels)\n",
    "\n",
    "    # attention_hook.remove()\n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "# predictions, true_labels = get_predictions(model, feas1_val_dataloader_clean, feas1_ecg_data_val_clean)\n",
    "# conf_mat = confusion_matrix(true_labels, np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_data_test[\"noise_probs\"] = feas1_ecg_data_test[\"noise_prediction\"]\n",
    "feas1_ecg_data_val[\"noise_probs\"] = feas1_ecg_data_val[\"noise_prediction\"]\n",
    "\n",
    "def get_noise_free_conf_mat(dataset):\n",
    "   return confusion_matrix(dataset[dataset[\"noise_probs\"] < 0][\"class_index\"], dataset[dataset[\"noise_probs\"] < 0][\"prediction\"].map(np.argmax))\n",
    "\n",
    "noise_free_conf_mat = get_noise_free_conf_mat(feas1_ecg_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "def print_results(conf_mat):\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(conf_mat)\n",
    "\n",
    "    print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1]):0.3f}\")\n",
    "    print(f\"Specificity: {(conf_mat[0, 0] + conf_mat[0, 2] + conf_mat[2, 0] + conf_mat[2, 2])/(np.sum(conf_mat[0]) + np.sum(conf_mat[2])):0.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"Normal F1: {F1_ind(conf_mat, 0):0.3f}\")\n",
    "    print(f\"AF F1: {F1_ind(conf_mat, 1):0.3f}\")\n",
    "    print(f\"Other F1: {F1_ind(conf_mat, 2):0.3f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print_results(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print noise free conf mats\n",
    "print_results(noise_free_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_inds = feas1_ecg_data_val[feas1_ecg_data_val[\"prediction\"].map(np.argmax) != feas1_ecg_data_val[\"class_index\"]].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_val_interesting = feas1_ecg_data_val[(feas1_ecg_data_val[\"class_index\"] != 0) | (feas1_ecg_data_val[\"class_index\"] != feas1_ecg_data_val[\"prediction\"].map(np.argmax))]\n",
    "feas1_val_interesting_dataloader = get_dataloaders(feas1_val_interesting, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.special import softmax\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=250)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "\n",
    "labels = [\"Normal\", \"AF\", \"Other\"]\n",
    "\n",
    "for i in range(3):\n",
    "    p, r, d = precision_recall_curve((feas1_ecg_data_val_clean[\"class_index\"] == i),  feas1_ecg_data_val_clean[\"prediction\"].map(lambda x: softmax(x)[i]))\n",
    "    if i == 1:\n",
    "        p_af = p\n",
    "        r_af = r\n",
    "        d_af = d\n",
    "\n",
    "    ax.plot(r, p, label=labels[i])\n",
    "    # plt.xlim((0, 1.1))\n",
    "    # plt.ylim((0, 1.1))\n",
    "\n",
    "    # closest_point_to_0_final = np.argmin(np.abs(d))\n",
    "    # ax.plot(r[closest_point_to_0_final], p[closest_point_to_0_final], \"o\", color=\"#ff7f0e\", label=r\"$p(AF) = 0.5$\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# fig.savefig(\"FinalReportFigs/CNN_NoiseDetect_precision_recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prec = np.argmax(p_af)\n",
    "print(p_af[max_prec])\n",
    "print(r_af[max_prec])\n",
    "print(d_af[max_prec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Utilities.Plotting)\n",
    "from Utilities.Plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "dataset = feas1_ecg_data_val_clean\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(lambda x: np.argmax(x))\n",
    "#  & (dataset[\"noise_probs\"] < 0)\n",
    "\n",
    "selection = dataset[(dataset[\"class_prediction\"] == 2) & (dataset[\"class_index\"] == 1)]\n",
    "\n",
    "\n",
    "for ecg_ind, ecg in selection.sample(frac=1).iterrows():\n",
    "    print(ecg_ind)\n",
    "    print(ecg[[\"measDiag\", \"prediction\", \"class_index\"]])\n",
    "    # filtered_ecg = scipy.signal.sosfiltfilt(sos, ecg[\"data\"], padlen=150)\n",
    "\n",
    "    plot_ecg(ecg[\"data\"], 300, n_split=3, r_peaks=ecg[\"r_peaks\"])\n",
    "    plot_ecg_spectrogram(ecg[\"data\"], 300, n_split=3, cut_range=[2, 18], figsize=(6, 2.5), export_quality=True)\n",
    "    plot_ecg_drr(ecg[\"rri_feature\"], ecg[\"rri_len\"], export_quality=True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_initial_transformer = np.array([[[ 717,   41,  226],\n",
    " [  88,  686,  129],\n",
    " [ 456,  184, 1407]], # CinC2020\n",
    "[[   0, 2237, 1457],\n",
    " [   0,  437,   67],\n",
    " [   0, 1094,  555]],  # CinC2017\n",
    "[[7384, 2281, 9848],\n",
    " [   0,    8,    8],\n",
    " [ 249,  151,  357]],  # Safer Feas2\n",
    "[[ 8502,  2400, 11362],\n",
    " [    2,    50,    67],\n",
    " [  223,    75,   256]]])  # safer feas1\n",
    "\n",
    "conf_mat_fine_tuned = np.array([[[  0, 320, 419],\n",
    " [  0,  91,  10],\n",
    " [  0, 171, 159]],  # CinC 2017\n",
    "[[14918,  1543,  3052],\n",
    " [    3,     7,     6],\n",
    " [  562,    79,   116]],  # SAFER feas 2\n",
    "[[17558,  2061,  2829],\n",
    " [    7,    92,    34],\n",
    " [  343,    60,   159]]])  # SAFER feas 1\n",
    "\n",
    "\n",
    "conf_mat_final = np.array([[[20413,   376,  1475],\n",
    " [   13,    82,    38],\n",
    " [  203,    73,   282]],  # SAFER feas1\n",
    "[[19113,    19,   381],\n",
    " [    7,     5,     4],\n",
    " [  428,    67,   262]]]) # SAFER feas2\n",
    "\n",
    "\n",
    "for c in conf_mat_final:\n",
    "    plot_confusion_matrix_2(c, [\"Normal\", \"AF\", \"Other Rhythm\"], colour=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with attention maps\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "dataset = feas1_val_interesting\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(lambda x: np.argmax(x))\n",
    "#  & (dataset[\"noise_probs\"] < 0)\n",
    "\n",
    "selection = dataset[(dataset[\"class_prediction\"] == 1) & (dataset[\"class_index\"] == 1)]\n",
    "\n",
    "for ecg_ind, ecg in selection.dropna(subset=[\"attention\"]).sample(frac=1).iterrows():\n",
    "    print(ecg_ind)\n",
    "    print(ecg[[\"measDiag\", \"prediction\", \"class_index\"]])\n",
    "    print(ecg[\"attention\"].shape)\n",
    "    # for 10s cut attention to 96\n",
    "    # filtered_ecg = scipy.signal.sosfiltfilt(sos, ecg[\"data\"], padlen=150)\n",
    "    plot_ecg_with_attention(ecg[\"data\"][:3000], 300, n_split=1, attention=ecg[\"attention\"][:, :, :96])# , figsize=(6, 4), export_quality=True)\n",
    "    # plot_ecg_spectrogram(ecg[\"data\"], 300, n_split=3, cut_range=[2, 18])\n",
    "    # plot_ecg_poincare(ecg[\"rri_feature\"], ecg[\"rri_len\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the attention mechanism (not very useful yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transformer_encoder.layers.\n",
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "def patch_attention(m):\n",
    "    forward_orig = m.forward\n",
    "\n",
    "    def wrap(*args, **kwargs):\n",
    "        kwargs['need_weights'] = True\n",
    "        kwargs['average_attn_weights'] = False\n",
    "\n",
    "        return forward_orig(*args, **kwargs)\n",
    "\n",
    "    m.forward = wrap\n",
    "\n",
    "attentions = []\n",
    "inds = []\n",
    "\n",
    "def save_outputs(module, x, y):\n",
    "    for att in y[1]:\n",
    "        attentions.append(att.cpu().numpy())\n",
    "\n",
    "patch_attention(model.transformer_encoder.layers[0].self_attn)\n",
    "attention_hook = model.transformer_encoder.layers[0].self_attn.register_forward_hook(save_outputs)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels, ind) in enumerate(feas1_val_interesting_dataloader):\n",
    "        signal = signals[0].to(device).float()\n",
    "        rris = signals[1].to(device).float()\n",
    "        rri_len = signals[2].to(device).float()\n",
    "\n",
    "        labels = labels.long().detach().numpy()\n",
    "        for i in ind:\n",
    "            if isinstance(i, str):\n",
    "                inds.append(i)\n",
    "            else:\n",
    "                inds.append(i.item())\n",
    "\n",
    "        output = model(signal, rris, rri_len) # rris).detach().to(\"cpu\").numpy()\n",
    "        # plot_ecg(signal[0].cpu().numpy())\n",
    "        # plt.show()\n",
    "\n",
    "attention_hook.remove()\n",
    "# attentions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(attentions)\n",
    "len(feas1_val_interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_val_interesting[\"attention\"] = pd.Series(data=attentions, index=inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the attention pooling weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transformer_encoder.layers.\n",
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "attentions = None\n",
    "\n",
    "def hook(module, x, y):\n",
    "    global attentions\n",
    "    print(\"hook\")\n",
    "    attentions = y[1].detach().to(\"cpu\").numpy()\n",
    "\n",
    "attention_hook = model.attention_pooling.attn.register_forward_hook(hook)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels, ind) in enumerate(test_dataloader_safer):\n",
    "        print(signals.shape)\n",
    "        signals = torch.transpose(signals.to(device), 0, 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        labels = labels.long().detach().numpy()\n",
    "        output = model(signals).detach().to(\"cpu\").numpy()\n",
    "\n",
    "        if labels[0] == 0:\n",
    "            print(attentions.shape)\n",
    "            fig = make_subplots(2, 1)\n",
    "            fig.add_trace(go.Scatter(y=signals[:, 0].detach().to(\"cpu\").numpy()), row=1, col=1)\n",
    "            for j in range(attentions.shape[-2]):\n",
    "                fig.add_trace(go.Scatter(y=attentions[0, j, :]), row=2, col=1)\n",
    "            fig.show()\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "attention_hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the final classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weight = model.decoder1.weight.data\n",
    "fc2_weight = model.decoder2.weight.data\n",
    "\n",
    "plt.imshow(fc1_weight.cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = feas1_ecg_data_val\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(lambda x: np.argmax(x))\n",
    "selection = dataset[(dataset[\"class_prediction\"] == 1) & (dataset[\"class_index\"] == 1) & (dataset[\"noise_probs\"]< 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_signal = np.vstack(selection[\"data\"].values)\n",
    "np_rri = np.vstack(selection[\"rri_feature\"].values)\n",
    "rri_len = selection[\"rri_len\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = torch.tensor(np_signal, dtype=torch.float, device=device)\n",
    "rri = torch.tensor(np_rri, dtype=torch.float, device=device)\n",
    "rri_lens = torch.tensor(rri_len, device=device)\n",
    "\n",
    "encoder_out = None\n",
    "\n",
    "def get_encoding(module, x, y):\n",
    "    global encoder_out\n",
    "    print(\"hook\")\n",
    "    print(x[0].shape)\n",
    "    encoder_out = x[0].detach().to(\"cpu\")\n",
    "\n",
    "encoding_hook = model.decoder1.register_forward_hook(get_encoding)\n",
    "\n",
    "output = model(signal, rri, rri_lens)\n",
    "\n",
    "encoding_hook.remove()\n",
    "\n",
    "# Now recreate the output from just the RRI or ECG and see which makes the biggest impact\n",
    "fc1_weight = model.decoder1.weight.data.to(\"cpu\")\n",
    "fc2_weight = model.decoder2.weight.data.to(\"cpu\")\n",
    "\n",
    "\n",
    "ecg_out = fc1_weight[:, :128] @ torch.unsqueeze(encoder_out[:, :128], dim=-1)\n",
    "rri_out = fc1_weight[:, 128:] @ torch.unsqueeze(encoder_out[:, 128:], dim=-1)\n",
    "\n",
    "print(ecg_out.shape)\n",
    "print(ecg_out.shape)\n",
    "\n",
    "ecg_out = nn.functional.relu(ecg_out)\n",
    "rri_out = nn.functional.relu(rri_out)\n",
    "\n",
    "ecg_out = fc2_weight @ ecg_out\n",
    "rri_out = fc2_weight @ rri_out\n",
    "\n",
    "print(ecg_out.shape)\n",
    "print(ecg_out.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"ECG Signal Outputs\")\n",
    "plt.imshow(ecg_out)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"RRI Sequence Outputs\")\n",
    "plt.imshow(rri_out)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE the encoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = []\n",
    "class_indexes = []\n",
    "inds = []\n",
    "\n",
    "def get_encoding(module, x, y):\n",
    "    encoder_out.append(x[0].detach().to(\"cpu\").numpy())\n",
    "\n",
    "encoding_hook = model.decoder1.register_forward_hook(get_encoding)\n",
    "\n",
    "dataloader = test_dataloader_safer\n",
    "\n",
    "with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signal = signals[0].to(device).float()\n",
    "            rris = signals[1].to(device).float()\n",
    "            rri_len = signals[2].to(device).float()\n",
    "\n",
    "            labels = labels.long().detach().numpy()\n",
    "            class_indexes.append(labels)\n",
    "\n",
    "            output = model(signal, rris, rri_len).detach().to(\"cpu\").numpy()\n",
    "            inds.append(ind.cpu().detach().numpy())\n",
    "\n",
    "encoding_hook.remove()\n",
    "\n",
    "encoder_out = np.concatenate(encoder_out, axis=0)\n",
    "class_indexes = np.concatenate(class_indexes, axis=0)\n",
    "inds = np.concatenate(inds, axis=0)\n",
    "\n",
    "test_dataset_safer[\"encodings\"] = pd.Series(data=[encoder_out[i] for i in range(encoder_out.shape[0])], index=inds)\n",
    "\n",
    "tsne = TSNE(perplexity=30)\n",
    "embeddings = tsne.fit_transform(encoder_out)\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=10)\n",
    "embeddings = tsne.fit_transform(encoder_out)\n",
    "\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c=class_indexes, marker=\"x\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation for SAFER (patient wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions for the entire feas1 dataset, with the cross validated models\n",
    "\n",
    "feas1_ecg_predictions = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = TransformerModel(3, embed_dim, n_head, 512, 6, n_fft, n_inp_rri, device=device).to(device)\n",
    "    model.load_state_dict(torch.load(f\"TrainedModels/Transformer_29_May_final_cross_validate_{i}_training_curve.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    feas1_ecg_data_test = pd.read_pickle(f\"C:/Users/daniel/Documents/feas1_train_set_cross_validate_{i}.pk\")\n",
    "    feas1_ecg_data_test_all = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(feas1_ecg_data_test[\"ptID\"])]\n",
    "    print(feas1_ecg_data_test_all[\"class_index\"].value_counts())\n",
    "\n",
    "    feas1_test_dataloader = get_dataloaders(feas1_ecg_data_test_all, 64)\n",
    "\n",
    "    get_predictions(model, feas1_test_dataloader, feas1_ecg_data_test_all)\n",
    "    feas1_ecg_predictions.append(feas1_ecg_data_test_all.drop(\"data\", axis=1))\n",
    "\n",
    "feas1_ecg_predictions = pd.concat(feas1_ecg_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_predictions.to_pickle(\"C:/Users/daniel/Documents/feas1_train_set_cross_validate_all_predictions_even_noisy.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_predictions = pd.read_pickle(\"C:/Users/daniel/Documents/feas1_train_set_cross_validate_all_predictions_even_noisy.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas1_ecg_predictions[\"noise_prediction\"] = feas1_noise_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(feas1_ecg_predictions[feas1_ecg_predictions[\"poss_AF_tag\"] == 1][\"noise_prediction\"].dropna() < 0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ecg_data[ecg_data[\"poss_AF_tag\"] == 1][\"noise_prediction\"] <= 0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data[\"prediction\"] = feas1_ecg_predictions[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_binary_results(conf_mat):\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(conf_mat)\n",
    "\n",
    "    print(f\"Sensitivity: {conf_mat[1, 1] / np.sum(conf_mat[1]):0.3f}\")\n",
    "    print(f\"Specificity: {(conf_mat[0, 0]) / np.sum(conf_mat[0]):0.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"Normal F1: {F1_ind(conf_mat, 0):0.3f}\")\n",
    "    print(f\"AF F1: {F1_ind(conf_mat, 1):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_num_review(pt_ordered_ecg_groups):\n",
    "    total_num_review = 0\n",
    "    for _, g in pt_ordered_ecg_groups:\n",
    "        total_num_review += (g[\"ECGIsAF\"].cumsum() == 0).sum()\n",
    "    return total_num_review\n",
    "\n",
    "def get_pt_conf_mat(pt_data, pt_has_af_review, ground_truth):\n",
    "    pt_data.loc[:, \"pt_prediction_af\"] = False\n",
    "    pt_data.loc[:, \"pt_prediction_af\"] = pt_has_af_review\n",
    "    pt_data[\"pt_prediction_af\"].fillna(False, axis=0, inplace=True)\n",
    "\n",
    "    return confusion_matrix(ground_truth, pt_data[\"pt_prediction_af\"].astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = feas1_pt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data[\"ptDiag\"].value_counts()\n",
    "allowed_pt_diags = [1, 2, 4 ] # [1, 2, 4 ], for feas 1   [1, 3] for feas 2\n",
    "af_pt_diag = 2  # 2 for feas 1, 1 for feas 2\n",
    "\n",
    "pt_data_limited_diag = pt_data[pt_data[\"ptDiag\"].isin(allowed_pt_diags)] # .isin([4, 2, 1])] # limit diagnoses - avoid screening failure and such!   feas2 limitations are [1, 3]\n",
    "\n",
    "dataset = feas1_ecg_predictions[feas1_ecg_predictions[\"noise_prediction\"] < 100].dropna(subset=[\"prediction\"])\n",
    "dataset[\"prob_af\"] = dataset[\"prediction\"].map(lambda x: softmax(x)[1])\n",
    "dataset[\"ECGIsAF\"] = (dataset[\"measDiag\"] == DiagEnum.AF)\n",
    "\n",
    "ground_truth = dataset.groupby(\"ptID\")[\"ECGIsAF\"].any()  # Any patient with at least 1 ECG signal labelled as AF is also AF - some patients were diagnosed with ECGs which were not included in the data?\n",
    "pt_in_dataset = pt_data.loc[ground_truth.index][pt_data[\"ptDiag\"].isin(allowed_pt_diags)]\n",
    "dataset = dataset[dataset[\"ptID\"].isin(pt_in_dataset[\"ptID\"])]\n",
    "ground_truth = dataset.groupby(\"ptID\")[\"ECGIsAF\"].any()  # Any patient with at least 1 ECG signal labelled as AF is also AF - some patients were diagnosed with ECGs which were not included in the data?\n",
    "\n",
    "print(ground_truth.value_counts())\n",
    "print(pt_in_dataset[\"ptDiag\"].value_counts())  # how did these 2 get their AF diagnosis with no AF ECGs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of patients who have at least one of the signals labelled as AF\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(np.argmax)\n",
    "dataset[\"pred_af\"] = dataset[\"class_prediction\"] == 1\n",
    "pt_diagnoses = dataset.groupby(\"ptID\")[\"pred_af\"].any()\n",
    "\n",
    "pt_in_dataset.loc[:, \"pt_prediction_af\"] = pt_diagnoses\n",
    "\n",
    "val_patients = pt_in_dataset.dropna(subset=[\"pt_prediction_af\"])\n",
    "pt_conf_mat = confusion_matrix((val_patients[\"ptDiag\"] == af_pt_diag).values, val_patients[\"pt_prediction_af\"].astype(bool))\n",
    "\n",
    "print_binary_results(pt_conf_mat)\n",
    "\n",
    "print(f\"Max number of ECGs which must be reviewed: {val_patients[val_patients['pt_prediction_af'] == 1]['noHQrecs'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, lets see if the cardiologist reviews only a fixed number of ECGs in order of prob AF.\n",
    "dataset[\"prob_af\"] = dataset[\"prediction\"].map(lambda x: softmax(x)[1])\n",
    "dataset[\"ECGIsAF\"] = (dataset[\"measDiag\"] == DiagEnum.AF)\n",
    "pt_ordered_ecg_groups = dataset.sort_values(\"prob_af\", ascending=False).groupby(\"ptID\")\n",
    "\n",
    "num_ecgs_review = 1000\n",
    "pt_has_af_limited_review = pt_ordered_ecg_groups.head(num_ecgs_review).groupby(\"ptID\")[\"ECGIsAF\"].any()\n",
    "\n",
    "pt_conf_mat = get_pt_conf_mat(pt_in_dataset, pt_has_af_limited_review, ground_truth)\n",
    "print_binary_results(pt_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curve as a function of number of reviews per patient\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "\n",
    "num_patients = len(val_patients.index)\n",
    "max_num_reviews = 8\n",
    "num_af_found = np.zeros(max_num_reviews)\n",
    "num_review = np.arange(max_num_reviews)\n",
    "num_review_avg = np.array([get_total_num_review(pt_ordered_ecg_groups.head(n).groupby(\"ptID\")) for n in num_review])/num_patients\n",
    "\n",
    "for i, num_ecgs_review in enumerate(num_review):\n",
    "    if i == 0:\n",
    "        continue\n",
    "        # IDK why but i dont get a decent answer for 0\n",
    "    pt_has_af_limited_review = pt_ordered_ecg_groups.head(num_ecgs_review).groupby(\"ptID\")[\"ECGIsAF\"].any()\n",
    "\n",
    "    pt_conf_mat = get_pt_conf_mat(pt_in_dataset, pt_has_af_limited_review, ground_truth)\n",
    "    num_af_found[i] = pt_conf_mat[1, 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=250)\n",
    "ax.plot(num_review_avg, num_af_found)\n",
    "ax.plot([0, num_review_avg.max()], [48, 48], linestyle=\"--\")\n",
    "for x, y, T in zip(num_review_avg, num_af_found, num_review):\n",
    "    ax.annotate(f\"{T}\", (x, y))\n",
    "\n",
    "ax.set_ylabel(\"AF patients detected\")\n",
    "ax.set_xlabel(\"Average ECGs reviewed per patient\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot num reviews per AF diagnosis vs num AF diagnosis\n",
    "total_num_review = np.array([get_total_num_review(pt_ordered_ecg_groups.head(n).groupby(\"ptID\")) for n in num_review]) # num_review * num_patients\n",
    "num_review_per_af = total_num_review[1:]/num_af_found[1:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=250)\n",
    "ax.plot(num_af_found[1:], num_review_per_af)\n",
    "ax.plot([48, 48], [0, num_review_per_af.max()],  linestyle=\"--\")\n",
    "\n",
    "ax.set_ylabel(\"Reviews per AF patient\")\n",
    "ax.set_xlabel(\"AF patients found\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(5))\n",
    "\n",
    "# ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_review_limited = total_num_review\n",
    "num_af_found_limited = num_af_found\n",
    "num_review_per_af_limited = num_review_per_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=250)\n",
    "ax.plot(num_review, total_num_review)\n",
    "\n",
    "ax.set_ylabel(\"Total reviews\")\n",
    "ax.set_xlabel(r\"Max ECGs reviewed per patient, $n$\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(200))\n",
    "\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_review * num_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we only use Zenicor flagged ECGs - The system has much worse sensitivity than Zenicor therefore not much benefit to not sending it through Zenicor first!\n",
    "# This is good actually!\n",
    "\n",
    "dataset_flagged = dataset[(dataset[\"poss_AF_tag\"] == 1)]\n",
    "pt_ordered_ecg_groups = dataset_flagged.sort_values(\"prob_af\", ascending=False).groupby(\"ptID\")\n",
    "\n",
    "num_ecgs_review = 3\n",
    "pt_has_af_limited_review = pt_ordered_ecg_groups.head(num_ecgs_review).groupby(\"ptID\")[\"ECGIsAF\"].any()\n",
    "print(get_total_num_review(pt_ordered_ecg_groups))\n",
    "\n",
    "pt_conf_mat = get_pt_conf_mat(pt_in_dataset, pt_has_af_limited_review, ground_truth)\n",
    "print_binary_results(pt_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thresholding - set a limit on p(AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataset[dataset[\"ECGIsAF\"]][\"prob_af\"], alpha=0.7)\n",
    "plt.hist(dataset[~dataset[\"ECGIsAF\"]][\"prob_af\"], alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.5\n",
    "dataset_thresh = dataset[(dataset[\"prob_af\"] > T) & (dataset[\"poss_AF_tag\"] == 1)]\n",
    "pt_ordered_ecg_groups_thresh = dataset_thresh.sort_values(\"prob_af\", ascending=False).groupby(\"ptID\")\n",
    "pt_has_af_thresholded_review = pt_ordered_ecg_groups_thresh[\"ECGIsAF\"].any()\n",
    "print(pt_has_af_thresholded_review.value_counts())\n",
    "\n",
    "pt_conf_mat = get_pt_conf_mat(pt_in_dataset, pt_has_af_thresholded_review, ground_truth)\n",
    "\n",
    "print_binary_results(pt_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curve as a function of number of reviews per patient\n",
    "n_points = 101\n",
    "Ts = np.linspace(0, 1, n_points)\n",
    "num_af_found = np.zeros(n_points)\n",
    "num_reviews = np.zeros(n_points)\n",
    "\n",
    "for i, T in enumerate(Ts):\n",
    "    dataset_thresh = dataset[(dataset[\"prob_af\"] > T) & (dataset[\"poss_AF_tag\"] == 1) & (dataset[\"noise_prediction\"] < 100)]\n",
    "    pt_ordered_ecg_groups_thresh = dataset_thresh.sort_values(\"prob_af\", ascending=False).groupby(\"ptID\")\n",
    "    pt_has_af_thresholded_review = pt_ordered_ecg_groups_thresh[\"ECGIsAF\"].any()\n",
    "\n",
    "    pt_conf_mat = get_pt_conf_mat(pt_in_dataset, pt_has_af_thresholded_review, ground_truth)\n",
    "    num_af_found[i] = pt_conf_mat[1, 1]\n",
    "    num_reviews[i] = get_total_num_review(pt_ordered_ecg_groups_thresh)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=250)\n",
    "ax.plot(num_reviews/len(pt_in_dataset.index), num_af_found)\n",
    "for x, y, T in zip(num_reviews[::20], num_af_found[::20], Ts[::20]):\n",
    "    ax.annotate(f\"{T:.2f}\", (x/len(pt_in_dataset.index), y))\n",
    "\n",
    "ax.plot([0, 4], [48, 48], linestyle=\"--\")\n",
    "\n",
    "ax.set_ylabel(\"AF patients detected\")\n",
    "ax.set_xlabel(\"ECGs reviewed per patient\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlim(left=0, right=1.5)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_af_found[15])\n",
    "print(num_reviews[15])\n",
    "print(Ts[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_num_review_limited\n",
    "# num_af_found_limited\n",
    "num_review_per_af_limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot num reviews per AF diagnosis vs num AF diagnosis\n",
    "total_num_review = num_reviews\n",
    "num_review_per_af = total_num_review/num_af_found\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.5), dpi=250)\n",
    "ax.plot([48, 48], [0, num_review_per_af_limited.max()],  linestyle=\"--\", color=\"#ff7f0e\")\n",
    "ax.plot(num_af_found_limited[1:], num_review_per_af_limited, color=\"#2ca02c\", label=\"Limited reviews per patient\")\n",
    "ax.plot(num_af_found, num_review_per_af, color=\"#1f77b4\", label=\"Probability threshold\")\n",
    "\n",
    "# ax.legend()\n",
    "\n",
    "ax.set_ylabel(\"Reviews per AF patient\")\n",
    "ax.set_xlabel(\"AF patients found\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlim(left=40)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(5))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_af_found[14:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_review_per_af[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats going on with the AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = SAFERDataset.load_pt_dataset(1)\n",
    "ecg_data = SAFERDataset.load_ecg_csv(1, pt_data, ecg_range=None, ecg_meas_diag=None, feas2_offset=10000, feas2_ecg_offset=200000)\n",
    "\n",
    "ecg_data[\"ECGIsAF\"] = ecg_data[\"measDiag\"] == DiagEnum.AF\n",
    "print(ecg_data.groupby(\"ptID\")[\"ECGIsAF\"].any().value_counts())\n",
    "\n",
    "ecg_data[\"feas\"] = 1\n",
    "ecg_data[\"length\"] = 9120\n",
    "ecg_data[\"rri_len\"] = 20\n",
    "\n",
    "# this removes a number of the AF samples, IDK where the other AF samples are going!\n",
    "pt_data, ecg_data = prepare_safer_data(pt_data, ecg_data)\n",
    "# train_pts, test_pts, val_pts = generate_patient_splits(pt_data, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data[\"ECGIsAF\"] = ecg_data[\"measDiag\"] == DiagEnum.AF\n",
    "ecg_data.groupby(\"ptID\")[\"ECGIsAF\"].any().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data = ecg_data.loc[feas1_noise_predictions.index][feas1_noise_predictions < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data[\"ECGIsAF\"] = ecg_data[\"measDiag\"] == DiagEnum.AF\n",
    "ecg_data.groupby(\"ptID\")[\"ECGIsAF\"].any().value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
