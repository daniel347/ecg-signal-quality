{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.io\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from DiagEnum import DiagEnum\n",
    "import SAFERDataset\n",
    "\n",
    "from Utilities.Plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the SAFER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data, feas2_ecg_data = SAFERDataset.load_feas_dataset(2, \"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_ecg_data[\"class_index\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the undecided data with zenicor labels - doesnt seem to help\n",
    "feas2_ecg_data[\"class_index\"] = feas2_ecg_data[\"class_index\"].where(feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided, feas2_ecg_data[\"tag_orig_Poor_Quality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data.index = feas2_pt_data[\"ptID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data[\"noRecs\"] = feas2_ecg_data[\"ptID\"].value_counts()\n",
    "feas2_pt_data[\"noHQrecs\"] = feas2_ecg_data[feas2_ecg_data[\"class_index\"] == 0][\"ptID\"].value_counts()\n",
    "\n",
    "feas2_pt_data[\"noHQrecsNotUndecided\"] = feas2_ecg_data[(feas2_ecg_data[\"class_index\"] == 0) & (feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided)][\"ptID\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data[[\"noRecs\", \"noHQrecs\", \"noHQrecsNotUndecided\"]] = feas2_pt_data[[\"noRecs\", \"noHQrecs\", \"noHQrecsNotUndecided\"]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_ecg_data[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(feas2_pt_data[\"noRecs\"] - feas2_pt_data[\"noHQrecs\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_ecg_data[(feas2_ecg_data[\"ptID\"] == 18) & (feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_ecg_data[\"ptID\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.ops import sigmoid_focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cuda\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a model\n",
    "hyperparameters = {\"lr\": 0.00075, \"k1\": 17, \"k2\": 11, \"k3\": 5, \"k4\": 3, \"k5\": 3, \"k6\": 3,\n",
    "                   \"c1\": 128, \"c2\": 256, \"c3\": 256, \"c4\": 128, \"c5\": 64, \"c6\": 32,\n",
    "                   \"lstm_n_hidden\": 32, \"dense1\": 256, \"dense2\": 16, \"gamma\": 0.5, \"sched_gamma\": 0.5, \"sched_step\": 6}\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, k1, k2, k3, k4, k5, k6, c1, c2, c3, c4, c5, c6, lstm_n_hidden, dense2, **_):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_section1 = nn.Sequential(\n",
    "            nn.Conv1d(1, c1, k1, stride=4, padding=math.ceil((k1 - 4)/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(c1)\n",
    "        )\n",
    "\n",
    "        self.conv_section2 = nn.Sequential(\n",
    "            nn.Conv1d(c1, c2, k2, stride=2, padding=math.ceil((k1 - 2)/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(c2)\n",
    "        )\n",
    "\n",
    "        self.conv_section3 = nn.Sequential(\n",
    "            nn.Conv1d(c2, c3, k3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(c3)\n",
    "        )\n",
    "\n",
    "        self.conv_section4 = nn.Sequential(\n",
    "            nn.Conv1d(c3, c4, k4, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            nn.BatchNorm1d(c4)\n",
    "        )\n",
    "\n",
    "        self.conv_section5 = nn.Sequential(\n",
    "            nn.Conv1d(c4, c5, k5, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            nn.BatchNorm1d(c5)\n",
    "        )\n",
    "\n",
    "        self.conv_section6 = nn.Sequential(\n",
    "            nn.Conv1d(c5, c6, k6, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(c6)\n",
    "        )\n",
    "\n",
    "        self.lstm_n_hidden = lstm_n_hidden\n",
    "        self.lstm = nn.LSTM(input_size=c6, hidden_size=lstm_n_hidden, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dense2 = nn.Linear(2*lstm_n_hidden, dense2)\n",
    "        self.dense3 = nn.Linear(dense2, 1)\n",
    "\n",
    "        self.activation = nn.ELU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def init_lstm_hidden(self, batch_size, device):\n",
    "        # This resets the LSTM hidden state after each batch\n",
    "        hidden_state = torch.zeros(2, batch_size, self.lstm_n_hidden, device=device)\n",
    "        cell_state = torch.zeros(2, batch_size, self.lstm_n_hidden, device=device)\n",
    "        return (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # [1, 9120]\n",
    "        x = self.conv_section1(x)\n",
    "\n",
    "        # [512, 1140]\n",
    "        x = self.conv_section2(x)\n",
    "\n",
    "        # [256, 570]\n",
    "        x = self.conv_section3(x)\n",
    "\n",
    "        # [128, 285]\n",
    "        x = self.conv_section4(x)\n",
    "\n",
    "        # [64, 143]\n",
    "        x = self.conv_section5(x)\n",
    "\n",
    "        # [32, 72]\n",
    "        x = self.conv_section6(x)\n",
    "\n",
    "        # [32, 36]\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        x, (h, _) = self.lstm(x, self.init_lstm_hidden(x.shape[0], x.device))\n",
    "        x = torch.flatten(torch.transpose(h, 0, 1), 1, 2) # torch.flatten(x, 1, -1)\n",
    "        # x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        # [1152]\n",
    "        x = self.dense2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # [128]\n",
    "        x = self.dense3(x)\n",
    "        # x = self.logsoftmax(x)\n",
    "\n",
    "        # [1]\n",
    "        return x\n",
    "\n",
    "resnet_hyperparameters = {\"k1\": 17, \"c1\": 64, \"c2\": 128, \"c3\": 196, \"c4\": 256, \"c5\": 384, \"c6\": 512, \"dense2\": 64}\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, k1, c1, c2, c3, c4, c5, c6, dense2, **_):\n",
    "        super(ResNetCNN, self).__init__()\n",
    "\n",
    "        self.conv_section1 = nn.Sequential(\n",
    "            nn.Conv1d(1, c1, k1, stride=4, padding=math.ceil((k1 - 4)/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.BatchNorm1d(c1)\n",
    "        )\n",
    "\n",
    "        self.conv_section2 = nn.Sequential(\n",
    "            nn.Conv1d(c1, c1, 3,padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c1),\n",
    "            nn.Conv1d(c1, c2, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c2),\n",
    "            nn.Conv1d(c2, c2, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c2),\n",
    "        )\n",
    "\n",
    "        self.bypass_conv2 = nn.Conv1d(c1, c2, 1, padding=\"same\")\n",
    "\n",
    "        self.conv_section3 = nn.Sequential(\n",
    "            nn.Conv1d(c2, c2, 3,padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c2),\n",
    "            nn.Conv1d(c2, c3, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c3),\n",
    "        )\n",
    "\n",
    "        self.bypass_conv3 = nn.Conv1d(c2, c3, 1, padding=\"same\")\n",
    "\n",
    "        self.conv_section4 = nn.Sequential(\n",
    "            nn.Conv1d(c3, c3, 3,padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c3),\n",
    "            nn.Conv1d(c3, c4, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c4),\n",
    "        )\n",
    "\n",
    "        self.bypass_conv4 = nn.Conv1d(c3, c4, 1, padding=\"same\")\n",
    "\n",
    "        self.conv_section5 = nn.Sequential(\n",
    "            nn.Conv1d(c4, c4, 3,padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c4),\n",
    "            nn.Conv1d(c4, c5, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c5),\n",
    "        )\n",
    "\n",
    "        self.bypass_conv5 = nn.Conv1d(c4, c5, 1, padding=\"same\")\n",
    "\n",
    "        self.conv_section6 = nn.Sequential(\n",
    "            nn.Conv1d(c5, c6, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(c6)\n",
    "        )\n",
    "\n",
    "        self.bypass_conv6 = nn.Conv1d(c5, c6, 1, padding=\"same\")\n",
    "\n",
    "        # self.lstm_n_hidden = lstm_n_hidden\n",
    "        # self.lstm = nn.LSTM(input_size=c6, hidden_size=lstm_n_hidden, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dense2 = nn.Linear(c6, dense2)\n",
    "        self.dense3 = nn.Linear(dense2, 1)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def init_lstm_hidden(self, batch_size, device):\n",
    "        # This resets the LSTM hidden state after each batch\n",
    "        hidden_state = torch.zeros(2, batch_size, self.lstm_n_hidden, device=device)\n",
    "        cell_state = torch.zeros(2, batch_size, self.lstm_n_hidden, device=device)\n",
    "        return (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # [1, 9120]\n",
    "        x = self.conv_section1(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [512, 1140]\n",
    "        x = self.conv_section2(x) + self.bypass_conv2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [256, 570]\n",
    "        x = self.conv_section3(x) + self.bypass_conv3(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [128, 285]\n",
    "        x = self.conv_section4(x) + self.bypass_conv4(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [64, 143]\n",
    "        x = self.conv_section5(x) + self.bypass_conv5(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [32, 72]\n",
    "        x = self.conv_section6(x) + self.bypass_conv6(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [32, 36]\n",
    "        x = torch.mean(x, dim=-1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # [1152]\n",
    "        x = self.dense2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # [128]\n",
    "        x = self.dense3(x)\n",
    "        # x = self.logsoftmax(x)\n",
    "\n",
    "        # [1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"data\"]\n",
    "        y = row[\"class_index\"]\n",
    "        ind = row.name\n",
    "\n",
    "        return X, y, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, val_frac, only_clean_training=True):\n",
    "    pt_data[\"noLQrecs\"] = pt_data[\"noRecs\"] - pt_data[\"noHQrecs\"]  # for Feas1 this might include stuff flagged by zenicor as noisy?\n",
    "\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "    val_patients = []\n",
    "\n",
    "    test_val_frac = test_frac + val_frac\n",
    "    val_second_frac = val_frac/test_val_frac\n",
    "\n",
    "    test_lq_count = 0\n",
    "    test_total_count = 0\n",
    "\n",
    "    val_lq_count = 0\n",
    "    val_total_count = 0\n",
    "\n",
    "    train_lq_count = 0\n",
    "    train_total_count = 0\n",
    "\n",
    "    test_val_ratio_overall = pt_data[\"noLQrecs\"].sum() / (pt_data[\"noLQrecs\"].sum() + pt_data[\"noHQrecsNotUndecided\"].sum())\n",
    "    train_ratio_overall = pt_data[\"noLQrecs\"].sum() / pt_data[\"noRecs\"].sum()\n",
    "\n",
    "    for val, df in list(pt_data.groupby(\"noLQrecs\"))[::-1]:\n",
    "        print(f\"processing {val}\")\n",
    "        print(f\"number of patients {len(df.index)}\")\n",
    "\n",
    "        n_test = math.floor(len(df.index) * test_frac)\n",
    "        n_val = math.floor(len(df.index) * val_frac)\n",
    "        n_train = math.floor(len(df.index) * (1 - test_frac - val_frac))\n",
    "\n",
    "        test_lq_count += n_test * val\n",
    "        val_lq_count += n_val * val\n",
    "        train_lq_count += n_train * val\n",
    "\n",
    "        test_sample = df.sample(n_test)\n",
    "        val_sample = df[~df[\"ptID\"].isin(test_sample[\"ptID\"])].sample(n_val)\n",
    "        train_sample = df[(~df[\"ptID\"].isin(test_sample[\"ptID\"])) & (~df[\"ptID\"].isin(val_sample[\"ptID\"]))].sample(n_train)\n",
    "\n",
    "        test_patients.append(test_sample)\n",
    "        val_patients.append(val_sample)\n",
    "        train_patients.append(train_sample)\n",
    "\n",
    "        residual_samples = df[(~df[\"ptID\"].isin(test_sample[\"ptID\"])) & (~df[\"ptID\"].isin(val_sample[\"ptID\"])) & (~df[\"ptID\"].isin(train_sample[\"ptID\"]))]\n",
    "\n",
    "        test_total_count += test_sample[\"noHQrecsNotUndecided\"].sum() + test_sample[\"noLQrecs\"].sum()\n",
    "        val_total_count += val_sample[\"noHQrecsNotUndecided\"].sum() + val_sample[\"noLQrecs\"].sum()\n",
    "        train_total_count += train_sample[\"noRecs\"].sum()\n",
    "\n",
    "        for _, pt in residual_samples.iterrows():\n",
    "            pt_ratio_not_undecided = pt[\"noLQrecs\"] / (pt[\"noHQrecsNotUndecided\"] +  pt[\"noLQrecs\"])\n",
    "\n",
    "            test_ratio = test_lq_count/test_total_count\n",
    "            val_ratio = val_lq_count/val_total_count\n",
    "\n",
    "            if math.isnan(test_ratio):\n",
    "                test_ratio = 0\n",
    "            if math.isnan(val_ratio):\n",
    "                val_ratio = 0\n",
    "\n",
    "            print(pt_ratio_not_undecided)\n",
    "            print(test_ratio)\n",
    "            print(val_ratio)\n",
    "\n",
    "            if pt_ratio_not_undecided > test_val_ratio_overall:\n",
    "                # Choose smallest of test and val_dataset\n",
    "                min_test_val = min(test_ratio, val_ratio)\n",
    "                if min_test_val < test_val_ratio_overall:\n",
    "                    # If test or val are lacking noisy samples\n",
    "                    if min_test_val == test_ratio:\n",
    "                        # Add to test\n",
    "                        print(\"Adding to test 1\")\n",
    "                        test_patients[-1] = test_patients[-1].append(pt)\n",
    "                        test_lq_count += pt[\"noLQrecs\"]\n",
    "                        test_total_count += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "                    else:\n",
    "                        # Add to va\n",
    "                        print(\"Adding to val 1\")\n",
    "                        val_patients[-1] = val_patients[-1].append(pt)\n",
    "                        val_lq_count += pt[\"noLQrecs\"]\n",
    "                        val_total_count += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "                else:\n",
    "                    # Add to train\n",
    "                    print(\"Adding to train 1\")\n",
    "                    train_patients[-1] = train_patients[-1].append(pt)\n",
    "                    train_lq_count += pt[\"noLQrecs\"]\n",
    "                    train_total_count += pt[\"noRecs\"]\n",
    "            else:\n",
    "                # Choose the maximum of\n",
    "                max_test_val = min(test_ratio, val_ratio)\n",
    "                if max_test_val > test_val_ratio_overall:\n",
    "                    # If test or val are lacking noisy samples\n",
    "                    if max_test_val == test_ratio:\n",
    "                        # Add to test\n",
    "                        print(\"Adding to test 2\")\n",
    "                        test_patients[-1] = test_patients[-1].append(pt)\n",
    "                        test_lq_count += pt[\"noLQrecs\"]\n",
    "                        test_total_count += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "                    else:\n",
    "                        # Add to va\n",
    "                        print(\"Adding to val 2\")\n",
    "                        val_patients[-1] = val_patients[-1].append(pt)\n",
    "                        val_lq_count += pt[\"noLQrecs\"]\n",
    "                        val_total_count += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "                else:\n",
    "                    # Add to train\n",
    "                    print(\"Adding to train 2\")\n",
    "                    train_patients[-1] = train_patients[-1].append(pt)\n",
    "                    train_lq_count += pt[\"noLQrecs\"]\n",
    "                    train_total_count += pt[\"noRecs\"]\n",
    "\n",
    "\n",
    "    train_pt_df = pd.concat(train_patients)\n",
    "    test_pt_df = pd.concat(test_patients)\n",
    "    val_pt_df = pd.concat(val_patients)\n",
    "\n",
    "    print(f\"Test high quality: {test_pt_df['noHQrecsNotUndecided'].sum()} low quality: {test_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Train high quality: {train_pt_df['noHQrecsNotUndecided'].sum()} low quality: {train_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Val high quality: {val_pt_df['noHQrecsNotUndecided'].sum()} low quality: {val_pt_df['noLQrecs'].sum()}\")\n",
    "\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "    val_dataloader = None\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "    val_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        # get ECG datasets\n",
    "        #  & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)\n",
    "        train_dataset = ecg_data[(ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"]))]\n",
    "        # Normalise\n",
    "        train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "        if only_clean_training:\n",
    "            torch_dataset_train = Dataset(train_dataset[train_dataset[\"class_index\"] == 0])\n",
    "        else:\n",
    "            torch_dataset_train = Dataset(train_dataset)\n",
    "\n",
    "        train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = ecg_data[(ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "        test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_test = Dataset(test_dataset)\n",
    "        test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not val_pt_df.empty:\n",
    "        val_dataset = ecg_data[(ecg_data[\"ptID\"].isin(val_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "        val_dataset[\"data\"] = (val_dataset[\"data\"] - val_dataset[\"data\"].map(lambda x: x.mean()))/val_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_val = Dataset(val_dataset)\n",
    "        val_dataloader = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader, train_dataset, test_dataset, val_dataset, test_pt_df, train_pt_df, val_pt_df\n",
    "\n",
    "train_dataloader, test_dataloader, val_dataloader, train_dataset, test_dataset, val_dataset, test_pt_df, train_pt_df, val_pt_df = make_SAFER_dataloaders(feas2_pt_data, feas2_ecg_data, test_frac=0.15, val_frac=0.15, only_clean_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, val_frac, only_clean_training=True):\n",
    "    pt_data[\"noLQrecs\"] = pt_data[\"noRecs\"] - pt_data[\"noHQrecs\"]  # for Feas1 this might include stuff flagged by zenicor as noisy?\n",
    "\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "    val_patients = []\n",
    "\n",
    "    test_val_frac = test_frac + val_frac\n",
    "    val_second_frac = val_frac/test_val_frac\n",
    "\n",
    "    lq_counts = np.array([0, 0, 0], dtype=int)\n",
    "    total_counts = np.array([0, 0, 0], dtype=int)\n",
    "\n",
    "    fracs = np.array([1-test_frac - val_frac, test_frac, val_frac])\n",
    "\n",
    "\n",
    "\n",
    "    total_lq_count = 0\n",
    "    total_total_count = 0\n",
    "\n",
    "    test_val_ratio_overall = pt_data[\"noLQrecs\"].sum() / (pt_data[\"noLQrecs\"].sum() + pt_data[\"noHQrecsNotUndecided\"].sum())\n",
    "    train_ratio_overall = pt_data[\"noLQrecs\"].sum() / pt_data[\"noRecs\"].sum()\n",
    "\n",
    "    for val, pt in pt_data.iterrows():\n",
    "        total_total_count += pt[\"noHQrecsNotUndecided\"]\n",
    "        total_lq_count += pt[\"noLQrecs\"]\n",
    "\n",
    "        exp_total_counts = total_total_count * fracs\n",
    "        exp_lq_counts = total_lq_count * fracs\n",
    "\n",
    "        loss_0 = np.sum(np.abs(lq_counts + np.array([pt[\"noLQrecs\"], 0, 0]) - exp_lq_counts) + np.abs(total_counts + np.array([pt[\"noHQrecsNotUndecided\"], 0, 0]) - exp_total_counts))\n",
    "        loss_1 = np.sum(np.abs(lq_counts + np.array([0, pt[\"noLQrecs\"], 0]) - exp_lq_counts) + np.abs(total_counts + np.array([0, pt[\"noHQrecsNotUndecided\"], 0]) - exp_total_counts))\n",
    "        loss_2 = np.sum(np.abs(lq_counts + np.array([0, 0, pt[\"noLQrecs\"]]) - exp_lq_counts) + np.abs(total_counts + np.array([0, 0, pt[\"noHQrecsNotUndecided\"]]) - exp_total_counts))\n",
    "\n",
    "        min_loss = min([loss_0, loss_1, loss_2])\n",
    "\n",
    "        if min_loss == loss_0:\n",
    "            train_patients.append(pt)\n",
    "            lq_counts += np.array([pt[\"noLQrecs\"], 0, 0], dtype=int)\n",
    "            total_counts += np.array([pt[\"noHQrecsNotUndecided\"], 0, 0], dtype=int)\n",
    "        elif min_loss == loss_1:\n",
    "            test_patients.append(pt)\n",
    "            lq_counts += np.array([0, pt[\"noLQrecs\"], 0], dtype=int)\n",
    "            total_counts += np.array([0, pt[\"noHQrecsNotUndecided\"], 0], dtype=int)\n",
    "        else:\n",
    "            val_patients.append(pt)\n",
    "            lq_counts += np.array([0, 0,  pt[\"noLQrecs\"]], dtype=int)\n",
    "            total_counts += np.array([0, 0, pt[\"noHQrecsNotUndecided\"]], dtype=int)\n",
    "\n",
    "    train_pt_df = pd.DataFrame(train_patients)\n",
    "    test_pt_df = pd.DataFrame(test_patients)\n",
    "    val_pt_df = pd.DataFrame(val_patients)\n",
    "\n",
    "    print(f\"Test high quality: {test_pt_df['noHQrecsNotUndecided'].sum()} low quality: {test_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Train high quality: {train_pt_df['noHQrecsNotUndecided'].sum()} low quality: {train_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Val high quality: {val_pt_df['noHQrecsNotUndecided'].sum()} low quality: {val_pt_df['noLQrecs'].sum()}\")\n",
    "\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "    val_dataloader = None\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "    val_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        # get ECG datasets\n",
    "        #  & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)\n",
    "        train_dataset = ecg_data[(ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"]))]\n",
    "        # Normalise\n",
    "        # train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "        if only_clean_training:\n",
    "            torch_dataset_train = Dataset(train_dataset[train_dataset[\"class_index\"] == 0])\n",
    "        else:\n",
    "            torch_dataset_train = Dataset(train_dataset)\n",
    "\n",
    "        train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = ecg_data[(ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "        # test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_test = Dataset(test_dataset)\n",
    "        test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not val_pt_df.empty:\n",
    "        val_dataset = ecg_data[(ecg_data[\"ptID\"].isin(val_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "        # val_dataset[\"data\"] = (val_dataset[\"data\"] - val_dataset[\"data\"].map(lambda x: x.mean()))/val_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_val = Dataset(val_dataset)\n",
    "        val_dataloader = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader, train_dataset, test_dataset, val_dataset, test_pt_df, train_pt_df, val_pt_df\n",
    "\n",
    "train_dataloader, test_dataloader, val_dataloader, train_dataset, test_dataset, val_dataset, test_pt_df, train_pt_df, val_pt_df = make_SAFER_dataloaders(feas2_pt_data, feas2_ecg_data, test_frac=0.15, val_frac=0.15, only_clean_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[train_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "\n",
    "# Label smoothing on the samples we don't know:\n",
    "# train_dataset.loc[(train_dataset[\"measDiag\"] == DiagEnum.Undecided) & (train_dataset[\"class_index\"] == 0), \"class_index\"] = 0.3\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataset[\"tag_orig_Poor_Quality\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pt_df[\"noHQrecs\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[\"class_index\"].value_counts())\n",
    "print(test_dataset[\"class_index\"].value_counts())\n",
    "print(val_dataset[\"class_index\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_test_dataset = test_dataset\n",
    "good_val_dataset = val_dataset\n",
    "good_train_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_pt_data[\"noHQrecsNotUndecided\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " feas2_pt_data[\"noLQrecs\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use only the labelled portions of feas2 data for training and testing\n",
    "\n",
    "feas2_ecg_data_no_undecided = feas2_ecg_data[feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided]\n",
    "\n",
    "feas2_pt_data_no_undecided = feas2_pt_data.copy()\n",
    "feas2_pt_data_no_undecided[\"noRecs\"] = feas2_ecg_data_no_undecided[\"ptID\"].value_counts()\n",
    "feas2_pt_data_no_undecided[\"noHQRecs\"] = feas2_ecg_data_no_undecided[feas2_ecg_data_no_undecided[\"measDiag\"] != DiagEnum.PoorQuality][\"ptID\"].value_counts()\n",
    "\n",
    "train_dataloader, test_dataloader, train_dataset, test_dataset = make_SAFER_dataloaders(feas2_pt_data_no_undecided, feas2_ecg_data_no_undecided, test_frac=0.2, only_clean_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas2_ecg_data_unbalanced = feas2_ecg_data_unbalanced.drop(train_ecgs.index)\n",
    "feas2_ecg_data_unbalanced[\"class_index\"] = feas2_ecg_data_unbalanced[\"measDiag\"].map(lambda x: int(x == DiagEnum.PoorQuality))\n",
    "\n",
    "torch_dataset_test_unbalanced = Dataset(feas2_ecg_data_unbalanced)\n",
    "test_unbalanced_dataloader = DataLoader(torch_dataset_test_unbalanced, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "model = CNN(**hyperparameters).to(device)\n",
    "\n",
    "# Use weightings to handle class imbalance\n",
    "\n",
    "\"\"\"\n",
    "class_counts = torch.tensor(train_dataset[\"class_index\"].value_counts().values.astype(np.float32))\n",
    "a = class_counts[0]/(class_counts[0] + class_counts[1])\n",
    "\"\"\"\n",
    "\n",
    "# If using label smoothing its a bit fiddly:\n",
    "a = (len(train_dataset.index) - train_dataset[\"class_index\"].round().sum())/len(train_dataset.index)\n",
    "\n",
    "a_test_counts = torch.tensor(feas2_ecg_data[feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided][\"class_index\"].value_counts().values.astype(np.float32))\n",
    "a_test = a_test_counts[0]/(a_test_counts[0] + a_test_counts[1])\n",
    "\n",
    "print(a, a_test)\n",
    "\n",
    "class binary_focal_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, _alpha, _gamma):\n",
    "        super(binary_focal_loss, self).__init__()\n",
    "        self.BCE_loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.alpha = _alpha\n",
    "        self.gamma = _gamma\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "        bce = self.BCE_loss(pred, targets)\n",
    "        prob_correct = torch.exp(-bce)\n",
    "        loss_unweighted = (1.0 - prob_correct)**self.gamma * bce\n",
    "        loss_weighted = torch.where(targets == 1,\n",
    "                           loss_unweighted * self.alpha,\n",
    "                           loss_unweighted * (1-self.alpha))\n",
    "        return torch.mean(loss_weighted)\n",
    "\n",
    "weight_modifier = 1\n",
    "loss_func = binary_focal_loss((1 - (1-a) * weight_modifier), 0)\n",
    "test_loss_func = binary_focal_loss(a_test, 0)\n",
    "\n",
    "# Hyperparameter optimisation for new model\n",
    "\"\"\"\n",
    "lr: 0.00862260094056823\n",
    "gamma: 0.06229441216331977\n",
    "sched_gamma: 1.0\n",
    "sched_step: 16\n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)\n",
    "scheduler = StepLR(optimizer, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model = model.to(device)\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, loss_func, test_loss_func, optimizer, scheduler):\n",
    "    best_test_loss = 100\n",
    "    best_epoch = -1\n",
    "    best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"starting epoch {epoch} ...\")\n",
    "        # Train\n",
    "        num_batches = 0\n",
    "        model.train()\n",
    "        for i, (signals, labels, _) in enumerate(train_dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(signals).to(\"cpu\")[:, 0]\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the number of signals in this batch to a counter\n",
    "            num_batches += signals.shape[0]\n",
    "            total_loss += float(loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "        print(\"Testing ...\")\n",
    "        # Test\n",
    "        num_test_batches = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, (signals, labels, _) in enumerate(test_dataloader):\n",
    "                signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "                # fft = torch.abs(torch.fft.fft(signals))\n",
    "                # signals = torch.cat([signals, fft], dim=1)\n",
    "                labels = labels.float()\n",
    "                output = model(signals).to(\"cpu\")[:, 0]\n",
    "                loss = test_loss_func(output, labels)\n",
    "                test_loss += float(loss)\n",
    "                num_test_batches += signals.shape[0]\n",
    "\n",
    "        print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "        losses.append([total_loss/num_batches, test_loss/num_test_batches])\n",
    "\n",
    "        if test_loss/num_test_batches < best_test_loss:\n",
    "            best_model = copy.deepcopy(model).cpu()\n",
    "            best_test_loss = test_loss/num_test_batches\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            if best_epoch + 20 <= epoch:\n",
    "                return best_model, losses\n",
    "        scheduler.step()\n",
    "\n",
    "    return best_model, losses\n",
    "\n",
    "model, losses = train(model, train_dataloader, test_dataloader, loss_func, loss_func, optimizer, scheduler)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_losses = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = last_losses + losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curve (2 seperate axes)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "train_l = ax.plot([l[0] for l in losses], label=\"training loss\")\n",
    "val_l = ax2.plot([l[1] for l in losses], label=\"validation loss\", color=\"#ff7f0e\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_ylabel(\"Training loss\")\n",
    "ax2.set_ylabel(\"Validation loss\")\n",
    "\n",
    "ax.legend(train_l + val_l, [\"training loss\", \"validation loss\"])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training curve (1 axis only)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "train_l = ax.plot([l[0] for l in losses[:150]], label=\"training loss\")\n",
    "val_l = ax.plot([l[1] for l in losses[:150]], label=\"validation loss\", color=\"#ff7f0e\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np_losses = np.array(losses)\n",
    "np.save(\"TrainedModels/CNN_16_may_TMR_no_undecided_training_curve\", np_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAFER_data_path = r\"D:\\2022_23_DSiromani\\Feas2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "torch.save(model.state_dict(), \"TrainedModels/CNN_16_may_TMR_no_undecided_unfortunately_good.pt\")\n",
    "\n",
    "# train_dataset.to_pickle(os.path.join(SAFER_data_path, \"CNN_16_may_TMR_no_undecided_train.pk\"))\n",
    "# test_dataset.to_pickle(os.path.join(SAFER_data_path, \"CNN_16_may_TMR_no_undecided_test.pk\"))\n",
    "# val_dataset.to_pickle(os.path.join(SAFER_data_path, \"CNN_16_may_TMR_no_undecided_val.pk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = CNN(**hyperparameters).to(device)\n",
    "model.load_state_dict(torch.load(\"TrainedModels/CNN_16_may_final_no_undecided.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = pd.read_pickle(os.path.join(SAFER_data_path, \"CNN_16_may_TMR_no_undecided.pk\"))\n",
    "test_dataset = pd.read_pickle(os.path.join(SAFER_data_path, \"CNN_16_may_TMR_no_undecided.pk\"))\n",
    "val_dataset = pd.read_pickle(os.path.join(SAFER_data_path, \"CNN_16_may_TMR_no_undecided.pk\"))\n",
    "\n",
    "torch_dataset_test = Dataset(test_dataset)\n",
    "test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset)\n",
    "val_dataloader = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a training set and get a test set from its exclusion\n",
    "train_dataset = pd.read_pickle(\"TrainedModels/CNN_AlexNet_LSTM_Bidirectional_Final_States_feas2_train_set.pk\")\n",
    "\n",
    "train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_pt_df = feas2_pt_data[~feas2_pt_data[\"ptID\"].isin(train_dataset[\"ptID\"])]\n",
    "\n",
    "if not test_pt_df.empty:\n",
    "    test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "    test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "    torch_dataset_test = Dataset(test_dataset)\n",
    "    test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garbage collection - in case of CUDA out of memory error\n",
    "import gc\n",
    "model = None\n",
    "signals = None\n",
    "labels = None\n",
    "gc.collect() # Python thing\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"prediction\"] = None\n",
    "\n",
    "def get_predictions(model, dataloader, dataset):\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    outputs = []\n",
    "    inds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "            labels = labels.detach().numpy()\n",
    "            true_labels.append(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(signals).detach().to(\"cpu\").numpy()\n",
    "\n",
    "            prediction = output # np.argmax(output, axis=-1)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            for i, o in zip(ind, output):\n",
    "                outputs.append(o[0])\n",
    "                inds.append(int(i))\n",
    "\n",
    "    dataset[\"prediction\"] = pd.Series(data=outputs, index=inds)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    true_labels = np.concatenate(true_labels)\n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "predictions, true_labels = get_predictions(model, val_dataloader, val_dataset)\n",
    "conf_mat = confusion_matrix(true_labels, predictions > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConfusionMatrixDisplay.from_predictions(true_labels, predictions, display_labels=[\"sufficint quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "\n",
    "# Same as the below function (as described in CinC)\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "\"\"\"\n",
    "def bin_F1_score(conf_mat, ind):\n",
    "    return conf_mat[ind, ind]/(conf_mat[ind, ind] + 0.5 * (conf_mat[0, 1] + conf_mat[1, 0]))\n",
    "\n",
    "print(f\"Normal F1: {bin_F1_score(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {bin_F1_score(conf_mat, 1)}\")\n",
    "\"\"\"\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1])}\")\n",
    "print(f\"Specificity: {conf_mat[0, 0]/np.sum(conf_mat[0])}\")\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check Zenicors F1 score (again)\n",
    "conf_mat_zenicor = confusion_matrix(val_dataset[\"class_index\"], val_dataset[\"tag_orig_Poor_Quality\"])\n",
    "print(conf_mat_zenicor)\n",
    "\n",
    "print(f\"Sensitivity: {conf_mat_zenicor[1, 1]/np.sum(conf_mat_zenicor[1])}\")\n",
    "print(f\"Specificity: {conf_mat_zenicor[0, 0]/np.sum(conf_mat_zenicor[0])}\")\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat_zenicor, 0)}\")\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat_zenicor, 1)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_mat = np.array([[97, 18], [3, 71]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the projected errors in safer\n",
    "\n",
    "p_noisy_given_noisy = conf_mat[1, 1]/(conf_mat[1, 1] + conf_mat[1, 0])\n",
    "p_noisy_given_not_noisy = conf_mat[0, 1]/(conf_mat[0, 0] + conf_mat[0, 1])\n",
    "p_not_noisy_given_noisy = conf_mat[1, 0]/(conf_mat[1, 1] + conf_mat[1, 0])\n",
    "\n",
    "print(p_noisy_given_not_noisy)\n",
    "print(p_not_noisy_given_noisy)\n",
    "\n",
    "total_num_noisy = feas2_ecg_data[\"class_index\"].value_counts()[1]\n",
    "total_num_clean = feas2_ecg_data[feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided][\"class_index\"].value_counts()[0]\n",
    "\n",
    "print(total_num_noisy, total_num_clean)\n",
    "\n",
    "num_correct_noisy = int(round(total_num_noisy * p_noisy_given_noisy))\n",
    "num_fp = int(round(total_num_clean * p_noisy_given_not_noisy))\n",
    "num_fn = int(round(total_num_noisy * p_not_noisy_given_noisy))\n",
    "\n",
    "print(f\"num_correct_noisy: {num_correct_noisy}\")\n",
    "print(f\"num_false_positive: {num_fp}\")\n",
    "print(f\"num_false_negative: {num_fn}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many of the AF samples have been incorrectly classified as noisy\n",
    "\n",
    "print(val_dataset[val_dataset[\"measDiag\"] == DiagEnum.AF][\"prediction\"])\n",
    "print(test_dataset[test_dataset[\"measDiag\"] == DiagEnum.AF][\"prediction\"])\n",
    "print(train_dataset[train_dataset[\"measDiag\"] == DiagEnum.AF][\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_pt_data.loc[15]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_pts = val_dataset[(val_dataset[\"prediction\"] > 0) & (val_dataset[\"class_index\"] == 0) & (val_dataset[\"measDiag\"] == DiagEnum.AF)]\n",
    "for _, pt_series in false_pos_pts.iterrows():\n",
    "    # best_test_record = test_dataset[test_dataset[\"ptID\"] == pt_series[\"ptID\"]][\"prediction\"].idxmin()\n",
    "    pt_examples = one_pt_dataset # pd.concat([train_dataset[train_dataset[\"ptID\"] == pt_series[\"ptID\"]],\n",
    "                                 # test_dataset[test_dataset[\"ptID\"] == pt_series[\"ptID\"]],\n",
    "                                 # val_dataset[val_dataset[\"ptID\"] == pt_series[\"ptID\"]]])\n",
    "    print(pt_examples[\"prediction\"])\n",
    "\n",
    "    if not pt_examples[pt_examples[\"poss_AF_tag\"] == 1].empty:\n",
    "        best_example = pt_examples[pt_examples[\"poss_AF_tag\"] == 1][\"prediction\"].idxmin()\n",
    "        print(pt_series[[\"ptDiag\", \"tag_orig_Poor_Quality\", \"poss_AF_tag\", \"measDiag\", \"prediction\", \"ptID\"]])\n",
    "        print(pt_examples.loc[best_example][[\"ptDiag\", \"tag_orig_Poor_Quality\", \"poss_AF_tag\", \"measDiag\", \"prediction\"]])\n",
    "        plot_ecg(pt_series[\"data\"], 300, n_split=3, figsize=(6, 5))\n",
    "        # plt.savefig(\"FinalReportFigs/AFECGDetectedNoisy1.png\")\n",
    "        plot_ecg(pt_examples.loc[best_example][\"data\"], 300, n_split=3, figsize=(6, 5))\n",
    "        # plt.savefig(\"FinalReportFigs/AFECGDetectedNoisy1_BestFromSamePatient.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_pt_dataset =  feas2_ecg_data[feas2_ecg_data[\"ptID\"] == 15]\n",
    "pt_15_dataset = Dataset(one_pt_dataset)\n",
    "pt_15_dataloader = DataLoader(pt_15_dataset, batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, d = precision_recall_curve(true_labels, predictions)\n",
    "\n",
    "point = np.argmin(np.abs(d - 0.5))\n",
    "p_point = p[point]\n",
    "r_point = r[point]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=r, y=p, hovertext=[f\"decision boundary: {x:.2f}\" for x in d]))\n",
    "\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_final, r_final, d_final = p, r, d\n",
    "d_final = np.append(d, -1000)\n",
    "\n",
    "prec_recall_curve = np.vstack([p_final, r_final, d_final])\n",
    "np.save(\"TrainedModels/prec_recall_curve_TMR_cnn\", prec_recall_curve)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load both precision recall curve data and plot together\n",
    "prec_recall_curve_final = np.load(\"TrainedModels/prec_recall_curve_final_cnn.npy\")\n",
    "prec_recall_curve_tmr = np.load(\"TrainedModels/prec_recall_curve_TMR_cnn.npy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A plot for the report of both curves together!\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=300)\n",
    "# plt.xlim((0, 1.1))\n",
    "# plt.ylim((0, 1.1))\n",
    "\n",
    "label_every = 20\n",
    "\n",
    "tmr_rp = prec_recall_curve_tmr[1::-1, :].T\n",
    "final_rp = prec_recall_curve_final[1::-1, :].T\n",
    "\n",
    "ax.plot(prec_recall_curve_tmr[1], prec_recall_curve_tmr[0], label=\"Initial CNN\")\n",
    "ax.plot(prec_recall_curve_final[1], prec_recall_curve_final[0], label=\"Final CNN\")\n",
    "\n",
    "\"\"\"\n",
    "for point in prec_recall_curve_tmr[:, ::30].T:\n",
    "    print(point)\n",
    "    p, r, dec = point[0], point[1], point[2]\n",
    "    ax.annotate(f\"{sigmoid(dec):.2f}\", xy=(r+0.05, p+0.05), textcoords='offset points', )\n",
    "\n",
    "for point in prec_recall_curve_final[:, ::30].T:\n",
    "    print(point)\n",
    "    p, r, dec = point[0], point[1], point[2]\n",
    "    ax.annotate(f\"{sigmoid(dec):.2f}\", xy=(r+0.05, p+0.05), textcoords='offset points')\n",
    "\"\"\"\n",
    "\n",
    "closest_point_to_0_tmr = np.argmin(np.abs(prec_recall_curve_tmr[2, :]))\n",
    "ax.plot(prec_recall_curve_tmr[1, closest_point_to_0_tmr], prec_recall_curve_tmr[0, closest_point_to_0_tmr], \"o\", color=\"#1f77b4\", label=r\"Initial CNN $\\hat{y} = 0.5$\")\n",
    "# ax.annotate(f\"Zero\", xy=(prec_recall_curve_tmr[1, closest_point_to_0_tmr], prec_recall_curve_tmr[0, closest_point_to_0_tmr]), textcoords='offset points')\n",
    "\n",
    "closest_point_to_0_final = np.argmin(np.abs(prec_recall_curve_final[2, :]))\n",
    "ax.plot(prec_recall_curve_final[1, closest_point_to_0_final], prec_recall_curve_final[0, closest_point_to_0_final], \"o\", color=\"#ff7f0e\", label=r\"Final CNN $\\hat{y} = 0.5$\")\n",
    "# ax.annotate(f\"Zero\", xy=(prec_recall_curve_final[1, closest_point_to_0_final], prec_recall_curve_final[0, closest_point_to_0_final]), textcoords='offset points')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "# fig.savefig(\"FinalReportFigs/CNN_NoiseDetect_precision_recall.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prec_recall_curve_tmr[:, ::5].T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [10,20,30], [100,200,300]])\n",
    "print(a[1::-1, :])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plot for the report\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=300)\n",
    "ax.plot(r, p)\n",
    "# plt.xlim((0, 1.1))\n",
    "# plt.ylim((0, 1.1))\n",
    "\n",
    "label_every = 20\n",
    "\n",
    "label_rp = [(rec, prec) for rec, prec in zip(r[::label_every], p[::label_every])]\n",
    "label_d = [dec for dec in d[::label_every]]\n",
    "\n",
    "for rp, dec in zip(label_rp, label_d):\n",
    "    ax.annotate(f\"{dec:.2f}\", xy=rp, textcoords='offset points')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "\n",
    "plt.show()\n",
    "# fig.savefig(\"FinalReportFigs/CNN_NoiseDetect_precision_recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Utilities.Plotting\n",
    "import importlib\n",
    "importlib.reload(Utilities.Plotting)\n",
    "from Utilities.Plotting import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset = val_dataset[(val_dataset[\"prediction\"] > 0) & (val_dataset[\"class_index\"] == 0)]\n",
    "\n",
    "\"\"\"\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "def plot_ecg(x, fs=500, title=None):\n",
    "    sample_len = x.shape[0]\n",
    "    time_axis = np.arange(sample_len)/fs\n",
    "\n",
    "    y_step = 2\n",
    "\n",
    "    cuts = [0, sample_len//3, (sample_len*2)//3, sample_len-1]\n",
    "\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(8, 6))\n",
    "    for j in range(3):\n",
    "        ax[j].plot(time_axis[cuts[j]:cuts[j+1]], x[cuts[j]:cuts[j+1]])\n",
    "        ax[j].set_xlabel(\"Time\")\n",
    "        ax[j].set_xlim((time_axis[cuts[j]], time_axis[cuts[j+1]]))\n",
    "\n",
    "        t_s = time_axis[cuts[j]]\n",
    "        t_f = time_axis[cuts[j+1]]\n",
    "        time_ticks = np.arange(t_s - t_s%0.2, t_f + (0.2 - t_f%0.2), 0.2)\n",
    "        decimal_labels = ~np.isclose(time_ticks, np.round(time_ticks))\n",
    "        time_labels = np.round(time_ticks).astype(int).astype(str)\n",
    "        time_labels[decimal_labels] = \"\"\n",
    "\n",
    "        ax[j].set_xticks(time_ticks, labels=time_labels)\n",
    "        ax[j].set_yticks(np.arange(x.min()-y_step, x.max()+y_step, y_step))\n",
    "\n",
    "        # ax[j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        # ax[j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "        ax[j].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax[j].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "        ax[j].set_ylim((x.min()-y_step, x.max()+y_step))\n",
    "        ax[j].set_xlim((t_s, t_f))\n",
    "\n",
    "        ax[j].grid(which='major', linestyle='-', linewidth='0.2', color='black')\n",
    "        ax[j].grid(which='minor', linestyle='-', linewidth='0.2', color='lightgray')\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    # plt.savefig(\"test_ecg_plot.png\", dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "c = DiagEnum.CannotExcludePathology\n",
    "\"\"\"\n",
    "\n",
    "for _, ecg in plot_dataset[plot_dataset[\"measDiag\"].map(lambda x: x.value) == 3].sample(frac=1).iterrows():\n",
    "    print(ecg[[\"ptDiag\", \"tag_orig_Poor_Quality\", \"poss_AF_tag\", \"measDiag\", \"prediction\"]])\n",
    "    plot_ecg(ecg[\"data\"], 300, n_split=3, figsize=(12, 9), export_quality=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.array([1.2999, 2.39132, 0.695852])\n",
    "sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecg_section_examples(xs, ranges, titles, fs=300):\n",
    "    fig, ax = plt.subplots(len(xs), 1, figsize=(6, 4))\n",
    "\n",
    "    for j, (x, r, t) in enumerate(zip(xs, ranges, titles)):\n",
    "        sample_len = r[1] - r[0]\n",
    "        time_axis = np.arange(sample_len)/fs\n",
    "\n",
    "        ax[j].plot(time_axis, x[r[0]:r[1]])\n",
    "        ax[j].set_xlabel(\"Time\")\n",
    "        ax[j].set_xlim((time_axis[0], time_axis[-1]))\n",
    "\n",
    "        t_s = time_axis[0]\n",
    "        t_f = time_axis[-1]\n",
    "        time_ticks = np.arange(t_s - t_s%0.2, t_f + (0.2 - t_f%0.2), 0.2)\n",
    "        decimal_labels = ~np.isclose(time_ticks, np.round(time_ticks))\n",
    "        time_labels = np.round(time_ticks).astype(int).astype(str)\n",
    "        time_labels[decimal_labels] = \"\"\n",
    "        ax[j].set_xticks(time_ticks, time_labels)\n",
    "\n",
    "        ax[j].set_title(t)\n",
    "\n",
    "        ax[j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "        ax[j].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax[j].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "        ax[j].grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "        ax[j].grid(which='minor', linestyle='-', linewidth='0.5', color='lightgray')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(\"TMRFigures/cnn_false_positive_examples.png\")\n",
    "\n",
    "ecg_ind_list = [1248, 12804]\n",
    "ranges = [(0, 3010), (3000, 6010)]\n",
    "\n",
    "xs = plot_dataset.loc[ecg_ind_list][\"data\"].tolist()\n",
    "\n",
    "titles = plot_dataset.loc[ecg_ind_list].apply(lambda x: f\"{x['measDiag'].name}, p(noisy) = {sigmoid(x['prediction']):.3f}\", axis=1)   # [\"measDiag\"].map(lambda x: x.name).tolist()\n",
    "print(len(titles))\n",
    "\n",
    "plot_ecg_section_examples(xs, ranges, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare probabilities of noise\n",
    "\n",
    "low = -0.2\n",
    "high = 0.0\n",
    "noise_levels = [0.2, 0.35, 0.5, 0.65, 0.8]\n",
    "\n",
    "val_dataset[\"noise_prob\"] = val_dataset[\"prediction\"].map(sigmoid)\n",
    "plot_dataset = [val_dataset[(val_dataset[\"noise_prob\"] > low) & (val_dataset[\"noise_prob\"] < high)] for low, high in zip(noise_levels[:-1], noise_levels[1:])]\n",
    "\n",
    "fig, ax = plt.subplots(len(noise_levels) -1, 3, figsize=(12, 7))\n",
    "\n",
    "fs = 300\n",
    "\n",
    "for j in range(len(noise_levels)-1):\n",
    "    dataset = plot_dataset[j]\n",
    "    k = 0\n",
    "    for _, ecg in dataset.iterrows():\n",
    "\n",
    "        print(ecg[[\"ptDiag\", \"tag_orig_Poor_Quality\", \"poss_AF_tag\", \"measDiag\", \"prediction\"]])\n",
    "        x = ecg[\"data\"][3000:6000]\n",
    "\n",
    "        sample_len = x.shape[0]\n",
    "        time_axis = np.arange(sample_len)/fs\n",
    "\n",
    "        ax[j][k].plot(time_axis, x)\n",
    "\n",
    "        if j == len(noise_levels)-2:\n",
    "            ax[j][k].set_xlabel(\"Time\")\n",
    "        ax[j][k].set_xlim((time_axis[0], time_axis[-1]))\n",
    "\n",
    "        t_s = time_axis[0]\n",
    "        t_f = time_axis[-1]\n",
    "        time_ticks = np.arange(t_s - t_s%0.2, t_f + (0.2 - t_f%0.2), 0.2)\n",
    "        decimal_labels = ~np.isclose(time_ticks, np.round(time_ticks))\n",
    "        time_labels = np.round(time_ticks).astype(int).astype(str)\n",
    "        time_labels[decimal_labels] = \"\"\n",
    "\n",
    "        ax[j][k].set_xticks(time_ticks, time_labels)\n",
    "        ax[j][k].set_yticklabels([])\n",
    "\n",
    "        ax[j][k].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax[j][k].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "        ax[j][k].grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "        ax[j][k].grid(which='minor', linestyle='-', linewidth='0.5', color='lightgray')\n",
    "\n",
    "        if k == 0:\n",
    "            ax[j][k].set_ylabel(r\"$[{:.2f}, {:.2f})$\".format(noise_levels[j], noise_levels[j+1]))\n",
    "\n",
    "        k += 1\n",
    "        if k >= 3:\n",
    "            break\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "conf_1 = np.array([[96, 19], [5, 69]])\n",
    "conf_2 = np.array([[97, 18], [3, 71]])\n",
    "conf_3 = np.array([[83, 32], [1, 73]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_3, display_labels=[\"Sufficient Quality\", \"Poor Quality\"]).plot(cmap=\"GnBu\", ax=ax, colorbar=False)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt_df[\"noHQrecsNotUndecided\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"class_index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation dataset construction for SAFER data\n",
    "# Split train and test data according to each patient\n",
    "feas2_pt_data[\"noLQrecs\"] = feas2_pt_data[\"noRecs\"] - feas2_pt_data[\"noHQrecs\"]\n",
    "\n",
    "num_cv = 4\n",
    "num_folds = 2 * num_cv  # twice to produce the val and test for each fold!\n",
    "pt_folds = [[] for _ in range(num_folds)]\n",
    "\n",
    "lq_counts = np.zeros(num_folds, dtype=int)\n",
    "total_counts = np.zeros(num_folds, dtype=int)\n",
    "\n",
    "total_total_count = 0\n",
    "total_lq_count = 0\n",
    "\n",
    "# Go around the folds and assign patients to each\n",
    "for _, pt in feas2_pt_data.iterrows():\n",
    "    total_total_count += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "    total_lq_count += pt[\"noLQrecs\"]\n",
    "\n",
    "    exp_total_counts = total_total_count * 1.0/num_folds\n",
    "    exp_lq_counts = total_lq_count * 1.0/num_folds\n",
    "\n",
    "    lq_rec_mat = np.diag(np.array([pt[\"noLQrecs\"] for _ in range(num_folds)]))\n",
    "    hq_rec_mat = np.diag(np.array([pt[\"noHQrecsNotUndecided\"] for _ in range(num_folds)]))\n",
    "\n",
    "    loss =  np.sum(np.abs(lq_counts[None, :] + lq_rec_mat - exp_lq_counts) + np.abs(total_counts[None, :] + hq_rec_mat - exp_total_counts), axis=-1)\n",
    "    best_fold = np.argmin(loss)\n",
    "\n",
    "    pt_folds[best_fold].append(pt)\n",
    "    lq_counts[best_fold] += pt[\"noLQrecs\"]\n",
    "    total_counts[best_fold] += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "\n",
    "\n",
    "test_pt_folds = [pd.DataFrame(fold) for fold in pt_folds[:4]]\n",
    "val_pt_folds = [pd.DataFrame(fold) for fold in pt_folds[4:]]\n",
    "train_pt_folds = [feas2_pt_data[(~feas2_pt_data[\"ptID\"].isin(test_fold[\"ptID\"])) & (~feas2_pt_data[\"ptID\"].isin(val_fold[\"ptID\"]))] for test_fold, val_fold in zip(test_pt_folds, val_pt_folds)]\n",
    "\n",
    "for f in test_pt_folds:\n",
    "    print(f[\"noLQrecs\"].sum(), f[\"noHQrecsNotUndecided\"].sum())\n",
    "\n",
    "for f in val_pt_folds:\n",
    "    print(f[\"noLQrecs\"].sum(), f[\"noHQrecsNotUndecided\"].sum())\n",
    "\n",
    "conf_mats = []\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for i, (train_pt_df, test_pt_df, val_pt_df) in enumerate(zip(train_pt_folds, test_pt_folds, val_pt_folds)):\n",
    "    print(f\"Fold {i}\")\n",
    "    train_df = feas2_ecg_data[feas2_ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])]\n",
    "    test_df = feas2_ecg_data[(feas2_ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])) & (feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "    val_df = feas2_ecg_data[(feas2_ecg_data[\"ptID\"].isin(val_pt_df[\"ptID\"])) & (feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "\n",
    "\n",
    "    class_counts = torch.tensor(train_df[\"class_index\"].value_counts().values.astype(np.float32))\n",
    "    a = class_counts[0]/(class_counts[0] + class_counts[1])\n",
    "\n",
    "    a_test_counts = torch.tensor(feas2_ecg_data[feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided][\"class_index\"].value_counts().values.astype(np.float32))\n",
    "    a_test = a_test_counts[0]/(a_test_counts[0] + a_test_counts[1])\n",
    "\n",
    "    print(train_df[\"class_index\"].value_counts())\n",
    "    print(test_df[\"class_index\"].value_counts())\n",
    "    print(val_df[\"class_index\"].value_counts())\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    torch_dataset_train = Dataset(train_df)\n",
    "    torch_dataset_test = Dataset(test_df)\n",
    "    torch_dataset_val = Dataset(val_df)\n",
    "\n",
    "    train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "    test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "    val_dataloader = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "    break\n",
    "\n",
    "    model = CNN(**hyperparameters).to(device)\n",
    "\n",
    "    weight_modifier = 1\n",
    "    loss_func = binary_focal_loss((1 - (1-a) * weight_modifier), 0)\n",
    "    test_loss_func = binary_focal_loss(a_test, 0)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    scheduler = StepLR(optimizer, 6, 1)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "    num_test_batches = len(test_dataloader)\n",
    "\n",
    "    model, losses = train(model, train_dataloader, test_dataloader, loss_func, test_loss_func, optimizer, scheduler)\n",
    "    model = model.to(device)\n",
    "\n",
    "    predictions, true_labels = get_predictions(model, val_dataloader, val_df)\n",
    "    conf_mat = confusion_matrix(true_labels, predictions > 0)\n",
    "\n",
    "    print(conf_mat)\n",
    "\n",
    "    conf_mats.append(conf_mat)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_normal = [F1_ind(c, 0) for c in conf_mats]\n",
    "f1_scores_noisy = [F1_ind(c, 1) for c in conf_mats]\n",
    "\n",
    "print(f\"Mean F1 normal: {np.mean(f1_scores_normal)}\")\n",
    "print(f\"Mean F1 noisy: {np.mean(f1_scores_noisy)}\")\n",
    "print(f\"Individual F1 scores (noisy): {f1_scores_noisy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in conf_mats:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the noise stress test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stress test noise data\n",
    "import wfdb\n",
    "import os\n",
    "import scipy.signal\n",
    "\n",
    "noise_stress_test_db = \"mit-bih-noise-stress-test-database\"\n",
    "stress_test_files = [\"118e24\", \"119e24\", \"118e06\", \"118e00\", \"118e_6\", \"119e06\", \"119e00\", \"119e_6\"]\n",
    "\n",
    "labels = []\n",
    "noise_level = []\n",
    "samples = []\n",
    "\n",
    "# Additionally band pass filter\n",
    "def filter_ecg(x, fs):\n",
    "    b, a = scipy.signal.butter(3, [0.66, 50], 'band', fs=fs)\n",
    "    x = scipy.signal.filtfilt(b, a, x, padlen=150)\n",
    "    x = (x - min(x)) / (max(x) - min(x))\n",
    "    return x\n",
    "\n",
    "for file in stress_test_files:\n",
    "    try:\n",
    "        print(f\"Reading file: {file}\")\n",
    "        data = wfdb.io.rdrecord(os.path.join(noise_stress_test_db, file))\n",
    "        all_data_v1 = data.p_signal[:,1]\n",
    "        # Resample to 300Hz\n",
    "        all_data_v1 = scipy.signal.resample(all_data_v1, int(all_data_v1.shape[0] * 300/data.fs))\n",
    "        # all_data_v1 = filter_ecg(all_data_v1, data.fs)\n",
    "        # all_data_v1 = adaptive_gain_norm(all_data_v1, 501)\n",
    "\n",
    "        sec_len = 300 * 30  # 30s segments\n",
    "        i = 1\n",
    "        while i * sec_len < all_data_v1.shape[0]:\n",
    "            s = all_data_v1[(i-1)*sec_len:i*sec_len]\n",
    "            samples.append(s)\n",
    "            noise_level.append(file.split(\"e\")[-1])\n",
    "\n",
    "            if i * 30 < 300:\n",
    "                labels.append(\"N\")\n",
    "            elif (i * 30 - 300) % 240 > 120 or (i * 30 - 300) % 240 == 0:\n",
    "                labels.append(\"N\")\n",
    "            elif (i * 30 - 300) % 240 <= 120:\n",
    "                labels.append(\"~\")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"error, scipping file\")\n",
    "        continue\n",
    "\n",
    "\n",
    "nst_df = pd.DataFrame({\"data\": samples, \"class\": labels, \"noise_level\": noise_level})\n",
    "pk_path = \"mit-bih-noise-stress-test-database/database.pk\"\n",
    "nst_df.to_pickle(pk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nst_df[\"class_index\"] = (nst_df[\"class\"] == \"~\").astype(int)\n",
    "\n",
    "class NSTDataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"data\"]  # The only dataset and nst dataset difference is in this line!\n",
    "        y = row[\"class_index\"]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Normalise the data\n",
    "nst_df[\"data\"] = (nst_df[\"data\"] - nst_df[\"data\"].map(lambda x: x.mean()))/nst_df[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "torch_dataset_nst = NSTDataset(nst_df)\n",
    "nst_dataloader = DataLoader(torch_dataset_nst, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels) in enumerate(nst_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        labels = labels.detach().numpy()\n",
    "        true_labels.append(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(signals).detach().to(\"cpu\").numpy()\n",
    "\n",
    "        prediction = np.argmax(output, axis=-1)\n",
    "        false_positive = np.logical_and(labels == 0, prediction == 1)\n",
    "        false_positives.append(signals[false_positive, 0, :].cpu().detach().numpy())\n",
    "\n",
    "        true_negative = np.logical_and(labels == 1, prediction == 0)\n",
    "        true_negatives.append(signals[true_negative, 0, :].cpu().detach().numpy())\n",
    "\n",
    "        predictions.append(np.argmax(output, axis=-1))\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "true_labels = np.concatenate(true_labels)\n",
    "false_positives = np.concatenate(false_positives, axis=0)\n",
    "true_negatives = np.concatenate(true_negatives, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(true_labels, predictions, display_labels=[\"sufficint quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "\n",
    "conf_mat = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Other F1: {F1_ind(conf_mat, 1)}\")\n",
    "# print(f\"AF F1: {F1_ind(conf_mat, 2)}\")\n",
    "# print(f\"Noisy F1: {F1_ind(conf_mat, 3)}\")\n",
    "\n",
    "print(f\"Average F1 score: {sum([F1_ind(conf_mat, i) for i in range(2)])/2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 40\n",
    "print(false_positives.shape)\n",
    "\n",
    "fig = go.Figure(go.Scatter(y=false_positives[index]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 15\n",
    "print(true_negatives.shape)\n",
    "\n",
    "fig = go.Figure(go.Scatter(y=true_negatives[index]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        self.model, _ = train(self.model, train_dataloader, test_dataloader, self.loss_func, self.loss_func, self.optimizer, self.scheduler)\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        model.eval()\n",
    "\n",
    "        true_labels = []\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (signals, labels, ind) in enumerate(test_dataloader):\n",
    "                signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "                labels = labels.detach().numpy()\n",
    "                true_labels.append(labels)\n",
    "\n",
    "                output = self.model(signals).detach().to(\"cpu\").numpy()\n",
    "\n",
    "                prediction = output # np.argmax(output, axis=-1)\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        true_labels = np.concatenate(true_labels)\n",
    "\n",
    "        conf_mat = confusion_matrix(true_labels, predictions > 0)\n",
    "        return - F1_ind(conf_mat, 1)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        print(\"setting parameters\")\n",
    "        self.model = CNN(**hyperparameters).to(device)\n",
    "        self.loss_func = binary_focal_loss(a, kwargs[\"gamma\"])\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=kwargs[\"lr\"])\n",
    "        self.scheduler = StepLR(optimizer, kwargs[\"sched_step\"], kwargs[\"sched_gamma\"])\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "\"\"\"\n",
    "priors = {\"lr\": Real(1e-6, 1e-2, prior=\"uniform\", name=\"lr\"),\n",
    "          \"gamma\": Real(0, 5, prior=\"uniform\", name=\"gamma\"),\n",
    "          \"sched_gamma\": Real(0.1, 1, prior=\"uniform\", name=\"sched_gamma\"),\n",
    "          \"sched_step\": Integer(5, 25, prior=\"uniform\", name=\"sched_step\"),\n",
    "\n",
    "          \"k1\": Integer(17, 17, \"uniform\", name=\"k1\"),\n",
    "          \"k2\": Integer(11, 11, \"uniform\", name=\"k2\"),\n",
    "          \"k3\": Integer( 5, 5, \"uniform\", name=\"k3\"),\n",
    "          \"k4\": Integer( 3, 3, \"uniform\", name=\"k4\"),\n",
    "          \"k5\": Integer( 3, 3, \"uniform\", name=\"k5\"),\n",
    "          \"k6\": Integer( 3, 3, \"uniform\", name=\"k6\"),\n",
    "\n",
    "          \"c1\": Integer(128, 128, \"uniform\", name=\"c1\"),\n",
    "          \"c2\": Integer( 256, 256, \"uniform\", name=\"c2\"),\n",
    "          \"c3\": Integer( 256, 256, \"uniform\", name=\"c3\"),\n",
    "          \"c4\": Integer( 128, 128, \"uniform\", name=\"c4\"),\n",
    "          \"c5\": Integer( 64, 64, \"uniform\", name=\"c5\"),\n",
    "          \"c6\": Integer( 32, 32, \"uniform\", name=\"c6\"),\n",
    "\n",
    "          \"lstm_n_hidden\": Integer(32, 32, \"uniform\", name=\"lstm_n_hidden\"),\n",
    "\n",
    "          \"dense1\": Integer(256, 256, \"uniform\", name=\"dense1\"),\n",
    "          \"dense2\": Integer(16, 16, \"uniform\", name=\"dense2\")}\n",
    "\"\"\"\n",
    "\n",
    "priors = {\"lr\": Real(1e-6, 1e-2, prior=\"uniform\", name=\"lr\"),\n",
    "          \"gamma\": Real(0, 5, prior=\"uniform\", name=\"gamma\"),\n",
    "          \"sched_gamma\": Real(0.1, 1, prior=\"uniform\", name=\"sched_gamma\"),\n",
    "          \"sched_step\": Integer(5, 25, prior=\"uniform\", name=\"sched_step\")}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    ModelWrapper(**hyperparameters),\n",
    "    priors,\n",
    "    n_iter=5,\n",
    "    cv=2\n",
    ")\n",
    "\n",
    "pointless = np.array([1,2,3,4,5])\n",
    "\n",
    "# opt.fit(pointless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(opt.best_params_)\n",
    "model = opt.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "dimensions = list(priors.values())\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def train_and_score(**kwargs):\n",
    "    print(f\"Testing with {kwargs}\")\n",
    "    m = ModelWrapper(**kwargs)\n",
    "    m.fit()\n",
    "    score = m.score()\n",
    "    print(type(score))\n",
    "    print(f\"Model score: {score}\")\n",
    "    return float(score)\n",
    "\n",
    "res = gp_minimize(train_and_score, dimensions, acq_func=\"EI\", n_calls=20, n_random_starts=5, noise=0.1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in zip(priors.keys(), res.x):\n",
    "    print(f\"{name}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def train_and_get_final_model(**kwargs):\n",
    "    print(f\"Testing with {kwargs}\")\n",
    "    m = ModelWrapper(**kwargs)\n",
    "    m.fit()\n",
    "    return m.model\n",
    "\n",
    "model = train_and_get_final_model(res.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
