{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.io\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import DataHandlers.SAFERDataset as SAFERDataset\n",
    "import DataHandlers.CinCDataset as CinCDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CinC training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = CinCDataset.load_cinc_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the SAFER dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pt_dataset, dataset = SAFERDataset.load_feas_dataset(feas=2, force_reload=False, process=True, force_reprocess=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute spectrograms\n",
    "ecg_tensor = torch.tensor(dataset[\"data\"])\n",
    "fs = 300\n",
    "n_fft = 256\n",
    "\n",
    "ecg_stfts = torch.stft(ecg_tensor, n_fft=n_fft, return_complex=True)\n",
    "print(ecg_stfts.shape)\n",
    "freq_axis = np.linspace(0, fs/2, int(round(n_fft/2)))\n",
    "low_cut = np.argmin(np.abs(freq_axis - 2))\n",
    "high_cut = np.argmin(np.abs(freq_axis - 50))\n",
    "\n",
    "print(low_cut, high_cut)\n",
    "\n",
    "dataset[\"stft\"] = pd.Series([torch.log(torch.abs(stft[low_cut:high_cut])).cpu().numpy() for stft in ecg_stfts], index=dataset.index)\n",
    "\n",
    "freq_axis = np.linspace(0, fs/2, int(round(n_fft/2)))[low_cut:high_cut]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalise spectrograms\n",
    "\n",
    "dataset[\"stft\"] = dataset[\"stft\"].map(lambda x: (x - x.mean()) / x.std())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete variables to save ram\n",
    "del ecg_stfts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete old stfts before recomputing\n",
    "dataset = dataset.drop(\"stft\", axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind = 10\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z=dataset[\"stft\"][0], y=freq_axis))\n",
    "fig.show()\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(y=ecg_tensor[ind].cpu().numpy()))\n",
    "fig2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset[\"stft\"][0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now define a model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_section1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.conv_section2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.conv_section3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.conv_section4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.conv_section5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.lstm_n_hidden = 32\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=32, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # self.dense1 = nn.Linear(352, 128)\n",
    "        self.dense2 = nn.Linear(896, 256)\n",
    "        self.dense3 = nn.Linear(256, 1)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def init_lstm_hidden(self, batch_size, device):\n",
    "        # This resets the LSTM hidden state after each batch\n",
    "        hidden_state = torch.zeros(2, batch_size, self.lstm_n_hidden, device=device)\n",
    "        cell_state = torch.zeros(2, batch_size, self.lstm_n_hidden, device=device)\n",
    "        return (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # [batch, 1, 40, 141]\n",
    "        x = self.conv_section1(x)\n",
    "\n",
    "        # [batch, 32, 38, 139]\n",
    "        x = self.conv_section2(x)\n",
    "\n",
    "        # [batch, 64, 18, 68]\n",
    "        x = self.conv_section3(x)\n",
    "\n",
    "        # [batch, 128, 7, 32]\n",
    "        x = self.conv_section4(x)\n",
    "\n",
    "        # [batch, 64, 5, 30]\n",
    "        x = self.conv_section5(x)\n",
    "\n",
    "        # [batch, 32, 1, 14]\n",
    "        x = x[:, :, 0, :]\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        x, _ = self.lstm(x, self.init_lstm_hidden(x.shape[0], x.device))\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        # [batch, 896]\n",
    "        x = self.dense2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # [batch, 256]\n",
    "        x = self.dense3(x)\n",
    "        # x = self.logsoftmax(x)\n",
    "\n",
    "        # [4]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Onehot encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def generate_onehot(c):\n",
    "    if c == \"N\":\n",
    "        return np.array([1, 0, 0, 0])\n",
    "    if c == \"O\":\n",
    "        return np.array([0, 1, 0, 0])\n",
    "    if c == \"A\":\n",
    "        return np.array([0, 0, 1, 0])\n",
    "    if c == \"~\":\n",
    "        return np.array([0, 0, 0, 1])\n",
    "\n",
    "def generate_index(c):\n",
    "    if c == \"N\":\n",
    "        return 0\n",
    "    if c == \"O\":\n",
    "        return 0\n",
    "    if c == \"A\":\n",
    "        return 0\n",
    "    if c == \"~\":\n",
    "        return 1\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"stft\"]\n",
    "        y = row[\"class_index\"]\n",
    "\n",
    "        return X, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset[\"onehot\"] = dataset[\"class\"].map(generate_onehot)\n",
    "dataset[\"class_index\"] = dataset[\"class\"].map(generate_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For CinC Data\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.15, stratify=dataset[\"class_index\"])\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "torch_dataset_test = Dataset(test_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "num_batches = len(train_dataloader)\n",
    "num_test_batches = len(test_dataloader)\n",
    "class_counts = torch.tensor(dataset[\"class_index\"].value_counts().values.astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reset to SAFER balanced data\n",
    "dataset = feas2_ecg_data_unbalanced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# print(feas2_ecg_data.head())\n",
    "\n",
    "# dataset[\"onehot\"] = dataset[\"class\"].map(generate_onehot)\n",
    "# Balance the data (for testing only)\n",
    "\n",
    "\"\"\"\n",
    "feas2_ecg_data_unbalanced = feas2_ecg_data.copy()\n",
    "feas2_ecg_data_non_noisy = feas2_ecg_data[feas2_ecg_data[\"measDiag\"] != DiagEnum.PoorQuality].sample(1000)\n",
    "feas2_ecg_data = pd.concat([feas2_ecg_data[feas2_ecg_data[\"measDiag\"] == DiagEnum.PoorQuality], feas2_ecg_data_non_noisy])\n",
    "\"\"\"\n",
    "train_patients, test_patients = train_test_split(pt_dataset, test_size=0.15)\n",
    "\n",
    "train_ecgs = dataset[dataset[\"ptID\"].isin(train_patients['ptID'])]\n",
    "test_ecgs = dataset[dataset[\"ptID\"].isin(test_patients['ptID'])]\n",
    "\n",
    "print(train_ecgs[\"class_index\"].value_counts())\n",
    "print(test_ecgs[\"class_index\"].value_counts())\n",
    "\n",
    "torch_dataset_train = Dataset(train_ecgs)\n",
    "torch_dataset_test = Dataset(test_ecgs)\n",
    "\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "num_batches = len(train_dataloader)\n",
    "num_test_batches = len(test_dataloader)\n",
    "class_counts = torch.tensor(train_ecgs[\"class_index\"].value_counts().values.astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Use weightings to handle class imbalance\n",
    "\n",
    "class_weights = (class_counts[1] + class_counts[0])/class_counts[1]\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model = model.to(device)\n",
    "\n",
    "def train(model):\n",
    "    best_test_loss = 100\n",
    "    best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"starting epoch {epoch} ...\")\n",
    "        # Train\n",
    "        model.train()\n",
    "        for i, (signals, labels) in enumerate(train_dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "            # fft = torch.abs(torch.fft.fft(signals))\n",
    "            # signals = torch.cat([signals, fft], dim=1)\n",
    "            labels = labels.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(signals).to(\"cpu\")[:, 0]\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "        print(\"Testing ...\")\n",
    "        # Test\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, (signals, labels) in enumerate(test_dataloader):\n",
    "                signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "                # fft = torch.abs(torch.fft.fft(signals))\n",
    "                # signals = torch.cat([signals, fft], dim=1)\n",
    "                labels = labels.float()\n",
    "                output = model(signals).to(\"cpu\")[:, 0]\n",
    "                loss = loss_func(output, labels)\n",
    "                test_loss += float(loss)\n",
    "\n",
    "        print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "\n",
    "        if test_loss/num_test_batches < best_test_loss:\n",
    "            best_model = copy.deepcopy(model).cpu()\n",
    "            best_test_loss = test_loss/num_test_batches\n",
    "\n",
    "    return best_model\n",
    "\n",
    "model = train(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(\"TrainedModels/CNN_AlexNet_LSTM_Bidirectional_2Class_norm.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    all_signals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (signals, labels) in enumerate(dataloader):\n",
    "            signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "            labels = labels.detach().numpy()\n",
    "            true_labels.append(labels)\n",
    "            all_signals.append(signals.detach().cpu().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(signals)[:, 0].detach().to(\"cpu\").numpy()\n",
    "\n",
    "            prediction = output # np.argmax(output, axis=-1)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    true_labels = np.concatenate(true_labels)\n",
    "    all_signals = np.concatenate(all_signals)\n",
    "\n",
    "    print(all_signals.shape)\n",
    "\n",
    "    return predictions, true_labels, all_signals\n",
    "\n",
    "\n",
    "predictions, true_labels, all_signals = get_predictions(model, test_dataloader)\n",
    "conf_mat = confusion_matrix(true_labels, predictions > 0)\n",
    "false_positives = all_signals[np.logical_and(true_labels == 0, predictions == 1), 0, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConfusionMatrixDisplay.from_predictions(true_labels, predictions, display_labels=[\"sufficint quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "\n",
    "# Same as the below function (as described in CinC)\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "\"\"\"\n",
    "def bin_F1_score(conf_mat, ind):\n",
    "    return conf_mat[ind, ind]/(conf_mat[ind, ind] + 0.5 * (conf_mat[0, 1] + conf_mat[1, 0]))\n",
    "\n",
    "print(f\"Normal F1: {bin_F1_score(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {bin_F1_score(conf_mat, 1)}\")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1])}\")\n",
    "print(f\"Specificity: {conf_mat[0, 0]/np.sum(conf_mat[0])}\")\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, d = precision_recall_curve(true_labels, predictions)\n",
    "\n",
    "point = np.argmin(np.abs(d - 0.5))\n",
    "p_point = p[point]\n",
    "r_point = r[point]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=r, y=p, hovertext=[f\"decision boundary: {x:.2f}\" for x in d]))\n",
    "\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "print(false_positives.shape)\n",
    "\n",
    "fig = go.Figure(go.Scatter(y=false_positives[index]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "torch.save(model.state_dict(), \"TrainedModels/CNN_AlexNet_LSTM_Bidirectional_2Class_adaptive_norm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garbage collection - in case of CUDA out of memory error\n",
    "import gc\n",
    "model = None\n",
    "signals = None\n",
    "labels = None\n",
    "gc.collect() # Python thing\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perform cross validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "conf_mats = []\n",
    "fps = []\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for i, (train_ind, test_ind) in enumerate(kf.split(dataset, dataset)):\n",
    "    print(f\"==========  s p l i t    {i} ========== \")\n",
    "    train_dataset = dataset.iloc[train_ind]\n",
    "    test_dataset = dataset.iloc[test_ind]\n",
    "\n",
    "    torch_dataset_train = Dataset(train_dataset)\n",
    "    torch_dataset_test = Dataset(test_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "    test_dataloader = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "    num_test_batches = len(test_dataloader)\n",
    "\n",
    "    model = train(model).to(device)\n",
    "\n",
    "    predictions, true_labels, all_signals = get_predictions(model, test_dataloader)\n",
    "    conf_mat = confusion_matrix(true_labels, predictions > 0)\n",
    "    false_positives = all_signals[np.logical_and(true_labels == 0, predictions == 1), 0, :]\n",
    "\n",
    "    fps.append(false_positives)\n",
    "    conf_mats.append(conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_scores_normal = [F1_ind(c, 0) for c in conf_mats]\n",
    "f1_scores_noisy = [F1_ind(c, 1) for c in conf_mats]\n",
    "\n",
    "print(f\"Mean F1 normal: {np.mean(f1_scores_normal)}\")\n",
    "print(f\"Mean F1 noisy: {np.mean(f1_scores_noisy)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the noise stress test database"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the data looks the same when you plot it, so something might be wrong with the data reading process!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stress test noise data\n",
    "import wfdb\n",
    "import os\n",
    "import scipy.signal\n",
    "\n",
    "noise_stress_test_db = \"mit-bih-noise-stress-test-database\"\n",
    "stress_test_files = [\"118e06\", \"118e00\", \"118e_6\", \"119e06\", \"119e00\", \"119e_6\"]\n",
    "\n",
    "labels = []\n",
    "noise_level = []\n",
    "samples = []\n",
    "\n",
    "# Additionally band pass filter\n",
    "def filter_ecg(x, fs):\n",
    "    b, a = scipy.signal.butter(3, [0.66, 50], 'band', fs=fs)\n",
    "    x = scipy.signal.filtfilt(b, a, x, padlen=150)\n",
    "    x = (x - min(x)) / (max(x) - min(x))\n",
    "    return x\n",
    "\n",
    "for file in stress_test_files:\n",
    "    try:\n",
    "        print(f\"Reading file: {file}\")\n",
    "        data = wfdb.io.rdrecord(os.path.join(noise_stress_test_db, file))\n",
    "        all_data_v1 = data.p_signal[:,1]\n",
    "        # Resample to 300Hz\n",
    "        all_data_v1 = scipy.signal.resample(all_data_v1, int(all_data_v1.shape[0] * 300/data.fs))\n",
    "        # all_data_v1 = filter_ecg(all_data_v1, data.fs)\n",
    "        # all_data_v1 = adaptive_gain_norm(all_data_v1, 501)\n",
    "\n",
    "        sec_len = 300 * 30  # 30s segments\n",
    "        i = 1\n",
    "        while i * sec_len < all_data_v1.shape[0]:\n",
    "            s = all_data_v1[(i-1)*sec_len:i*sec_len]\n",
    "            samples.append(s)\n",
    "            noise_level.append(file.split(\"e\")[-1])\n",
    "\n",
    "            if i * 30 < 300:\n",
    "                labels.append(\"N\")\n",
    "            elif (i * 30 - 300) % 240 > 120 or (i * 30 - 300) % 240 == 0:\n",
    "                labels.append(\"N\")\n",
    "            elif (i * 30 - 300) % 240 <= 120:\n",
    "                labels.append(\"~\")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"error, scipping file\")\n",
    "        continue\n",
    "\n",
    "\n",
    "nst_df = pd.DataFrame({\"data\": samples, \"class\": labels, \"noise_level\": noise_level})\n",
    "pk_path = \"mit-bih-noise-stress-test-database/database.pk\"\n",
    "nst_df.to_pickle(pk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nst_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"onehot\"] = dataset[\"class\"].map(generate_onehot)\n",
    "nst_df[\"class_index\"] = nst_df[\"class\"].map(generate_index)\n",
    "\n",
    "class NSTDataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"data\"]  # The only dataset and nst dataset difference is in this line!\n",
    "        y = row[\"class_index\"]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Normalise the data\n",
    "nst_df[\"data\"] = (nst_df[\"data\"] - nst_df[\"data\"].map(lambda x: x.mean()))/nst_df[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "torch_dataset_nst = NSTDataset(nst_df)\n",
    "nst_dataloader = DataLoader(torch_dataset_nst, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels) in enumerate(nst_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        labels = labels.detach().numpy()\n",
    "        true_labels.append(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(signals).detach().to(\"cpu\").numpy()\n",
    "\n",
    "        prediction = np.argmax(output, axis=-1)\n",
    "        false_positive = np.logical_and(labels == 0, prediction == 1)\n",
    "        false_positives.append(signals[false_positive, 0, :].cpu().detach().numpy())\n",
    "\n",
    "        true_negative = np.logical_and(labels == 1, prediction == 0)\n",
    "        true_negatives.append(signals[true_negative, 0, :].cpu().detach().numpy())\n",
    "\n",
    "        predictions.append(np.argmax(output, axis=-1))\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "true_labels = np.concatenate(true_labels)\n",
    "false_positives = np.concatenate(false_positives, axis=0)\n",
    "true_negatives = np.concatenate(true_negatives, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(true_labels, predictions, display_labels=[\"sufficint quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "\n",
    "conf_mat = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Other F1: {F1_ind(conf_mat, 1)}\")\n",
    "# print(f\"AF F1: {F1_ind(conf_mat, 2)}\")\n",
    "# print(f\"Noisy F1: {F1_ind(conf_mat, 3)}\")\n",
    "\n",
    "print(f\"Average F1 score: {sum([F1_ind(conf_mat, i) for i in range(2)])/2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 40\n",
    "print(false_positives.shape)\n",
    "\n",
    "fig = go.Figure(go.Scatter(y=false_positives[index]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 15\n",
    "print(true_negatives.shape)\n",
    "\n",
    "fig = go.Figure(go.Scatter(y=true_negatives[index]))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
