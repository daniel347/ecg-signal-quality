{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import ecg_noise_detector.src.ecg_noise_detector.noiseDetector as nd\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from DiagEnum import DiagEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use this if changing noiseDetector.py\n",
    "import importlib\n",
    "importlib.reload(nd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"CinC2017Data/database.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset[\"length\"] = dataset[\"data\"].map(lambda arr: arr.shape[-1])\n",
    "# select only the 30s length records\n",
    "dataset = dataset[dataset[\"length\"] == 9000]\n",
    "dataset[\"data\"] = dataset[\"data\"].map(lambda d: d[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"onehot\"] = dataset[\"class\"].map(generate_onehot)\n",
    "def generate_index(c):\n",
    "    if c == \"N\":\n",
    "        return 0\n",
    "    if c == \"O\":\n",
    "        return 0\n",
    "    if c == \"A\":\n",
    "        return 0\n",
    "    if c == \"~\":\n",
    "        return 1\n",
    "\n",
    "dataset[\"class_index\"] = dataset[\"class\"].map(generate_index)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.15, stratify=dataset[\"class_index\"])\n",
    "\n",
    "print(len(test_dataset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the SAFER data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from DiagEnum import DiagEnum\n",
    "import SAFERDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_pt_data, feas2_ecg_data = SAFERDataset.load_feas_dataset(2, \"dataframe\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_pt_data.index = feas2_pt_data[\"ptID\"]\n",
    "feas2_pt_data[\"noRecs\"] = feas2_ecg_data[\"ptID\"].value_counts()\n",
    "feas2_pt_data[\"noHQrecs\"] = feas2_ecg_data[feas2_ecg_data[\"class_index\"] == 0][\"ptID\"].value_counts()\n",
    "\n",
    "feas2_pt_data[\"noHQrecsNotUndecided\"] = feas2_ecg_data[(feas2_ecg_data[\"class_index\"] == 0) & (feas2_ecg_data[\"measDiag\"] != DiagEnum.Undecided)][\"ptID\"].value_counts()\n",
    "feas2_pt_data[[\"noRecs\", \"noHQrecs\", \"noHQrecsNotUndecided\"]] = feas2_pt_data[[\"noRecs\", \"noHQrecs\", \"noHQrecsNotUndecided\"]].fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_ecg_data[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(feas2_pt_data[\"noRecs\"] - feas2_pt_data[\"noHQrecs\"]).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, only_clean_training=True):\n",
    "    pt_data[\"noLQrecs\"] = pt_data[\"noRecs\"] - pt_data[\"noHQrecs\"]  # for Feas1 this might include stuff flagged by zenicor as noisy?\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "\n",
    "    for val, df in pt_data.groupby(\"noLQrecs\"):\n",
    "        print(f\"processing {val}\")\n",
    "        print(f\"number of patients {len(df.index)}\")\n",
    "        test = df.sample(frac=test_frac)\n",
    "        test_patients.append(test)\n",
    "        train_patients.append(df[~df[\"ptID\"].isin(test[\"ptID\"])])\n",
    "\n",
    "    train_pt_df = pd.concat(train_patients)\n",
    "    test_pt_df = pd.concat(test_patients)\n",
    "\n",
    "    print(f\"Test high quality: {test_pt_df['noHQrecs'].sum()} low quality: {test_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Train high quality: {train_pt_df['noHQrecs'].sum()} low quality: {train_pt_df['noLQrecs'].sum()} \")\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        # get ECG datasets\n",
    "        train_dataset = ecg_data[ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])]\n",
    "        # Normalise\n",
    "        train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = ecg_data[(ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "        test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_ecg_df, test_ecg_df = make_SAFER_dataloaders(feas2_pt_data, feas2_ecg_data, test_frac=0.2, only_clean_training=False, )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, val_frac, only_clean_training=True):\n",
    "    pt_data[\"noLQrecs\"] = pt_data[\"noRecs\"] - pt_data[\n",
    "        \"noHQrecs\"]  # for Feas1 this might include stuff flagged by zenicor as noisy?\n",
    "\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "    val_patients = []\n",
    "\n",
    "    test_val_frac = test_frac + val_frac\n",
    "    val_second_frac = val_frac / test_val_frac\n",
    "\n",
    "    lq_counts = np.array([0, 0, 0], dtype=int)\n",
    "    total_counts = np.array([0, 0, 0], dtype=int)\n",
    "\n",
    "    fracs = np.array([1 - test_frac - val_frac, test_frac, val_frac])\n",
    "\n",
    "    total_lq_count = 0\n",
    "    total_total_count = 0\n",
    "\n",
    "    test_val_ratio_overall = pt_data[\"noLQrecs\"].sum() / (\n",
    "                pt_data[\"noLQrecs\"].sum() + pt_data[\"noHQrecsNotUndecided\"].sum())\n",
    "    train_ratio_overall = pt_data[\"noLQrecs\"].sum() / pt_data[\"noRecs\"].sum()\n",
    "\n",
    "    for val, pt in pt_data.iterrows():\n",
    "        total_total_count += pt[\"noHQrecsNotUndecided\"]\n",
    "        total_lq_count += pt[\"noLQrecs\"]\n",
    "\n",
    "        exp_total_counts = total_total_count * fracs\n",
    "        exp_lq_counts = total_lq_count * fracs\n",
    "\n",
    "        loss_0 = np.sum(np.abs(lq_counts + np.array([pt[\"noLQrecs\"], 0, 0]) - exp_lq_counts) + np.abs(\n",
    "            total_counts + np.array([pt[\"noHQrecsNotUndecided\"], 0, 0]) - exp_total_counts))\n",
    "        loss_1 = np.sum(np.abs(lq_counts + np.array([0, pt[\"noLQrecs\"], 0]) - exp_lq_counts) + np.abs(\n",
    "            total_counts + np.array([0, pt[\"noHQrecsNotUndecided\"], 0]) - exp_total_counts))\n",
    "        loss_2 = np.sum(np.abs(lq_counts + np.array([0, 0, pt[\"noLQrecs\"]]) - exp_lq_counts) + np.abs(\n",
    "            total_counts + np.array([0, 0, pt[\"noHQrecsNotUndecided\"]]) - exp_total_counts))\n",
    "\n",
    "        min_loss = min([loss_0, loss_1, loss_2])\n",
    "\n",
    "        if min_loss == loss_0:\n",
    "            train_patients.append(pt)\n",
    "            lq_counts += np.array([pt[\"noLQrecs\"], 0, 0], dtype=int)\n",
    "            total_counts += np.array([pt[\"noHQrecsNotUndecided\"], 0, 0], dtype=int)\n",
    "        elif min_loss == loss_1:\n",
    "            test_patients.append(pt)\n",
    "            lq_counts += np.array([0, pt[\"noLQrecs\"], 0], dtype=int)\n",
    "            total_counts += np.array([0, pt[\"noHQrecsNotUndecided\"], 0], dtype=int)\n",
    "        else:\n",
    "            val_patients.append(pt)\n",
    "            lq_counts += np.array([0, 0, pt[\"noLQrecs\"]], dtype=int)\n",
    "            total_counts += np.array([0, 0, pt[\"noHQrecsNotUndecided\"]], dtype=int)\n",
    "\n",
    "    train_pt_df = pd.DataFrame(train_patients)\n",
    "    test_pt_df = pd.DataFrame(test_patients)\n",
    "    val_pt_df = pd.DataFrame(val_patients)\n",
    "\n",
    "    print(f\"Test high quality: {test_pt_df['noHQrecsNotUndecided'].sum()} low quality: {test_pt_df['noLQrecs'].sum()} \")\n",
    "    print(\n",
    "        f\"Train high quality: {train_pt_df['noHQrecsNotUndecided'].sum()} low quality: {train_pt_df['noLQrecs'].sum()} \")\n",
    "    print(f\"Val high quality: {val_pt_df['noHQrecsNotUndecided'].sum()} low quality: {val_pt_df['noLQrecs'].sum()}\")\n",
    "\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "    val_dataloader = None\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "    val_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        train_dataset = ecg_data[(ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"]))]\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = ecg_data[\n",
    "            (ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "\n",
    "\n",
    "    if not val_pt_df.empty:\n",
    "        val_dataset = ecg_data[\n",
    "            (ecg_data[\"ptID\"].isin(val_pt_df[\"ptID\"])) & (ecg_data[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = make_SAFER_dataloaders(feas2_pt_data, feas2_ecg_data, test_frac=0.15, val_frac=0.15, only_clean_training=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### An aside on optimising the length transform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_yoav = nd._length_transfrom(test_dataset[\"data\"][0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_mine = nd._length_transform_faster(test_dataset[\"data\"][0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_code():\n",
    "    ecgs = test_dataset[\"data\"].head(20)\n",
    "    nd.is_noisy_batch(ecgs, fs=300)\n",
    "\n",
    "import cProfile\n",
    "cProfile.run('test_code()')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run Yoav's classifier and analyse the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgs = val_dataset[\"data\"]\n",
    "val_dataset[\"predictions\"] = nd.is_noisy_batch(ecgs, fs=300, filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataset[\"predictions\"] = val_dataset[\"predictions\"].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot examples of the data with window by window classification of the errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 100\n",
    "\n",
    "print(test_dataset.iloc[i][\"class\"])\n",
    "nd.plot_ecg(test_dataset.iloc[i][\"data\"], fs=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(val_dataset[\"class_index\"], val_dataset[\"predictions\"])\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(conf_mat)\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Other F1: {F1_ind(conf_mat, 1)}\")\n",
    "\n",
    "# ConfusionMatrixDisplay.from_predictions(test_dataset[\"class_index\"], test_dataset[\"predictions\"], display_labels=[\"sufficient quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1])}\")\n",
    "print(f\"Specificity: {conf_mat[0, 0]/np.sum(conf_mat[0])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "false_positives = test_dataset[(test_dataset[\"predictions\"] == 1) & (test_dataset[\"class_index\"] == 0)]\n",
    "false_negatives = test_dataset[(test_dataset[\"predictions\"] == 0) & (test_dataset[\"class_index\"] == 1)]\n",
    "noisy = test_dataset[(test_dataset[\"predictions\"] == 1) & (test_dataset[\"class_index\"] == 1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nd.plot_ecg(false_positives.iloc[50][\"data\"], fs=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nd.plot_ecg(false_negatives.iloc[0][\"data\"], fs=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nd.plot_ecg(noisy.iloc[0][\"data\"], fs=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try on SAFER data\n",
    "ecgs = feas2_ecg_data[\"data\"]\n",
    "feas2_ecg_data[\"predictions\"] = nd.is_noisy_batch(ecgs, fs=300, filter=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_ecg_data[\"predictions\"] = feas2_ecg_data[\"predictions\"].astype(int)\n",
    "feas2_ecg_data[\"class_index\"] = feas2_ecg_data[\"measDiag\"].map(lambda x: int(x == DiagEnum.PoorQuality))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(feas2_ecg_data[\"class_index\"], feas2_ecg_data[\"predictions\"])\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat, 1)}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(feas2_ecg_data[\"class_index\"], feas2_ecg_data[\"predictions\"], display_labels=[\"sufficient quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "false_positives = feas2_ecg_data[(feas2_ecg_data[\"predictions\"] == 1) & (feas2_ecg_data[\"class_index\"] == 0)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nd.plot_ecg(false_positives.iloc[70][\"data\"], fs=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try Training the SVM ourselves and see what the results are\n",
    "\n",
    "### first split the ECGs into segments and compute the features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import ecg_noise_detector.src.ecg_noise_detector.noiseDetector as nd\n",
    "import importlib\n",
    "importlib.reload(nd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ecg_df = train_dataset\n",
    "test_ecg_df = test_dataset\n",
    "val_ecg_df = val_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ecg_df[\"measDiag\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract features for 5s segments of the data using Yoav's code\n",
    "\n",
    "def process_ecgs(dataset):\n",
    "    i = 0\n",
    "    ecg_features = []\n",
    "\n",
    "    for ind, x in dataset.iterrows():\n",
    "        print(f\"Processing ecg {i}/{len(dataset.index)}\\r\", end=\"\")\n",
    "        df = nd._process_ecg(x[\"data\"], fs=300, filter=False)\n",
    "        df[\"class_index\"] = x[\"class_index\"]\n",
    "        df[\"ecg_ind\"] = ind\n",
    "\n",
    "        ecg_start_inds = np.arange(0, int(len(x[\"data\"]) + (- 5 + 2.5)*300), int(2.5*300))\n",
    "        print(ecg_start_inds)\n",
    "        df[\"ecg_start\"] = ecg_start_inds\n",
    "        df[\"measDiag\"] = x[\"measDiag\"]\n",
    "        ecg_features.append(df)\n",
    "        i += 1\n",
    "\n",
    "    return pd.concat(ecg_features, keys=dataset.index)\n",
    "\n",
    "train_dataset = process_ecgs(train_ecg_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract features for 5s segments of the data using Yoav's code\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "def process_ecgs_multicore(dataset):\n",
    "    ecg_features = []\n",
    "\n",
    "    dataset_iter = dataset.iterrows()\n",
    "    with Pool(processes=12) as pool:\n",
    "        for f in tqdm.tqdm(pool.imap_unordered(nd._process_single_ecg, dataset_iter, chunksize=32)):\n",
    "            ecg_features.append(f)\n",
    "\n",
    "    return pd.concat(ecg_features, keys=dataset.index)\n",
    "\n",
    "# train_dataset = process_ecgs_multicore(train_ecg_df)\n",
    "test_dataset = process_ecgs_multicore(test_ecg_df)\n",
    "val_dataset = process_ecgs_multicore(val_ecg_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "pk_path = r\"D:\\2022_23_DSiromani\\Feas2\\ECGs\"\n",
    "train_dataset.to_pickle(os.path.join(pk_path, \"safer_yeov_processed_4.pk\"))\n",
    "test_dataset.to_pickle(os.path.join(pk_path, \"safer_yeov_processed_test_4.pk\"))\n",
    "val_dataset.to_pickle(os.path.join(pk_path, \"safer_yeov_processed_val_4.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the dataset of precomputed features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "pk_path = r\"D:\\2022_23_DSiromani\\Feas2\\ECGs\"\n",
    "train_dataset = pd.read_pickle(os.path.join(pk_path, \"safer_yeov_processed_4.pk\"))\n",
    "test_dataset = pd.read_pickle(os.path.join(pk_path, \"safer_yeov_processed_test_4.pk\"))\n",
    "val_dataset = pd.read_pickle(os.path.join(pk_path, \"safer_yeov_processed_val_4.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[train_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "val_dataset = val_dataset[val_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "# A full dataset for cross validation/other processing\n",
    "total_dataset = pd.concat([train_dataset, test_dataset, val_dataset])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise some features to have 0 mean and variance 1 (other elements are already limited/normalised)\n",
    "\n",
    "var_1_features = [\"sSQI\", \"kSQI\"]\n",
    "for f in var_1_features:\n",
    "    train_dataset[f] = (train_dataset[f] - total_dataset[f].mean())/total_dataset[f].std()\n",
    "    test_dataset[f] = (test_dataset[f] - total_dataset[f].mean())/total_dataset[f].std()\n",
    "    val_dataset[f] = (val_dataset[f] - total_dataset[f].mean())/total_dataset[f].std()\n",
    "    total_dataset[f] = (total_dataset[f] - total_dataset[f].mean())/total_dataset[f].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the data as a matlab file with the raw ECG segments for the signal processing selection/relabelling\n",
    "\n",
    "mat_path = r\"D:\\2022_23_DSiromani\\Feas2\\ECGs\"\n",
    "cut_size = int(2.5 * 300)\n",
    "\n",
    "total_dataset['ecg'] = total_dataset.apply(lambda x: feas2_ecg_data.loc[x[\"ecg_ind\"]][\"data\"][x[\"ecg_start\"]:x[\"ecg_start\"]+cut_size], axis=1)\n",
    "matlab_dict = total_dataset.to_dict(\"list\")\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "scipy.io.savemat(os.path.join(mat_path, \"safer_yeov_processed_norm_3.mat\"), matlab_dict, appendmat=True, format='5', long_field_names=False, do_compression=False, oned_as='row')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load selection/relabelling results from Matlab\n",
    "\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "mat_path = r\"D:\\2022_23_DSiromani\\Feas2\\ECGs\"\n",
    "rejection_list = scipy.io.loadmat(os.path.join(mat_path, \"safer_yeov_processed_rejection_total.mat\"))[\"noisy_samples\"][:, 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_dataset['sig_proc_noise_score'] = rejection_list\n",
    "train_dataset = train_dataset[train_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate the results\n",
    "\n",
    "plt.hist(train_dataset[train_dataset[\"class_index\"] == 0][\"sig_proc_noise_score\"], bins=np.linspace(0, 750, 75))\n",
    "plt.hist(train_dataset[train_dataset[\"class_index\"] == 1][\"sig_proc_noise_score\"], bins=np.linspace(0, 750, 75))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signal_proc_sel  = ((train_dataset['sig_proc_noise_score'] < 50) & (train_dataset[\"class_index\"] == 0)) | ((train_dataset[\"sig_proc_noise_score\"] > 100) & (train_dataset[\"class_index\"] == 1))\n",
    "print(signal_proc_sel.value_counts())\n",
    "\n",
    "train_dataset_selected = train_dataset[signal_proc_sel]\n",
    "\n",
    "# signal_proc_relabel = train_dataset[\"sig_proc_noise_score\"] > 100\n",
    "# train_dataset[\"class_index_sig_proc\"] = signal_proc_relabel.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signal_proc_sel = ((total_dataset['sig_proc_noise_score'] < 50) & (total_dataset[\"class_index\"] == 0)) | (\n",
    "            (total_dataset[\"sig_proc_noise_score\"] > 100) & (total_dataset[\"class_index\"] == 1\n",
    "))\n",
    "print(signal_proc_sel.value_counts())\n",
    "total_dataset_selected = total_dataset[signal_proc_sel]\n",
    "\n",
    "# signal_proc_relabel = train_dataset[\"sig_proc_noise_score\"] > 100\n",
    "# train_dataset[\"class_index_sig_proc\"] = signal_proc_relabel.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset_selected = train_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = total_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "class_weights = 1/train_dataset_selected[\"class_index\"].value_counts()\n",
    "class_weights /= np.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "model = SVC(class_weight=class_weights.to_dict())\n",
    "# See documentation for default values e.g. use rbf, regularising C = 1\n",
    "features = [\"sSQI\", \"kSQI\", \"pSQI\", \"basSQI\", \"bSQI\", \"rSQI\"]\n",
    "\n",
    "model = model.fit(train_dataset_selected[features].values, train_dataset_selected[\"class_index\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset[features].values)\n",
    "\n",
    "predictions_series = pd.Series(data=predictions, index=test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided].index)\n",
    "test_dataset[\"predictions\"] = predictions_series\n",
    "\n",
    "# Select the values with more than 50% noisy as overall noisy\n",
    "results_df = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided].groupby(level=0).mean()\n",
    "\n",
    "conf_mat = confusion_matrix(results_df[\"class_index\"], results_df[\"predictions\"].round())\n",
    "print(conf_mat)\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How good is the signal processing alone\n",
    "\n",
    "sig_proc_results_df = train_dataset[train_dataset[\"measDiag\"] != DiagEnum.Undecided].groupby(level=0).mean()\n",
    "conf_mat = confusion_matrix(sig_proc_results_df[\"class_index\"], sig_proc_results_df[\"sig_proc_noise_score\"] > 20)\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Noisy F1: {F1_ind(conf_mat, 1)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "p, r, d = precision_recall_curve(sig_proc_results_df[\"class_index\"], sig_proc_results_df[\"sig_proc_noise_score\"])\n",
    "\n",
    "F1 = 2 * p * r /(p + r)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=r, y=p, hovertext=[f\"decision boundary: {x:.2f}\\nF1 score: {f:.03f}\" for x, f in zip(d, F1)]))\n",
    "\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig.show()\n",
    "\n",
    "# Why are the F values different!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mat_path = r\"D:\\2022_23_DSiromani\\Feas2\\ECGs\"\n",
    "cut_size = int(2.5 * 300)\n",
    "\n",
    "test_dataset['ecg'] = test_dataset.apply(lambda x: feas2_ecg_data.loc[x[\"ecg_ind\"]][\"data\"][x[\"ecg_start\"]:x[\"ecg_start\"]+cut_size], axis=1)\n",
    "matlab_dict = test_dataset.to_dict(\"list\")\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "scipy.io.savemat(os.path.join(mat_path, \"safer_yeov_processed_norm_3_test.mat\"), matlab_dict, appendmat=True, format='5', long_field_names=False, do_compression=False, oned_as='row')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import os\n",
    "\n",
    "mat_path = r\"D:\\2022_23_DSiromani\\Feas2\\ECGs\"\n",
    "rejection_list = scipy.io.loadmat(os.path.join(mat_path, \"safer_yeov_processed_rejection_test.mat\"))[\"noisy_samples\"][:, 0]\n",
    "test_dataset['sig_proc_noise_score'] = rejection_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signal_proc_sel  = ((test_dataset['sig_proc_noise_score'] < 50) & (test_dataset[\"class_index\"] == 0)) | ((test_dataset[\"sig_proc_noise_score\"] > 100) & (test_dataset[\"class_index\"] == 1))\n",
    "print(signal_proc_sel.value_counts())\n",
    "\n",
    "test_dataset_selected = test_dataset[signal_proc_sel]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = total_dataset_selected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = pd.concat([train_dataset, test_dataset])\n",
    "inds = []\n",
    "ptIds = []\n",
    "\n",
    "for i, e in df.iterrows():\n",
    "    inds.append(i)\n",
    "    ptIds.append(feas2_ecg_data[\"ptID\"].loc[e[\"ecg_ind\"]])\n",
    "\n",
    "df[\"ptID\"] = pd.Series(data=ptIds, index=inds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas2_pt_data[\"noRecs\"] = df[\"ptID\"].value_counts()\n",
    "feas2_pt_data[\"noLQrecs\"] = df[df[\"class_index\"] == 1][\"ptID\"].value_counts()\n",
    "\n",
    "num_folds = 5\n",
    "test_pt_folds = [[] for _ in range(num_folds)]\n",
    "\n",
    "sorted_pts = feas2_pt_data.sort_values(\"noLQrecs\", axis=0)\n",
    "group_num = 0\n",
    "\n",
    "# Go around the folds and assign patients to each\n",
    "for _, pt in sorted_pts.iterrows():\n",
    "    test_pt_folds[group_num].append(pt)\n",
    "    group_num = (group_num + 1) % num_folds\n",
    "\n",
    "test_pt_folds = [pd.DataFrame(fold) for fold in test_pt_folds]\n",
    "train_pt_folds = [feas2_pt_data[~feas2_pt_data[\"ptID\"].isin(fold[\"ptID\"])] for fold in test_pt_folds]\n",
    "\n",
    "\"\"\"\n",
    "ind = np.unique(np.array(df.index.get_level_values(0)))\n",
    "classes = np.array([df[\"class_index\"].loc[i, 0] for i in ind])\n",
    "\"\"\"\n",
    "\n",
    "features = [\"sSQI\", \"kSQI\", \"pSQI\", \"basSQI\", \"bSQI\", \"rSQI\"]\n",
    "\n",
    "conf_mats = []\n",
    "\n",
    "for i, (train_pt_df, test_pt_df) in enumerate(zip(train_pt_folds, test_pt_folds)):\n",
    "    print(f\"======== Split {i} ========\")\n",
    "    train_dataset = df[df[\"ptID\"].isin(train_pt_df[\"ptID\"])]\n",
    "    test_dataset =  df[df[\"ptID\"].isin(test_pt_df[\"ptID\"])]\n",
    "\n",
    "    train_dataset = train_dataset[train_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "\n",
    "    model = SVC(class_weight='balanced')\n",
    "    model = model.fit(train_dataset[features].values, train_dataset[\"class_index\"].values)\n",
    "\n",
    "    test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]\n",
    "\n",
    "    predictions = model.predict(test_dataset[features].values)\n",
    "\n",
    "    test_dataset[\"predictions\"] = predictions\n",
    "    results_df = test_dataset.groupby(level=0).mean()\n",
    "    results_df[\"predictions\"] = results_df[\"predictions\"].round()\n",
    "\n",
    "    conf_mat = confusion_matrix(results_df[\"class_index\"], results_df[\"predictions\"])\n",
    "    conf_mats.append(conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"class_index\"].value_counts(dropna=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cv = 4\n",
    "num_folds = 2 * num_cv  # twice to produce the val and test for each fold!\n",
    "pt_folds = [[] for _ in range(num_folds)]\n",
    "\n",
    "lq_counts = np.zeros(num_folds, dtype=int)\n",
    "total_counts = np.zeros(num_folds, dtype=int)\n",
    "\n",
    "total_total_count = 0\n",
    "total_lq_count = 0\n",
    "\n",
    "# Go around the folds and assign patients to each\n",
    "for _, pt in feas2_pt_data.iterrows():\n",
    "    total_total_count += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "    total_lq_count += pt[\"noLQrecs\"]\n",
    "\n",
    "    exp_total_counts = total_total_count * 1.0/num_folds\n",
    "    exp_lq_counts = total_lq_count * 1.0/num_folds\n",
    "\n",
    "    lq_rec_mat = np.diag(np.array([pt[\"noLQrecs\"] for _ in range(num_folds)]))\n",
    "    hq_rec_mat = np.diag(np.array([pt[\"noHQrecsNotUndecided\"] for _ in range(num_folds)]))\n",
    "\n",
    "    loss =  np.sum(np.abs(lq_counts[None, :] + lq_rec_mat - exp_lq_counts) + np.abs(total_counts[None, :] + hq_rec_mat - exp_total_counts), axis=-1)\n",
    "    best_fold = np.argmin(loss)\n",
    "\n",
    "    pt_folds[best_fold].append(pt)\n",
    "    lq_counts[best_fold] += pt[\"noLQrecs\"]\n",
    "    total_counts[best_fold] += pt[\"noHQrecsNotUndecided\"] + pt[\"noLQrecs\"]\n",
    "\n",
    "\n",
    "test_pt_folds = [pd.DataFrame(fold) for fold in pt_folds[:4]]\n",
    "val_pt_folds = [pd.DataFrame(fold) for fold in pt_folds[4:]]\n",
    "train_pt_folds = [feas2_pt_data[(~feas2_pt_data[\"ptID\"].isin(test_fold[\"ptID\"])) & (~feas2_pt_data[\"ptID\"].isin(val_fold[\"ptID\"]))] for test_fold, val_fold in zip(test_pt_folds, val_pt_folds)]\n",
    "\n",
    "for f in test_pt_folds:\n",
    "    print(f[\"noLQrecs\"].sum(), f[\"noHQrecsNotUndecided\"].sum())\n",
    "\n",
    "for f in val_pt_folds:\n",
    "    print(f[\"noLQrecs\"].sum(), f[\"noHQrecsNotUndecided\"].sum())\n",
    "\n",
    "conf_mats = []\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for i, (train_pt_df, test_pt_df, val_pt_df) in enumerate(zip(train_pt_folds, test_pt_folds, val_pt_folds)):\n",
    "    print(f\"Fold {i}\")\n",
    "    train_df = df[(df[\"ptID\"].isin(train_pt_df[\"ptID\"])) & (df[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "    test_df = df[(df[\"ptID\"].isin(test_pt_df[\"ptID\"])) & (df[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "    val_df = df[(df[\"ptID\"].isin(val_pt_df[\"ptID\"])) & (df[\"measDiag\"] != DiagEnum.Undecided)]\n",
    "\n",
    "    print(train_df[\"class_index\"].value_counts())\n",
    "    print(test_df[\"class_index\"].value_counts())\n",
    "    print(val_df[\"class_index\"].value_counts())\n",
    "\n",
    "    model = SVC(class_weight='balanced')\n",
    "    model = model.fit(train_df[features].values, train_df[\"class_index\"].values)\n",
    "\n",
    "    predictions = model.predict(val_df[features].values)\n",
    "    print(len(predictions))\n",
    "\n",
    "    test_predictions = model.predict(test_df[features].values)\n",
    "\n",
    "    val_df.loc[:, \"predictions\"] = predictions\n",
    "    results_df = val_df.groupby(level=0).mean()\n",
    "    results_df.loc[:, \"predictions\"] = results_df[\"predictions\"].round()\n",
    "\n",
    "    test_df[\"predictions\"] = test_predictions\n",
    "    test_results_df = val_df.groupby(level=0).mean()\n",
    "    test_results_df[\"predictions\"] = test_results_df[\"predictions\"].round()\n",
    "\n",
    "    print(results_df[\"predictions\"].value_counts(dropna=False))\n",
    "\n",
    "    conf_mat = confusion_matrix(results_df[\"class_index\"], results_df[\"predictions\"])\n",
    "    conf_mats.append(conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Normal F1: {np.mean([F1_ind(c, 0) for c in conf_mats])}\")\n",
    "print(f\"Poor quality F1: {np.mean([F1_ind(c, 1) for c in conf_mats])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[F1_ind(c, 1) for c in conf_mats]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try putting features for each section into the classifier as one input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct test and train matrices\n",
    "\n",
    "features = [\"sSQI\", \"kSQI\", \"pSQI\", \"basSQI\", \"bSQI\", \"rSQI\"]\n",
    "train_matrix = []\n",
    "train_targets = []\n",
    "indexes = []\n",
    "\n",
    "for i in set(train_dataset.index.get_level_values(0)):\n",
    "    indexes.append(i)\n",
    "    train_matrix.append(train_dataset.loc[i][features].values.flatten())\n",
    "    train_targets.append(train_dataset.loc[(i, 0)][\"class_index\"])\n",
    "\n",
    "train_matrix = np.array(train_matrix)\n",
    "train_targets = np.array(train_targets)\n",
    "\n",
    "print(train_matrix.shape)\n",
    "print(train_targets.shape)\n",
    "\n",
    "\n",
    "test_matrix=  []\n",
    "test_targets = []\n",
    "for i in set(test_dataset.index.get_level_values(0)):\n",
    "    test_matrix.append(test_dataset.loc[i][features].values.flatten())\n",
    "    test_targets.append(test_dataset.loc[(i, 0)][\"class_index\"])\n",
    "\n",
    "\n",
    "test_matrix = np.array(test_matrix)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "\n",
    "val_matrix=  []\n",
    "val_targets = []\n",
    "for i in set(val_dataset.index.get_level_values(0)):\n",
    "    val_matrix.append(val_dataset.loc[i][features].values.flatten())\n",
    "    val_targets.append(val_dataset.loc[(i, 0)][\"class_index\"])\n",
    "\n",
    "val_matrix = np.array(val_matrix)\n",
    "val_targets = np.array(val_targets)\n",
    "\n",
    "print(test_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the model\n",
    "class_weights = 1/train_dataset[\"class_index\"].value_counts()\n",
    "class_weights /= np.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "model = SVC(class_weight=class_weights.to_dict(), probability=True)\n",
    "# See documentation for default values e.g. use rbf, regularising C = 1\n",
    "features = [\"sSQI\", \"kSQI\", \"pSQI\", \"basSQI\", \"bSQI\", \"rSQI\"]\n",
    "\n",
    "model = model.fit(train_matrix, train_targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(val_matrix)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(val_targets, np.round(predictions[:, 1]), display_labels=[\"sufficient quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "plt.show()\n",
    "conf_mat = confusion_matrix(val_targets, np.round(predictions[:, 1]))\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(conf_mat)\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Other F1: {F1_ind(conf_mat, 1)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p, r, d = precision_recall_curve(val_targets, predictions[:, 1])\n",
    "\n",
    "F1 = 2 * p * r /(p + r)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=r, y=p, hovertext=[f\"decision boundary: {x:.2f}\\nF1 score: {f:.03f}\" for x, f in zip(d, F1)]))\n",
    "\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1])}\")\n",
    "print(f\"Specificity: {conf_mat[0, 0]/np.sum(conf_mat[0])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "false_positives = np.array(list(set(test_dataset.index.get_level_values(0))))[(test_targets == 0) * (predictions == 1)]\n",
    "print(false_positives[0])\n",
    "\n",
    "print(test_matrix[(test_targets == 0) * (predictions == 1)][1])\n",
    "\n",
    "plt.plot(dataset[\"data\"].loc[false_positives[1]])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why am I having poorer accuracy here?\n",
    " - Is it because the SVM is not able to understand so many variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Put all the segments into an LSTM classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        X = row[\"data\"]\n",
    "        y = row[\"class_index\"]\n",
    "        ind = row[\"ind\"]\n",
    "\n",
    "        return X, y, ind"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = [\"sSQI\", \"kSQI\", \"pSQI\", \"basSQI\", \"bSQI\", \"rSQI\"]\n",
    "train_matrix = []\n",
    "train_targets = []\n",
    "indexes = []\n",
    "\n",
    "for i in set(train_dataset.index.get_level_values(0)):\n",
    "    indexes.append(i)\n",
    "    train_matrix.append(train_dataset.loc[i][features].values.flatten())\n",
    "    train_targets.append(train_dataset.loc[(i, 0)][\"class_index\"])\n",
    "\n",
    "train_df = pd.DataFrame({\"data\": train_matrix, \"class_index\": train_targets, \"ind\": indexes})\n",
    "train_dataloader = DataLoader(Dataset(train_df), batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_matrix=  []\n",
    "test_targets = []\n",
    "indexes = []\n",
    "\n",
    "for i in set(test_dataset.index.get_level_values(0)):\n",
    "    indexes.append(i)\n",
    "    test_matrix.append(test_dataset.loc[i][features].values.flatten())\n",
    "    test_targets.append(test_dataset.loc[(i, 0)][\"class_index\"])\n",
    "\n",
    "test_df = pd.DataFrame({\"data\": test_matrix, \"class_index\": test_targets, \"ind\": indexes})\n",
    "test_dataloader = DataLoader(Dataset(test_df), batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "        self.lstm_n_hidden = 16\n",
    "        self.lstm_n_input = 6\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_n_input, hidden_size=self.lstm_n_hidden, bidirectional=True, batch_first=True, num_layers=2)\n",
    "\n",
    "        self.linear1 = nn.Linear(self.lstm_n_hidden * 2, self.lstm_n_hidden*2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.lstm_n_hidden*2, 1)\n",
    "\n",
    "    def init_lstm_hidden(self, batch_size, device):\n",
    "        # This resets the LSTM hidden state after each batch\n",
    "        hidden_state = torch.zeros(4, batch_size, self.lstm_n_hidden, device=device)\n",
    "        cell_state = torch.zeros(4, batch_size, self.lstm_n_hidden, device=device)\n",
    "        return (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, 30, 5]\n",
    "        _, (h, _) = self.lstm(x, self.init_lstm_hidden(x.shape[0], x.device))\n",
    "        h = torch.flatten(torch.transpose(h, 0, 1)[:, 2:, :], 1, 2)\n",
    "\n",
    "        pred = self.linear1(h)\n",
    "        pred = self.activation(pred)\n",
    "        pred = self.linear2(pred)\n",
    "        return pred[:, 0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model = LSTM_Classifier().to(device)\n",
    "\n",
    "# Use weightings to handle class imbalance\n",
    "\n",
    "class_counts = torch.tensor(train_df[\"class_index\"].value_counts().values.astype(np.float32))\n",
    "class_weights = (class_counts[1] + class_counts[0])/class_counts[1]\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "model = model.to(device)\n",
    "\n",
    "def train(model):\n",
    "    best_test_loss = 100\n",
    "    best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"starting epoch {epoch} ...\")\n",
    "        # Train\n",
    "        num_batches = 0\n",
    "        model.train()\n",
    "        for i, (features, labels, _) in enumerate(train_dataloader):\n",
    "            features = torch.unsqueeze(features.to(device), 1).float()\n",
    "            features = torch.reshape(features, (features.shape[0], -1, 6))\n",
    "\n",
    "            # fft = torch.abs(torch.fft.fft(signals))\n",
    "            # signals = torch.cat([signals, fft], dim=1)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            loss = loss_func(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            num_batches += 1\n",
    "            total_loss += float(loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "        print(\"Testing ...\")\n",
    "        # Test\n",
    "        num_test_batches = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, (features, labels, _) in enumerate(test_dataloader):\n",
    "                features = torch.unsqueeze(features.to(device), 1).float()\n",
    "                features = torch.reshape(features, (features.shape[0], -1, 6))\n",
    "                # fft = torch.abs(torch.fft.fft(signals))\n",
    "                # signals = torch.cat([signals, fft], dim=1)\n",
    "                labels = labels.float().to(device)\n",
    "                output = model(features)\n",
    "                loss = loss_func(output, labels)\n",
    "                test_loss += float(loss)\n",
    "                num_test_batches += 1\n",
    "\n",
    "        print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "\n",
    "        if test_loss/num_test_batches < best_test_loss:\n",
    "            best_model = copy.deepcopy(model).cpu()\n",
    "            best_test_loss = test_loss/num_test_batches\n",
    "\n",
    "        losses.append([total_loss/num_batches, test_loss/num_test_batches])\n",
    "\n",
    "    return best_model, losses\n",
    "\n",
    "model, losses = train(model)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot test data reconstruction\n",
    "test_df[\"prediction\"] = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (signals, _, ind) in enumerate(test_dataloader):\n",
    "        signals = torch.unsqueeze(signals.to(device), 1).float()\n",
    "        signals = torch.reshape(signals, (signals.shape[0], -1, 6))\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        output = model(signals).detach().cpu().numpy()\n",
    "\n",
    "        for i, o in zip(ind, output):\n",
    "            test_df[\"prediction\"].loc[int(i)] = o\n",
    "\n",
    "test_df_predicted = test_df.dropna(subset=[\"prediction\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "test_targets = test_df_predicted[\"class_index\"].to_numpy(dtype=float)\n",
    "predictions = sigmoid(test_df_predicted[\"prediction\"].to_numpy(dtype=float))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(test_targets, np.round(predictions), display_labels=[\"sufficient quality\", \"insufficient quality\"], cmap=\"inferno\")\n",
    "plt.show()\n",
    "conf_mat = confusion_matrix(test_targets, np.round(predictions))\n",
    "\n",
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "print(f\"Normal F1: {F1_ind(conf_mat, 0)}\")\n",
    "print(f\"Other F1: {F1_ind(conf_mat, 1)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "p, r, d = precision_recall_curve(test_targets, predictions)\n",
    "\n",
    "F1 = 2 * p * r /(p + r)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=r, y=p, hovertext=[f\"decision boundary: {x:.2f}\\nF1 score: {f:.03f}\" for x, f in zip(d, F1)]))\n",
    "\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
