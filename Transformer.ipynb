{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import scipy.signal\n",
    "from ecgdetectors import Detectors\n",
    "\n",
    "import math\n",
    "from DataHandlers.DiagEnum import DiagEnum\n",
    "import DataHandlers.DiagEnum\n",
    "import DataHandlers.SAFERDataset as SAFERDataset\n",
    "import DataHandlers.CinC2020Dataset as CinC2020Dataset\n",
    "import DataHandlers.CinC2020Enums\n",
    "import importlib\n",
    "import DataHandlers.CinCDataset as CinCDataset\n",
    "import DataHandlers.DataAugmentations as DataAugmentations\n",
    "from multiprocesspandas import applyparallel\n",
    "importlib.reload(SAFERDataset)\n",
    "importlib.reload(CinC2020Dataset)\n",
    "\n",
    "import DataHandlers.DataProcessUtilities\n",
    "importlib.reload(DataHandlers.DataProcessUtilities)\n",
    "from DataHandlers.DataProcessUtilities import *\n",
    "import Utilities.Plotting\n",
    "importlib.reload(Utilities.Plotting)\n",
    "from Utilities.Plotting import *\n",
    "\n",
    "# A fudge because I moved the files\n",
    "sys.modules[\"SAFERDataset\"] = SAFERDataset\n",
    "sys.modules[\"CinC2020Dataset\"] = CinC2020Dataset\n",
    "sys.modules[\"DiagEnum\"] = DataHandlers.DiagEnum\n",
    "sys.modules[\"CinC2020Enums\"] = DataHandlers.CinC2020Enums\n",
    "sys.modules[\"CinCDataset\"] = CinCDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59732477 0.08337213 0.3193031 ]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "print(softmax(np.array([0.99704283, -0.9721041, 0.37072268])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "enable_cuda = True\n",
    "\n",
    "if torch.cuda.is_available() and enable_cuda:\n",
    "    print(\"Using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load SAFER data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\2022_23_DSiromani\\Feas2\\ECGs/filtered_dataframe_reload.pk\n"
     ]
    }
   ],
   "source": [
    "feas2_pt_data, feas2_ecg_data = SAFERDataset.load_feas_dataset(2, \"dataframe_reload\")\n",
    "feas2_ecg_data[\"measID\"] += 300000\n",
    "feas2_ecg_data.index = feas2_ecg_data[\"measID\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "feas2_ecg_data[\"feas\"] = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "def reduce_normals_all_other_af(pt_data, ecg_data):\n",
    "    accepted_meas_diags = [DiagEnum.AF, DiagEnum.NoAF, DiagEnum.HeartBlock]\n",
    "    ecg_data = ecg_data[(ecg_data[\"measDiag\"].isin(accepted_meas_diags)) | (ecg_data[\"measID\"] < 20000) | (ecg_data[\"not_tagged_ign_wide_qrs\"] == 0)]\n",
    "    pt_data = pt_data[pt_data[\"ptID\"].isin(ecg_data[\"ptID\"])]\n",
    "\n",
    "    return pt_data, ecg_data\n",
    "\n",
    "# warning: changing these chunk sizes may reload feas1 data from scratch, which will take ages\n",
    "chunk_size = 20000\n",
    "num_chunks = math.ceil(162515 / chunk_size )\n",
    "\n",
    "def load_feas1_chunk_range(chunk_range=(0, num_chunks)):\n",
    "    ecg_data = []\n",
    "    pt_data = []\n",
    "\n",
    "    for chunk_num in range(chunk_range[0], chunk_range[1]):\n",
    "        feas1_pt_data, feas1_ecg_data = SAFERDataset.load_feas_dataset(1, f\"dataframe_{chunk_num}.pk\")\n",
    "\n",
    "        ecg_data.append(feas1_ecg_data)\n",
    "        pt_data.append(feas1_pt_data)\n",
    "\n",
    "    feas1_ecg_data = pd.concat(ecg_data)\n",
    "    feas1_ecg_data[\"feas\"] = 1\n",
    "    feas1_ecg_data[\"rri_len\"] = feas1_ecg_data[\"rri_feature\"].map(lambda x: x[x > 0].shape[-1])\n",
    "    feas1_pt_data = pd.concat(pt_data).drop_duplicates()\n",
    "\n",
    "    return feas1_ecg_data, feas1_pt_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas1_ecg_data, feas1_pt_data = load_feas1_chunk_range((0, num_chunks))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "def prepare_safer_data(pt_data, ecg_data):\n",
    "    if \"length\" in ecg_data:\n",
    "        ecg_data = ecg_data[ecg_data[\"length\"] == 9120]\n",
    "\n",
    "    ecg_data = ecg_data[ecg_data[\"measDiag\"] != DiagEnum.PoorQuality]\n",
    "    # ecg_data = ecg_data[ecg_data[\"tag_orig_Poor_Quality\"] == 0]\n",
    "\n",
    "    ecg_data = ecg_data[ecg_data[\"rri_len\"] > 5]\n",
    "\n",
    "\n",
    "    pt_data.index = pt_data[\"ptID\"]\n",
    "    ecg_data = SAFERDataset.generate_af_class_labels(ecg_data)\n",
    "    pt_data = SAFERDataset.add_ecg_class_counts(pt_data, ecg_data)\n",
    "\n",
    "    return pt_data, ecg_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# just use feas2\n",
    "safer_ecg_data = feas2_ecg_data\n",
    "safer_ecg_data[\"ffReview_sent\"] = -1\n",
    "safer_ecg_data[\"ffReview_remain\"] = -1\n",
    "safer_pt_data = feas2_pt_data\n",
    "\n",
    "safer_pt_data, safer_ecg_data = prepare_safer_data(safer_pt_data, safer_ecg_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22274\n",
      "2      377\n",
      "1      102\n",
      "Name: class_index, dtype: int64\n",
      "0    22862\n",
      "2      452\n",
      "1      126\n",
      "Name: class_index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Just use feas1 to prepare test and validation datasets (The train is best handled with a DatasetSequenceIterator)\n",
    "feas1_pt_data, feas1_ecg_data = prepare_safer_data(feas1_pt_data, feas1_ecg_data)\n",
    "feas1_ecg_data[\"class_index\"].value_counts()\n",
    "\n",
    "feas1_ecg_data_test = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(test_pts[\"ptID\"])]\n",
    "feas1_ecg_data_val = feas1_ecg_data[feas1_ecg_data[\"ptID\"].isin(val_pts[\"ptID\"])]\n",
    "\n",
    "print(feas1_ecg_data_test[\"class_index\"].value_counts())\n",
    "print(feas1_ecg_data_val[\"class_index\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "doc_path = r\"C:\\Users\\daniel\\Documents\"\n",
    "\n",
    "feas1_ecg_data_test.to_pickle(os.path.join(doc_path, \"feas1_test_27_mar.pk\"))\n",
    "feas1_ecg_data_val.to_pickle(os.path.join(doc_path, \"feas1_val_27_mar.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "safer_ecg_data = pd.concat([feas2_ecg_data, feas1_ecg_data])\n",
    "safer_pt_data = pd.concat([feas2_pt_data, feas1_pt_data])\n",
    "\n",
    "safer_pt_data, safer_ecg_data = prepare_safer_data(safer_pt_data, safer_ecg_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "feas  class_index\n2     0              19513\n      2                757\n      1                 16\nName: class_index, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safer_ecg_data.groupby(\"feas\")[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# Plot a heartrate histogram for AF and not AF\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=300)\n",
    "ax.hist(safer_ecg_data[\"heartrate\"][(safer_ecg_data[\"measDiag\"] != DiagEnum.AF) & (safer_ecg_data[\"feas\"] == 1)], alpha=0.7, density=True, label=\"Normal or Other Rhythm\")\n",
    "ax.hist(safer_ecg_data[\"heartrate\"][(safer_ecg_data[\"measDiag\"] == DiagEnum.AF) & (safer_ecg_data[\"feas\"] == 1)], alpha=0.7, density=True, label=\"AF\")\n",
    "ax.set_xlabel(\"Heartrate (bpm)\")\n",
    "ax.set_ylabel(\"Frequency proportion\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Cut out high and low heartrates - I dont think this makes a difference so havent been doing it mostly\n",
    "safer_ecg_data = safer_ecg_data[(safer_ecg_data[\"heartrate\"] < 120) & (safer_ecg_data[\"heartrate\"] > 50)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _, ecg in safer_ecg_data[safer_ecg_data[\"feas\"] == 1].sample(frac=1).iterrows():\n",
    "    print(ecg[[\"measDiag\", \"class_index\", \"heartrate\", \"r_peaks\"]])\n",
    "    plot_ecg(ecg[\"data\"], r_peaks=ecg[\"r_peaks\"], fs=300, n_split=3)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Plot the 1 feas2 AF example with high heartrate!\n",
    "\n",
    "plot_ecg(safer_ecg_data.loc[310209][\"data\"], r_peaks=safer_ecg_data.loc[310209][\"r_peaks\"], fs=300, n_split=3, figsize=(6, 5), export_quality=True)\n",
    "plot_ecg_spectrogram(safer_ecg_data.loc[310209][\"data\"], fs=300, n_split=3, figsize=(6, 5), export_quality=True, cut_range=(2, 18))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load CinC 2020 data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import DataHandlers.CinC2020Dataset as CinC2020Dataset\n",
    "import importlib\n",
    "importlib.reload(CinC2020Dataset)\n",
    "\n",
    "df = CinC2020Dataset.load_dataset(save_name=\"dataframe_2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# At the moment we only select data with length which can be truncated to 3000 samples (10s)\n",
    "def select_length(df):\n",
    "    df_within_range = df[(df[\"length\"] <= 5000) & (df[\"length\"] >= 3000)].copy()\n",
    "    df_within_range[\"data\"] = df_within_range[\"data\"].map(lambda x: x[:3000])\n",
    "    df_within_range[\"length\"] = df_within_range[\"data\"].map(lambda x: x.shape[0])\n",
    "    return df_within_range\n",
    "\n",
    "df = select_length(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Plot a heartrate histogram for AF and not AF\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=300)\n",
    "ax.hist(df[\"heartrate\"][(df[\"measDiag\"] != DiagEnum.AF)], alpha=0.7, density=True, label=\"Normal or Other Rhythm\")\n",
    "ax.hist(df[\"heartrate\"][(df[\"measDiag\"] == DiagEnum.AF)], alpha=0.7, density=True, label=\"AF\")\n",
    "ax.set_xlabel(\"Heartrate (bpm)\")\n",
    "ax.set_ylabel(\"Frequency proportion\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04636585375203334\n",
      "-0.12941265829123266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [26], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m plot_ecg(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m noise, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m2\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m plot_ecg(noise, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m2\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "noise = noise_df.sample()[\"data\"].iloc[0] * np.random.normal(scale=1)\n",
    "\n",
    "for _, ecg in df.iterrows():\n",
    "    noise_scale = np.random.normal(scale=0.2)\n",
    "    noise = noise_df.sample()[\"data\"].iloc[0] * noise_scale\n",
    "    print(noise_scale)\n",
    "    plot_ecg(ecg[\"data\"], figsize=(5, 2), export_quality=True)\n",
    "    plot_ecg(ecg[\"data\"] + noise, figsize=(5, 2), export_quality=True)\n",
    "    plot_ecg(noise, figsize=(5, 2), export_quality=True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [83], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, ecg \u001B[38;5;129;01min\u001B[39;00m df[df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[0;32m      2\u001B[0m     plot_ecg(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m1500\u001B[39m], figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m2.5\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 3\u001B[0m     \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for _, ecg in df[df[\"class_index\"] == 0].iterrows():\n",
    "    plot_ecg(ecg[\"data\"][:1500], figsize=(5, 2.5), export_quality=True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "dataset               class_index\ncpsc_2018             2               2047\n                      0                984\n                      1                903\ncpsc_2018_extra       2                364\n                      0                350\n                      1                113\ngeorgia               2               5257\n                      0               3508\n                      1                566\nptb-xl                0              10692\n                      2               9349\n                      1               1514\nst_petersburg_incart  0              10955\n                      1               1010\n                      2                363\nName: class_index, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"dataset\")[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load noise from MIT database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "noises = [\"em\", \"ma\"]\n",
    "noise_dfs = []\n",
    "mit_dataset_path = \"Datasets/mit-bih-noise-stress-test-database\"\n",
    "\n",
    "f_low = 0.67\n",
    "f_high = 25\n",
    "\n",
    "def split_signal(data, split_len):\n",
    "    data_splits = []\n",
    "    splits = np.arange(0, data[\"data\"].shape[0], split_len)\n",
    "\n",
    "    for i, (start, end) in enumerate(zip(splits, splits[1:])):\n",
    "        data_split = data.copy()\n",
    "        data_split[\"data\"] = data[\"data\"][start:end]\n",
    "        data_split[\"data\"] = (data_split[\"data\"] - data_split[\"data\"].mean())/ data_split[\"data\"].std()\n",
    "\n",
    "        data_split.name = i\n",
    "        data_splits.append(data_split)\n",
    "\n",
    "    return data_splits\n",
    "\n",
    "\n",
    "for n_path in noises:\n",
    "    rec = wfdb.rdrecord(os.path.join(mit_dataset_path, n_path))\n",
    "    sig = np.concatenate([rec.p_signal[:, 0], rec.p_signal[:, 1]])\n",
    "\n",
    "    bandpass = signal.butter(3, [f_low, f_high], 'bandpass', fs=rec.fs, output='sos')\n",
    "    notch = signal.butter(3, [48, 52], 'bandstop', fs=rec.fs, output='sos')\n",
    "\n",
    "    sig = filter_and_norm(sig, bandpass)\n",
    "    sig = filter_and_norm(sig, notch)\n",
    "\n",
    "    sig = resample(sig, rec.fs, 300)\n",
    "    sig_series = pd.Series(data={\"data\": sig, \"fs\": 300, \"noise_type\": n_path})\n",
    "\n",
    "    split_signals = split_signal(sig_series, 3000)\n",
    "    split_signals = pd.DataFrame(split_signals)\n",
    "\n",
    "    noise_dfs.append(split_signals)\n",
    "\n",
    "noise_df = pd.concat(noise_dfs, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load CinC2017 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8528/8528 [00:05<00:00, 1609.09it/s]\n",
      "C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\DataHandlers\\CinCDataset.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ecg_data[\"rri_len\"] = ecg_data[\"rri_feature\"].map(lambda x: x[x > 0].shape[-1])\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(CinCDataset)\n",
    "import DataHandlers.DataProcessUtilities\n",
    "importlib.reload(DataHandlers.DataProcessUtilities)\n",
    "from DataHandlers.DataProcessUtilities import *\n",
    "\n",
    "cinc2017_df = CinCDataset.load_cinc_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "DiagEnum.NoAF                      3694\nDiagEnum.CannotExcludePathology    1649\nDiagEnum.AF                         504\nDiagEnum.PoorQuality                123\nName: measDiag, dtype: int64"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cinc2017_df = cinc2017_df[cinc2017_df[\"length\"] == 9000]\n",
    "cinc2017_df[\"measDiag\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "cinc2017_df = cinc2017_df[cinc2017_df[\"length\"] == 9000]\n",
    "cinc2017_df = cinc2017_df[cinc2017_df[\"measDiag\"] != DiagEnum.PoorQuality]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "0    5847\nName: class_index, dtype: int64"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cinc2017_df[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "# Plot a heartrate histogram for AF and not AF\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=300)\n",
    "ax.hist(cinc2017_df[\"heartrate\"][(cinc2017_df[\"class_index\"] != 1)], alpha=0.7, density=True, label=\"Normal or Other Rhythm\")\n",
    "ax.hist(cinc2017_df[\"heartrate\"][(cinc2017_df[\"class_index\"] == 1)], alpha=0.7, density=True, label=\"AF\")\n",
    "ax.set_xlabel(\"Heartrate (bpm)\")\n",
    "ax.set_ylabel(\"Frequency proportion\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [81], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m plot_ecg(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m3000\u001B[39m], \u001B[38;5;241m300\u001B[39m, n_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, r_peaks\u001B[38;5;241m=\u001B[39mecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr_peaks\u001B[39m\u001B[38;5;124m\"\u001B[39m], figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m2.5\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      5\u001B[0m plot_ecg_spectrogram(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m3000\u001B[39m], \u001B[38;5;241m300\u001B[39m, n_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, cut_range\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m18\u001B[39m], figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m2.5\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 6\u001B[0m \u001B[43mplot_ecg_poincare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mecg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrri_feature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m# ecg[\"rri_len\"])\u001B[39;00m\n\u001B[0;32m      7\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\Utilities\\Plotting.py:40\u001B[0m, in \u001B[0;36mplot_ecg_poincare\u001B[1;34m(rri, rri_len, figsize)\u001B[0m\n\u001B[0;32m     38\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlim((\u001B[38;5;241m0.3\u001B[39m, \u001B[38;5;241m1.5\u001B[39m))\n\u001B[0;32m     39\u001B[0m plt\u001B[38;5;241m.\u001B[39mylim((\u001B[38;5;241m0.3\u001B[39m, \u001B[38;5;241m1.5\u001B[39m))\n\u001B[1;32m---> 40\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ecgs = cinc2017_df[(cinc2017_df[\"class_index\"] == 2) & (cinc2017_df[\"heartrate\"] > 120)]\n",
    "\n",
    "for _, ecg in ecgs.iterrows():\n",
    "    plot_ecg(ecg[\"data\"][:3000], 300, n_split=1, r_peaks=ecg[\"r_peaks\"], figsize=(6, 2.5), export_quality=True)\n",
    "    plot_ecg_spectrogram(ecg[\"data\"][:3000], 300, n_split=1, cut_range=[2, 18], figsize=(6, 2.5), export_quality=True)\n",
    "    plot_ecg_poincare(ecg[\"rri_feature\"][:10], 10)# ecg[\"rri_len\"])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "mapper = CinC2020Dataset.CinC2020DiagMapper()\n",
    "num_unique_classes = len(mapper.diag_desc.index)\n",
    "\n",
    "# Note this only gets used for CinC data - the safer data labels were decided to have different meanings\n",
    "def class_index_map(diag):\n",
    "    if diag == DiagEnum.NoAF:\n",
    "        return 0\n",
    "    elif diag == DiagEnum.AF:\n",
    "        return 1\n",
    "    elif diag == DiagEnum.CannotExcludePathology:\n",
    "        return 2\n",
    "    elif diag == DiagEnum.Undecided:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cinc2017_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [39], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m cinc2017_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mcinc2017_df\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasDiag\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(class_index_map)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cinc2017_df' is not defined"
     ]
    }
   ],
   "source": [
    "cinc2017_df[\"class_index\"] = cinc2017_df[\"measDiag\"].map(class_index_map)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Onehot encoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "        self.noise_prob = 0\n",
    "        self.temp_warp = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dataset.index)\n",
    "\n",
    "    def set_noise_prob(self, prob, power_std, noise_df):\n",
    "        self.noise_prob = prob\n",
    "        self.noise_power_std = power_std\n",
    "        self.noise_df = noise_df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.dataset.iloc[index]\n",
    "\n",
    "        data = row[\"data\"]\n",
    "        rri = row[\"rri_feature\"]\n",
    "        rri_len = row[\"rri_len\"]\n",
    "\n",
    "        warp = np.random.binomial(1, self.temp_warp)\n",
    "        if warp:\n",
    "            data, r_peaks = DataAugmentations.temporal_warp(data, row[\"r_peaks_hamilton\"])\n",
    "            rri = get_rri_feature(r_peaks, 20)\n",
    "\n",
    "        add_noise = np.random.binomial(1, self.noise_prob)\n",
    "        if add_noise:\n",
    "            noise = noise_df.sample()[\"data\"].iloc[0] * np.random.normal(scale=self.noise_power_std)\n",
    "            data += noise\n",
    "\n",
    "        X = (data, rri, rri_len)\n",
    "        y = row[\"class_index\"]\n",
    "        ind = row.name\n",
    "\n",
    "        return X, y, ind"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# For SAFER data\n",
    "# Split train and test data according to each patient\n",
    "# Note this function stratifies for AF and non AF!\n",
    "def generate_patient_splits(pt_data, test_frac, val_frac):\n",
    "    train_patients = []\n",
    "    test_patients = []\n",
    "    val_patients = []\n",
    "\n",
    "    test_val_frac = test_frac + val_frac\n",
    "    val_second_frac = val_frac/test_val_frac\n",
    "\n",
    "    for val, df in pt_data.groupby(\"noAFRecs\"):\n",
    "        print(f\"processing {val}\")\n",
    "        print(f\"number of patients {len(df.index)}\")\n",
    "\n",
    "\n",
    "\n",
    "        n = math.floor(len(df.index) * test_val_frac)\n",
    "        if  test_val_frac > 0:\n",
    "            res = ((len(df.index) * test_val_frac) - n)/test_val_frac\n",
    "        else:\n",
    "            res = 0\n",
    "        n += np.random.binomial(res, test_val_frac)\n",
    "        test_val = df.sample(n)\n",
    "\n",
    "        n = math.floor(len(test_val.index) * val_second_frac)\n",
    "        if  val_second_frac > 0:\n",
    "            res = ((len(test_val.index) * val_second_frac) - n)/val_second_frac\n",
    "        else:\n",
    "            res = 0\n",
    "        n += np.random.binomial(res, val_second_frac)\n",
    "        val = test_val.sample(n)\n",
    "        val_patients.append(val)\n",
    "\n",
    "        test_patients.append(test_val[~test_val[\"ptID\"].isin(val[\"ptID\"])])\n",
    "        train_patients.append(df[~df[\"ptID\"].isin(test_val[\"ptID\"])])\n",
    "\n",
    "    train_pt_df = pd.concat(train_patients)\n",
    "    test_pt_df = pd.concat(test_patients)\n",
    "    val_pt_df = pd.concat(val_patients)\n",
    "\n",
    "    return train_pt_df, test_pt_df, val_pt_df\n",
    "\n",
    "\n",
    "def make_SAFER_dataloaders(pt_data, ecg_data, test_frac, val_frac, batch_size=128):\n",
    "    train_pt_df, test_pt_df, val_pt_df = generate_patient_splits(pt_data, test_frac, val_frac)\n",
    "\n",
    "    print(f\"Test AF: {test_pt_df['noAFRecs'].sum()} Normal: {test_pt_df['noNormalRecs'].sum()} Other: {test_pt_df['noOtherRecs'].sum()}\")\n",
    "    print(f\"Train AF: {train_pt_df['noAFRecs'].sum()} Normal: {train_pt_df['noNormalRecs'].sum()} Other: {train_pt_df['noOtherRecs'].sum()}\")\n",
    "    print(f\"Val AF: {val_pt_df['noAFRecs'].sum()} Normal: {val_pt_df['noNormalRecs'].sum()} Other: {val_pt_df['noOtherRecs'].sum()}\")\n",
    "\n",
    "    train_dataloader = None\n",
    "    test_dataloader = None\n",
    "    val_dataloader = None\n",
    "\n",
    "    train_dataset = None\n",
    "    test_dataset = None\n",
    "    val_dataset = None\n",
    "\n",
    "    if not train_pt_df.empty:\n",
    "        # get ECG datasets\n",
    "        train_dataset = ecg_data[ecg_data[\"ptID\"].isin(train_pt_df[\"ptID\"])]\n",
    "        # Normalise\n",
    "        train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_train = Dataset(train_dataset)\n",
    "        train_dataloader = DataLoader(torch_dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not test_pt_df.empty:\n",
    "        test_dataset = ecg_data[(ecg_data[\"ptID\"].isin(test_pt_df[\"ptID\"]))]\n",
    "        test_dataset[\"data\"] = (test_dataset[\"data\"] - test_dataset[\"data\"].map(lambda x: x.mean()))/test_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_test = Dataset(test_dataset)\n",
    "        test_dataloader = DataLoader(torch_dataset_test, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    if not val_pt_df.empty:\n",
    "        val_dataset = ecg_data[(ecg_data[\"ptID\"].isin(val_pt_df[\"ptID\"]))]\n",
    "        val_dataset[\"data\"] = (val_dataset[\"data\"] - val_dataset[\"data\"].map(lambda x: x.mean()))/val_dataset[\"data\"].map(lambda x: x.std())\n",
    "        torch_dataset_val = Dataset(val_dataset)\n",
    "        val_dataloader = DataLoader(torch_dataset_val, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader, train_dataset, test_dataset, val_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0.0\n",
      "number of patients 2366\n",
      "processing 1.0\n",
      "number of patients 12\n",
      "processing 2.0\n",
      "number of patients 11\n",
      "processing 3.0\n",
      "number of patients 4\n",
      "processing 4.0\n",
      "number of patients 5\n",
      "processing 5.0\n",
      "number of patients 3\n",
      "processing 6.0\n",
      "number of patients 1\n",
      "processing 8.0\n",
      "number of patients 2\n",
      "processing 9.0\n",
      "number of patients 2\n",
      "processing 10.0\n",
      "number of patients 3\n",
      "processing 11.0\n",
      "number of patients 3\n",
      "processing 18.0\n",
      "number of patients 2\n",
      "processing 19.0\n",
      "number of patients 2\n",
      "processing 22.0\n",
      "number of patients 2\n",
      "processing 26.0\n",
      "number of patients 1\n",
      "processing 29.0\n",
      "number of patients 2\n",
      "processing 35.0\n",
      "number of patients 2\n",
      "processing 39.0\n",
      "number of patients 1\n",
      "processing 45.0\n",
      "number of patients 1\n",
      "processing 53.0\n",
      "number of patients 1\n",
      "processing 62.0\n",
      "number of patients 1\n",
      "processing 80.0\n",
      "number of patients 1\n",
      "processing 94.0\n",
      "number of patients 1\n",
      "Test AF: 155.0 Normal: 24905.0 Other: 853.0\n",
      "Train AF: 498.0 Normal: 118092.0 Other: 2360.0\n",
      "Val AF: 176.0 Normal: 25902.0 Other: 713.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_83556\\1040082616.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_dataset[\"data\"] = (train_dataset[\"data\"] - train_dataset[\"data\"].map(lambda x: x.mean()))/train_dataset[\"data\"].map(lambda x: x.std())\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [70], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_dataloader_safer, test_dataloader_safer, val_dataloader_safer, train_dataset_safer, test_dataset_safer, val_dataset_safer \u001B[38;5;241m=\u001B[39m \u001B[43mmake_SAFER_dataloaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43msafer_pt_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafer_ecg_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [69], line 65\u001B[0m, in \u001B[0;36mmake_SAFER_dataloaders\u001B[1;34m(pt_data, ecg_data, test_frac, val_frac, batch_size)\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m# Normalise\u001B[39;00m\n\u001B[0;32m     64\u001B[0m     train_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (train_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m-\u001B[39m train_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mmean()))\u001B[38;5;241m/\u001B[39mtrain_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mstd())\n\u001B[1;32m---> 65\u001B[0m     torch_dataset_train \u001B[38;5;241m=\u001B[39m \u001B[43mDataset\u001B[49m(train_dataset)\n\u001B[0;32m     66\u001B[0m     train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(torch_dataset_train, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m test_pt_df\u001B[38;5;241m.\u001B[39mempty:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader_safer, test_dataloader_safer, val_dataloader_safer, train_dataset_safer, test_dataset_safer, val_dataset_safer = make_SAFER_dataloaders(safer_pt_data, safer_ecg_data, test_frac=0.15, val_frac=0.15, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, batch_size=32):\n",
    "    torch_dataset = Dataset(dataset)\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    return dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# validate on Feas2 and train/test on feas1\n",
    "val_dataset_safer = safer_ecg_data[safer_ecg_data[\"feas\"] == 2]\n",
    "val_dataloader_safer = get_dataloaders(val_dataset_safer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0    19513\n2      757\n1       16\nName: class_index, dtype: int64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_safer[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "### Make dataloaders for CinC data - separate cpsc as the validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "val_dataset = df[df[\"dataset\"] == \"cpsc_2018\"]\n",
    "train_dataset, test_dataset = train_test_split(df[df[\"dataset\"] != \"cpsc_2018\"], test_size=0.15, stratify=df[df[\"dataset\"] != \"cpsc_2018\"][\"class_index\"])\n",
    "# test_dataset, val_dataset = train_test_split(test_dataset, test_size=0.5, stratify=test_dataset[\"class_index\"])\n",
    "\n",
    "test_dataset = test_dataset[test_dataset[\"measDiag\"] != DiagEnum.Undecided]  # Should just remove any errors in loading the dataset\n",
    "val_dataset = val_dataset[val_dataset[\"measDiag\"] != DiagEnum.Undecided]  # Should just remove any errors in loading the dataset\n",
    "\n",
    "torch_dataset_test = Dataset(test_dataset)\n",
    "test_dataloader = DataLoader(torch_dataset_test, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset)\n",
    "val_dataloader = DataLoader(torch_dataset_val, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset)\n",
    "# torch_dataset_train.temp_warp = 0.2\n",
    "# torch_dataset_train.set_noise_prob(0.1, 0.2, noise_df)\n",
    "train_dataloader = DataLoader(torch_dataset_train, batch_size=128, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# Set the proportion of AF samples in the test data to that of the train data\n",
    "\n",
    "val_df_counts = val_dataset[\"class_index\"].value_counts()\n",
    "train_df_counts = train_dataset[\"class_index\"].value_counts()\n",
    "\n",
    "train_not_af = train_df_counts.loc[2] + train_df_counts.loc[0]\n",
    "val_not_af = val_df_counts.loc[2] + val_df_counts.loc[0]\n",
    "\n",
    "val_af_wanted = int(round((train_df_counts.loc[1]/train_not_af) * val_not_af))\n",
    "\n",
    "wanted_af_samples = val_dataset[val_dataset[\"class_index\"] == 1].sample(val_af_wanted)\n",
    "val_dataset = pd.concat([val_dataset[val_dataset[\"class_index\"] != 1], wanted_af_samples])\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset)\n",
    "val_dataloader = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "### CinC2017 data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "\n",
    "train_dataset_2017, test_val = train_test_split(cinc2017_df.dropna(subset=\"class_index\"), test_size=test_size + val_size, stratify=cinc2017_df[\"class_index\"].dropna())\n",
    "test_dataset_2017, val_dataset_2017 = train_test_split(test_val, test_size=val_size/(test_size + val_size), stratify=test_val[\"class_index\"])\n",
    "\n",
    "# test_dataset_2017 = test_dataset_2017[test_dataset_2017[\"measDiag\"] != DiagEnum.Undecided]  # Should just remove any errors in loading the dataset\n",
    "\n",
    "torch_dataset_test = Dataset(test_dataset_2017)\n",
    "test_dataloader_2017 = DataLoader(torch_dataset_test, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_train = Dataset(train_dataset_2017)\n",
    "train_dataloader_2017 = DataLoader(torch_dataset_train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "torch_dataset_val = Dataset(val_dataset_2017)\n",
    "val_dataloader_2017 = DataLoader(torch_dataset_val, batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2585\n",
      "2    1154\n",
      "1     353\n",
      "Name: class_index, dtype: int64\n",
      "0    554\n",
      "2    247\n",
      "1     76\n",
      "Name: class_index, dtype: int64\n",
      "0    555\n",
      "2    248\n",
      "1     75\n",
      "Name: class_index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_2017[\"class_index\"].value_counts())\n",
    "print(test_dataset_2017[\"class_index\"].value_counts())\n",
    "print(val_dataset_2017[\"class_index\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "# Save the CinC2017 data splits for consistent results!\n",
    "train_dataset_2017.to_pickle(\"TrainedModels/19_May_cinc_2017_train.pk\")\n",
    "test_dataset_2017.to_pickle(\"TrainedModels/19_May_cinc_2017_test.pk\")\n",
    "val_dataset_2017.to_pickle(\"TrainedModels/19_May_cinc_2017_val.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_dataset_2017 = pd.read_pickle(\"TrainedModels/19_May_cinc_2017_train.pk\")\n",
    "test_dataset_2017 = pd.read_pickle(\"TrainedModels/19_May_cinc_2017_test.pk\")\n",
    "val_dataset_2017 = pd.read_pickle(\"TrainedModels/19_May_cinc_2017_val.pk\")\n",
    "\n",
    "train_dataloader_2017 = get_dataloaders(train_dataset_2017, 32)\n",
    "test_dataloader_2017 = get_dataloaders(test_dataset_2017, 32)\n",
    "val_dataloader_2017 = get_dataloaders(val_dataset_2017, 32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "### Use whole CinC2017 as a test\n",
    "dataset_2017 = cinc2017_df[cinc2017_df[\"measDiag\"] != DiagEnum.Undecided].dropna(subset=\"class_index\")\n",
    "\n",
    "torch_dataset = Dataset(dataset_2017)\n",
    "dataloader_2017 = DataLoader(torch_dataset, batch_size=32, shuffle=True, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "0    3694\n2    1649\n1     504\nName: class_index, dtype: int64"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2017[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the noise detector to filter the datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "# Filter noisy things out of SAFER\n",
    "import Models.NoiseCNN\n",
    "import importlib\n",
    "\n",
    "importlib.reload(Models.NoiseCNN)\n",
    "from Models.NoiseCNN import CNN, hyperparameters\n",
    "\n",
    "noiseDetector = CNN(**hyperparameters).to(device)\n",
    "noiseDetector.load_state_dict(torch.load(\"TrainedModels/CNN_16_may_final_no_undecided.pt\", map_location=device))\n",
    "noiseDetector.eval()\n",
    "\n",
    "def add_noise_predictions(nd, dataloader, dataset):\n",
    "    noise_ps = []\n",
    "    inds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signal = signals[0].to(device).float()\n",
    "            noise_prob = nd(torch.unsqueeze(signal, 1)).detach().to(\"cpu\").numpy()\n",
    "\n",
    "            for i, n in zip(ind, noise_prob):\n",
    "                if type(i) == str:\n",
    "                    inds.append(i)\n",
    "                else:\n",
    "                    inds.append(i.item())\n",
    "                noise_ps.append(float(n))\n",
    "\n",
    "    if dataset is not None:\n",
    "        dataset[\"noise_probs\"] = pd.Series(data=noise_ps, index=inds)\n",
    "    else:\n",
    "        return pd.Series(data=noise_ps, index=inds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "add_noise_predictions(noiseDetector, val_dataloader_safer, val_dataset_safer)\n",
    "# add_noise_predictions(noiseDetector, test_dataloader_safer, test_dataset_safer)\n",
    "# add_noise_predictions(noiseDetector, train_dataloader_safer, train_dataset_safer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17861\n"
     ]
    }
   ],
   "source": [
    "# Remove the noisy samples\n",
    "# train_dataset_safer_clean = train_dataset_safer[train_dataset_safer[\"noise_probs\"] < 0]\n",
    "# test_dataset_safer_clean = test_dataset_safer[test_dataset_safer[\"noise_probs\"] < 0]\n",
    "val_dataset_safer_clean = val_dataset_safer[val_dataset_safer[\"noise_probs\"] < 0]\n",
    "\n",
    "# print(len(train_dataset_safer_clean.index))\n",
    "# print(len(test_dataset_safer_clean.index))\n",
    "print(len(val_dataset_safer_clean.index))\n",
    "\n",
    "# train_dataloader_safer_clean = get_dataloaders(train_dataset_safer_clean)\n",
    "# test_dataloader_safer_clean = get_dataloaders(test_dataset_safer_clean)\n",
    "val_dataloader_safer_clean = get_dataloaders(val_dataset_safer_clean)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "add_noise_predictions(noiseDetector, train_dataloader_2017, train_dataset_2017)\n",
    "add_noise_predictions(noiseDetector, test_dataloader_2017, test_dataset_2017)\n",
    "add_noise_predictions(noiseDetector, val_dataloader_2017, val_dataset_2017)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2585\n",
      "2    1154\n",
      "1     353\n",
      "Name: class_index, dtype: int64\n",
      "0    554\n",
      "2    247\n",
      "1     76\n",
      "Name: class_index, dtype: int64\n",
      "0    555\n",
      "2    248\n",
      "1     75\n",
      "Name: class_index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_2017[\"class_index\"].value_counts())\n",
    "print(test_dataset_2017[\"class_index\"].value_counts())\n",
    "print(val_dataset_2017[\"class_index\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1955\n",
      "2     857\n",
      "1     255\n",
      "Name: class_index, dtype: int64\n",
      "0    405\n",
      "2    172\n",
      "1     50\n",
      "Name: class_index, dtype: int64\n",
      "0    392\n",
      "2    178\n",
      "1     50\n",
      "Name: class_index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove the noisy samples\n",
    "thresh = 0.3\n",
    "\n",
    "train_dataset_2017_clean = train_dataset_2017[train_dataset_2017[\"noise_probs\"] < 0]\n",
    "test_dataset_2017_clean = test_dataset_2017[test_dataset_2017[\"noise_probs\"] < 0]\n",
    "val_dataset_2017_clean = val_dataset_2017[val_dataset_2017[\"noise_probs\"] < 0]\n",
    "\n",
    "print(train_dataset_2017_clean[\"class_index\"].value_counts())\n",
    "print(test_dataset_2017_clean[\"class_index\"].value_counts())\n",
    "print(val_dataset_2017_clean[\"class_index\"].value_counts())\n",
    "\n",
    "\n",
    "train_dataloader_2017_clean = get_dataloaders(train_dataset_2017_clean)\n",
    "test_dataloader_2017_clean = get_dataloaders(test_dataset_2017_clean)\n",
    "val_dataset_2017_clean = val_dataset_2017[val_dataset_2017[\"noise_probs\"] < 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class DatasetSequenceIterator:\n",
    "\n",
    "    def __init__(self, data_loading_functions, batch_sizes, filter=lambda x:x):\n",
    "        self.dl_functions = data_loading_functions\n",
    "\n",
    "        self.dataset = None\n",
    "        self.next_dataset = None\n",
    "\n",
    "        self.dataloader_iterator = None\n",
    "        self.next_dataloader_iterator = None\n",
    "\n",
    "        self.next_dataset_loaded = False\n",
    "        self.dataloader_thread = None\n",
    "\n",
    "        self.filter = filter\n",
    "\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.dl_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.dl_index = -1\n",
    "        self.dataloader_thread = threading.Thread(target=self.load_next_dataset)\n",
    "        self.dataloader_thread.start()\n",
    "        self.dataloader_thread.join()\n",
    "        self.swap_to_next_dataset()\n",
    "        self.dl_index += 1\n",
    "        self.dataloader_thread = threading.Thread(target=self.load_next_dataset)\n",
    "        self.dataloader_thread.start()\n",
    "        print(self.dl_index)\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO make this return the right value\n",
    "        return 100\n",
    "\n",
    "    def swap_to_next_dataset(self):\n",
    "        self.dataset = self.next_dataset\n",
    "        self.dataloader_iterator = self.next_dataloader_iterator\n",
    "        self.next_dataset_loaded = False\n",
    "\n",
    "    def load_next_dataset(self):\n",
    "        if self.dl_index + 1 < len(self.dl_functions):\n",
    "            print(f\"Loading dataset {self.dl_index + 1}\")\n",
    "            self.next_dataset = self.dl_functions[self.dl_index + 1]()\n",
    "            self.next_dataset = self.filter(self.next_dataset)\n",
    "\n",
    "            torch_dataset = Dataset(self.next_dataset)\n",
    "            self.next_dataloader_iterator = iter(DataLoader(torch_dataset, batch_size=self.batch_sizes[self.dl_index], shuffle=True, pin_memory=True))\n",
    "            self.next_dataset_loaded = True\n",
    "        else:\n",
    "            print(\"Finished loading all datasets\")\n",
    "            self.next_dataset_loaded = False\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            ret = next(self.dataloader_iterator)\n",
    "        except StopIteration:\n",
    "            print(\"stop_iteration\")\n",
    "            if self.dl_index >= len(self.dl_functions):\n",
    "                # We have gone through all the datasets\n",
    "                print(\"Completed all datasets\")\n",
    "                raise StopIteration\n",
    "            else:\n",
    "\n",
    "                if not self.next_dataset_loaded:\n",
    "                    print(\"waiting_for_next_dataset\")\n",
    "                    self.dataloader_thread.join()\n",
    "\n",
    "                self.swap_to_next_dataset()\n",
    "                self.dl_index += 1\n",
    "                self.dataloader_thread = threading.Thread(target=self.load_next_dataset)\n",
    "                self.dataloader_thread.start()\n",
    "                ret = next(self.dataloader_iterator)\n",
    "\n",
    "        return ret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# Testing the DatasetSequenceIterator by dividing feas1 into two parts\n",
    "\n",
    "def load_feas1_first_half():\n",
    "    ecg_data, pt_data = load_feas1_chunk_range((0, 1))\n",
    "    return prepare_safer_data(pt_data, ecg_data)[1]\n",
    "\n",
    "def load_feas1_second_half():\n",
    "    ecg_data, pt_data = load_feas1_chunk_range((4, 5))\n",
    "    return prepare_safer_data(pt_data, ecg_data)[1]\n",
    "\n",
    "def load_feas1_nth_chuck(n):\n",
    "    ecg_data, pt_data = load_feas1_chunk_range((n, n+1))\n",
    "    return prepare_safer_data(pt_data, ecg_data)[1]\n",
    "\n",
    "loading_functions = [lambda n=n: load_feas1_nth_chuck(n) for n in range(num_chunks)]\n",
    "\n",
    "feas1_dataloader_entire = DatasetSequenceIterator(loading_functions, [128 for n in range(num_chunks)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_ecgs = 0\n",
    "for i, (signals, labels, _) in enumerate(feas1_dataloader):\n",
    "    signal = signals[0].to(device).float()\n",
    "    rris = signals[1].to(device).float()\n",
    "    rri_len = signals[2].to(device).float()\n",
    "\n",
    "    num_ecgs += signal.shape[0]\n",
    "\n",
    "print(num_ecgs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feas1_noise_predictions = add_noise_predictions(noiseDetector, feas1_dataloader_entire, None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"number of noisy ECGs: {(feas1_noise_predictions > 0).sum()}\")\n",
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "feas1_noise_predictions.to_pickle(os.path.join(feas1_path, \"ECGs/feas1_noise_predictions.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "feas1_noise_predictions = pd.read_pickle(os.path.join(feas1_path, \"ECGs/feas1_noise_predictions.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "pt_data = SAFERDataset.load_pt_dataset(1)\n",
    "ecg_data = SAFERDataset.load_ecg_csv(1, pt_data, ecg_range=None, ecg_meas_diag=None, feas2_offset=10000, feas2_ecg_offset=200000)\n",
    "\n",
    "ecg_data[\"feas\"] = 1\n",
    "ecg_data[\"length\"] = 9120\n",
    "ecg_data[\"rri_len\"] = 20\n",
    "\n",
    "pt_data, ecg_data = prepare_safer_data(pt_data, ecg_data)\n",
    "\n",
    "# train_pts, test_pts, val_pts = generate_patient_splits(pt_data, 0.15, 0.15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[18614   496     0]\n",
      " [   21    77     0]\n",
      " [  260   206     0]]\n",
      "Sensitivity: 0.786\n",
      "Specificity: 0.964\n",
      "\n",
      "Normal F1: 0.980\n",
      "AF F1: 0.176\n",
      "Other F1: 0.000\n"
     ]
    }
   ],
   "source": [
    "zenicor_conf_mat = confusion_matrix(feas1_ecg_data_test[\"class_index\"], feas1_ecg_data_test[\"poss_AF_tag\"])\n",
    "print_results(zenicor_conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    125865\n",
      "2      2554\n",
      "1       529\n",
      "Name: class_index, dtype: int64\n",
      "0    149586\n",
      "2      3120\n",
      "1       745\n",
      "Name: class_index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ecg_data[\"noise_prediction\"] = feas1_noise_predictions\n",
    "print(ecg_data[ecg_data[\"noise_prediction\"] < 0][\"class_index\"].value_counts())\n",
    "print(ecg_data[\"class_index\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104364.0\n",
      "515.0\n",
      "2282.0\n",
      "\n",
      "22294.0\n",
      "102.0\n",
      "377.0\n",
      "\n",
      "22928.0\n",
      "128.0\n",
      "461.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pts in [train_pts, test_pts, val_pts]:\n",
    "    print(pts[\"noNormalRecs\"].sum())\n",
    "    print(pts[\"noAFRecs\"].sum())\n",
    "    print(pts[\"noOtherRecs\"].sum())\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "train_pts.to_pickle(os.path.join(feas1_path, \"all_feas1_train_pts.pk\"))\n",
    "test_pts.to_pickle(os.path.join(feas1_path, \"all_feas1_test_pts.pk\"))\n",
    "val_pts.to_pickle(os.path.join(feas1_path, \"all_feas1_val_pts.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "train_pts = pd.read_pickle(os.path.join(feas1_path, \"ECGs/all_feas1_train_pts.pk\"))\n",
    "test_pts = pd.read_pickle(os.path.join(feas1_path, \"ECGs/all_feas1_test_pts.pk\"))\n",
    "val_pts = pd.read_pickle(os.path.join(feas1_path, \"ECGs/all_feas1_val_pts.pk\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "feas1_ecg_data_test = pd.read_pickle(os.path.join(feas1_path, \"ECGs/feas1_test_26_mar.pk\"))\n",
    "feas1_ecg_data_test = feas1_ecg_data_test[feas1_ecg_data_test[\"rri_len\"] > 5]\n",
    "\n",
    "feas1_ecg_data_val = pd.read_pickle(os.path.join(feas1_path, \"ECGs/feas1_val_26_mar.pk\"))\n",
    "feas1_ecg_data_val = feas1_ecg_data_val[feas1_ecg_data_val[\"rri_len\"] > 5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_20808\\3520699426.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  feas1_ecg_data_test[\"noise_prediction\"] = feas1_noise_predictions\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_20808\\3520699426.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  feas1_ecg_data_val[\"noise_prediction\"] = feas1_noise_predictions\n"
     ]
    }
   ],
   "source": [
    "feas1_ecg_data_test[\"noise_prediction\"] = feas1_noise_predictions\n",
    "feas1_ecg_data_test_clean = feas1_ecg_data_test[feas1_ecg_data_test[\"noise_prediction\"] < 0]\n",
    "\n",
    "feas1_ecg_data_val[\"noise_prediction\"] = feas1_noise_predictions\n",
    "feas1_ecg_data_val_clean = feas1_ecg_data_val[feas1_ecg_data_val[\"noise_prediction\"] < 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "0    18534\n2      364\n1       80\nName: class_index, dtype: int64"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feas1_ecg_data_val_clean[\"class_index\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexingError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [179], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mecg_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeas1_noise_predictions\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\frame.py:3796\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3794\u001B[0m \u001B[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001B[39;00m\n\u001B[0;32m   3795\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_bool_indexer(key):\n\u001B[1;32m-> 3796\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_bool_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001B[39;00m\n\u001B[0;32m   3799\u001B[0m \u001B[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m is_single_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_list_like(key)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\frame.py:3849\u001B[0m, in \u001B[0;36mDataFrame._getitem_bool_array\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3843\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3844\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mItem wrong length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(key)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3845\u001B[0m     )\n\u001B[0;32m   3847\u001B[0m \u001B[38;5;66;03m# check_bool_indexer will throw exception if Series key cannot\u001B[39;00m\n\u001B[0;32m   3848\u001B[0m \u001B[38;5;66;03m# be reindexed to match DataFrame rows\u001B[39;00m\n\u001B[1;32m-> 3849\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_bool_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3850\u001B[0m indexer \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   3851\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_take_with_is_copy(indexer, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\pandas\\core\\indexing.py:2548\u001B[0m, in \u001B[0;36mcheck_bool_indexer\u001B[1;34m(index, key)\u001B[0m\n\u001B[0;32m   2546\u001B[0m indexer \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mget_indexer_for(index)\n\u001B[0;32m   2547\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01min\u001B[39;00m indexer:\n\u001B[1;32m-> 2548\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m IndexingError(\n\u001B[0;32m   2549\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnalignable boolean Series provided as \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2550\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexer (index of the boolean Series and of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2551\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe indexed object do not match).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2552\u001B[0m     )\n\u001B[0;32m   2554\u001B[0m result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   2556\u001B[0m \u001B[38;5;66;03m# fall through for boolean\u001B[39;00m\n",
      "\u001B[1;31mIndexingError\u001B[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "ecg_data[(feas1_noise_predictions[ecg_data.index] < 0)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# Create some a filter function to select data from each partition\n",
    "def filter_train_pts(ecg_data):\n",
    "    print(f\"filtering {(feas1_noise_predictions[ecg_data.index] > 0).sum()} ECGs out\")\n",
    "    return ecg_data[(ecg_data[\"ptID\"].isin(train_pts[\"ptID\"])) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "def filter_test_pts(ecg_data):\n",
    "    return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "def filter_val_pts(ecg_data):\n",
    "    return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "feas1_train_dataloader = DatasetSequenceIterator(loading_functions, [64 for n in range(num_chunks)], filter=filter_train_pts)\n",
    "feas1_test_dataloader = get_dataloaders(feas1_ecg_data_test)\n",
    "feas1_val_dataloader = get_dataloaders(feas1_ecg_data_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feas1_ecg_data_test_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [33], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m feas1_test_dataloader_clean \u001B[38;5;241m=\u001B[39m get_dataloaders(\u001B[43mfeas1_ecg_data_test_clean\u001B[49m)\n\u001B[0;32m      2\u001B[0m feas1_val_dataloader_clean \u001B[38;5;241m=\u001B[39m get_dataloaders(feas1_ecg_data_val_clean)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'feas1_ecg_data_test_clean' is not defined"
     ]
    }
   ],
   "source": [
    "feas1_test_dataloader_clean = get_dataloaders(feas1_ecg_data_test_clean)\n",
    "feas1_val_dataloader_clean = get_dataloaders(feas1_ecg_data_val_clean)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    19247\n",
      "2      325\n",
      "1       60\n",
      "Name: class_index, dtype: int64\n",
      "0    19151\n",
      "2      391\n",
      "1       90\n",
      "Name: class_index, dtype: int64\n",
      "  \n",
      "0    22274\n",
      "2      377\n",
      "1      102\n",
      "Name: class_index, dtype: int64\n",
      "0    22862\n",
      "2      452\n",
      "1      126\n",
      "Name: class_index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(feas1_ecg_data_test_clean[\"class_index\"].value_counts())\n",
    "print(feas1_ecg_data_val_clean[\"class_index\"].value_counts())\n",
    "\n",
    "print(\"  \")\n",
    "\n",
    "print(feas1_ecg_data_test[\"class_index\"].value_counts())\n",
    "print(feas1_ecg_data_val[\"class_index\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare for training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "5522"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del feas1_train_dataloader\n",
    "del feas1_test_dataloader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [],
   "source": [
    "import Models.SpectrogramTransformer\n",
    "importlib.reload(Models.SpectrogramTransformer)\n",
    "# from Models.SpectrogramTransformer import TransformerModel\n",
    "import Models.SpectrogramTransformerAttentionPooling\n",
    "importlib.reload(Models.SpectrogramTransformerAttentionPooling)\n",
    "from Models.SpectrogramTransformerAttentionPooling import TransformerModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR, LambdaLR, SequentialLR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 18)\n"
     ]
    }
   ],
   "source": [
    "n_head = 4\n",
    "n_fft = 128\n",
    "embed_dim = 128 # int(n_fft/2)\n",
    "n_inp_rri = 64\n",
    "\n",
    "model = TransformerModel(3, embed_dim, n_head, 512, 6, n_fft, n_inp_rri, device=device).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "class focal_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, weights, gamma=2, label_smoothing=0):\n",
    "        super(focal_loss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction=\"none\", label_smoothing=label_smoothing)\n",
    "        self.weights = weights\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "        ce = self.ce_loss(pred, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "\n",
    "        loss_sum = torch.sum(((1-pt) ** self.gamma) * ce * self.weights[targets])\n",
    "        norm_factor = torch.sum(self.weights[targets])\n",
    "        return loss_sum/norm_factor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_counts = torch.tensor(train_dataset[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "loss_func = focal_loss(class_weights, 2) # nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1) # focal_loss(class_weights, 2, 0.05) #\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "number_warmup_batches = 600\n",
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "# scheduler = SequentialLR(optimizer, [warmup_scheduler, scheduler], [number_warmup_epochs])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0035, 0.8255, 0.1710])\n"
     ]
    }
   ],
   "source": [
    "# Remake scheduler before retraining on SAFER\n",
    "\n",
    "\"\"\"\n",
    "class_counts = torch.tensor(train_dataset_safer_clean[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# just approximate weights using feas2 rather than computing for feas 1 - these might be fundamentally different because in feas2 the cardiologist stopped labelling after the first AF from a patient therefore fewer AF.\n",
    "class_counts = torch.tensor(val_dataset_safer[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Use all of feas1 to compute the class counts - precomputed values for next time: tensor([0.0043, 0.7924, 0.2033])\n",
    "class_counts = torch.tensor(feas1_ecg_data[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "\"\"\"\n",
    "\n",
    "class_counts = torch.tensor(ecg_data[ecg_data[\"noise_prediction\"] < 0][\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "\n",
    "# class_weights = torch.tensor([0.0043, 0.7924, 0.2033])\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "loss_func = focal_loss(class_weights, gamma=2, label_smoothing=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "number_warmup_batches = 600\n",
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0947, 0.6933, 0.2121])\n"
     ]
    }
   ],
   "source": [
    "# Remake scheduler before retraining on CinC2017\n",
    "\n",
    "class_counts = torch.tensor(train_dataset_2017[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weight=class_weights) # multiclass_cross_entropy_loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00004)\n",
    "\n",
    "number_warmup_batches = 600\n",
    "def warmup(current_step: int):\n",
    "    if current_step < number_warmup_batches:\n",
    "        # print(current_step / number_warmup_batches ** 1.5)\n",
    "        return current_step / number_warmup_batches ** 1.5\n",
    "    else:\n",
    "        # print(1/math.sqrt(current_step))\n",
    "        return 1/math.sqrt(current_step)  # 1 / (10 ** (float(number_warmup_epochs - current_step)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1427, 0.7559, 0.1014])\n"
     ]
    }
   ],
   "source": [
    "# Train the model I stole\n",
    "\n",
    "import OtherModels.Prna.physionet2020_submission.model\n",
    "importlib.reload(OtherModels.Prna.physionet2020_submission.model)\n",
    "from OtherModels.Prna.physionet2020_submission.model import CTN\n",
    "import OtherModels.Prna.physionet2020_submission.optimizer\n",
    "importlib.reload(OtherModels.Prna.physionet2020_submission.optimizer)\n",
    "from OtherModels.Prna.physionet2020_submission.optimizer import NoamOpt\n",
    "\n",
    "# Train prna's transformer\n",
    "n_head = 8\n",
    "n_fft = 128\n",
    "embed_dim = 128 # int(n_fft/2)\n",
    "n_inp_rri = 64\n",
    "\n",
    "class_counts = torch.tensor(train_dataset[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "class_weights = (1/class_counts)\n",
    "class_weights /= torch.sum(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "model = CTN(256, n_head, 2048, 4, 0.1, 64, 0, 0, 3).to(device)\n",
    "\n",
    "# Initialize parameters with Glorot / fan_avg.\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# optimizer = NoamOpt(256, 1, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "number_warmup_batches = 2\n",
    "def warmup(current_step: int):\n",
    "    return 1 / (10 ** (float(number_warmup_batches - current_step)))\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "scheduler = SequentialLR(optimizer, [warmup_scheduler, scheduler], [number_warmup_batches])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "from torch.profiler import profile, tensorboard_trace_handler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "model = model.to(device)\n",
    "model.fix_transformer_params(fix_spec=False, fix_rri=False)\n",
    "num_epochs = 40\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader):\n",
    "    best_test_loss = 100\n",
    "    best_epoch = -1\n",
    "    best_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"starting epoch {epoch} ...\")\n",
    "        # Train\n",
    "        num_batches = 0\n",
    "        model.train()\n",
    "        for i, (signals, labels, _) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            signal = signals[0].to(device).float()\n",
    "            rris = signals[1].to(device).float()\n",
    "            rri_len = signals[2].to(device).float()\n",
    "\n",
    "            if torch.any(torch.isnan(signal)):\n",
    "                print(\"Signals are nan\")\n",
    "                continue\n",
    "\n",
    "            if torch.any(torch.isnan(rris)):\n",
    "                print(\"Signals are nan\")\n",
    "                continue\n",
    "\n",
    "            labels = labels.long()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(signal, rris, rri_len).to(\"cpu\")\n",
    "\n",
    "            if torch.any(torch.isnan(output)):\n",
    "                print(signal)\n",
    "                print(rris)\n",
    "                print(rri_len)\n",
    "                print(output)\n",
    "                raise ValueError\n",
    "\n",
    "            loss = loss_func(output, labels)\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            num_batches += 1\n",
    "            total_loss += float(loss)\n",
    "\n",
    "        print(num_batches)\n",
    "\n",
    "        print(f\"Epoch {epoch} finished with average loss {total_loss/num_batches}\")\n",
    "        # writer.add_scalar(\"Loss/train\", total_loss/num_batches, epoch)\n",
    "        print(\"Testing ...\")\n",
    "        # Test\n",
    "        num_test_batches = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, (signals, labels, _) in enumerate(test_dataloader):\n",
    "                signal = signals[0].to(device).float()\n",
    "                rris = signals[1].to(device).float()\n",
    "                rri_len = signals[2].to(device).float()\n",
    "\n",
    "                if torch.any(torch.isnan(signal)):\n",
    "                    print(\"Signals are nan\")\n",
    "                    continue\n",
    "\n",
    "                labels = labels.long()\n",
    "                output = model(signal, rris, rri_len).to(\"cpu\")\n",
    "                loss = loss_func(output, labels)\n",
    "                test_loss += float(loss)\n",
    "                num_test_batches += 1\n",
    "\n",
    "        print(f\"Average test loss: {test_loss/num_test_batches}\")\n",
    "        losses.append([total_loss/num_batches, test_loss/num_test_batches])\n",
    "        # writer.add_scalar(\"Loss/test\", test_loss/num_t est_batches, epoch)\n",
    "\n",
    "        if test_loss/num_test_batches < best_test_loss:\n",
    "            best_model = copy.deepcopy(model).cpu()\n",
    "            best_test_loss = test_loss/num_test_batches\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            if best_epoch + 5 <= epoch:\n",
    "                return best_model, losses\n",
    "\n",
    "    return best_model, losses\n",
    "\n",
    "model, losses = train(model, val_dataloader_safer_clean, val_dataset_safer_clean)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "losses = np.load(\"TrainedModels/Transformer_23_May_cinc_train_attention_pooling_no_augmentation_smoothing_training_curve.npy\")\n",
    "\n",
    "# \"C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\TrainedModels\\Transformer_23_May_feas1_train_attention_pooling_augmentation_smoothing_retrain.npy\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "# plot the training curve (1 axis only)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "train_l = ax.plot([l[0] for l in losses], label=\"training loss\")\n",
    "val_l = ax.plot([l[1] for l in losses], label=\"validation loss\", color=\"#ff7f0e\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "losses_np = np.array(losses)\n",
    "np.save(\"TrainedModels/Transformer_24_May_cinc_2017_train_attention_pooling_augmentation_smoothing\", losses_np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "# Save a model\n",
    "torch.save(model.state_dict(), \"TrainedModels/Transformer_24_May_cinc_2017_train_attention_pooling_augmentation_smoothing.pt\")\n",
    "\n",
    "# train_dataset_safer.to_pickle(\"TrainedModels/Transformer_15_Mar_train.pk\")\n",
    "# test_dataset_safer.to_pickle(\"TrainedModels/Transformer_15_Mar_test.pk\")\n",
    "# val_dataset_safer.to_pickle(\"TrainedModels/Transformer_15_Mar_val.pk\")\n",
    "# train_pt_df.to_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_train.pk\")\n",
    "# val_pt_df.to_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_val.pk\")\n",
    "# test_pt_df.to_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_test.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "train_dataset.to_pickle(\"TrainedModels/Transformer_13_May_cinc_trained_initial_train.pk\")\n",
    "test_dataset.to_pickle(\"TrainedModels/Transformer_13_May_cinc_trained_initial_test.pk\")\n",
    "val_dataset.to_pickle(\"TrainedModels/Transformer_13_May_cinc_trained_initial_val.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_dataset_safer = pd.read_pickle(\"TrainedModels/Transformer_13_Mar_train.pk\")\n",
    "test_dataset_safer = pd.read_pickle(\"TrainedModels/Transformer_13_Mar_test.pk\")\n",
    "val_dataset_safer = pd.read_pickle(\"TrainedModels/Transformer_13_Mar_val.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_dataloader_safer = get_dataloaders(train_dataset_safer)\n",
    "test_dataloader_safer = get_dataloaders(test_dataset_safer)\n",
    "val_dataloader_safer = get_dataloaders(val_dataset_safer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "train_dataset_2017.to_pickle(\"TrainedModels/Transformer_13_May_cinc_2017_trained_train.pk\")\n",
    "test_dataset_2017.to_pickle(\"TrainedModels/Transformer_13_May_cinc_2017_trained_test.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Set this for safer cross validation later\n",
    "cinc_model_path = \"TrainedModels/Transformer_15_Mar_cinc_trained_noise_augmentation.pt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model\n",
    "# model = TransformerModel(2, embed_dim, n_head, 1024, 4, 47, n_fft).to(device)\n",
    "model.load_state_dict(torch.load(\"TrainedModels/Transformer_20_May_cinc_train_attention_pooling_augmentation_smoothing.pt\", map_location=device))\n",
    "\n",
    "# train_pt_df = pd.read_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_train.pk\")\n",
    "# val_pt_df = pd.read_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_val.pk\")\n",
    "# test_pt_df = pd.read_pickle(\"TrainedModels/Transformer_spectrogram_small_fft_cut_all_safer_trained_average_warped_test.pk\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_pt_df = pd.read_pickle(\"TrainedModels/Transformer_13_May_cinc_2017_trained_train.pk\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tonights training schedule\n",
    "\n",
    "print(\"Stage 0: Training final model with noisy samples removed\")\n",
    "try:\n",
    "    raise Exception\n",
    "    feas1_noise_predictions = add_noise_predictions(noiseDetector, feas1_dataloader_entire, None)\n",
    "\n",
    "    print(f\"number of noisy ECGs: {(feas1_noise_predictions > 0).sum()}\")\n",
    "    feas1_path = r\"D:\\2022_23_DSiromani\\Feas1\"\n",
    "    feas1_noise_predictions.to_pickle(os.path.join(feas1_path, \"ECGs/feas1_noise_predictions.pk\"))\n",
    "\n",
    "    ecg_data[\"noise_prediction\"] = feas1_noise_predictions\n",
    "    print(ecg_data[ecg_data[\"noise_prediction\"] < 0][\"class_index\"].value_counts())\n",
    "    print(ecg_data[\"class_index\"].value_counts())\n",
    "\n",
    "    feas1_ecg_data_test[\"noise_prediction\"] = feas1_noise_predictions\n",
    "    feas1_ecg_data_test_clean = feas1_ecg_data_test[feas1_ecg_data_test[\"noise_prediction\"] < 0]\n",
    "\n",
    "    feas1_ecg_data_val[\"noise_prediction\"] = feas1_noise_predictions\n",
    "    feas1_ecg_data_val_clean = feas1_ecg_data_val[feas1_ecg_data_val[\"noise_prediction\"] < 0]\n",
    "\n",
    "    # Create some a filter function to select data from each partition\n",
    "    def filter_train_pts(ecg_data):\n",
    "        print(f\"filtering {(feas1_noise_predictions[ecg_data.index] > 0).sum()} ECGs out\")\n",
    "        return ecg_data[(ecg_data[\"ptID\"].isin(train_pts[\"ptID\"])) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "    def filter_test_pts(ecg_data):\n",
    "        return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "    def filter_val_pts(ecg_data):\n",
    "        return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"]) & (feas1_noise_predictions[ecg_data.index] < 0)]\n",
    "\n",
    "    feas1_train_dataloader = DatasetSequenceIterator(loading_functions, [64 for n in range(num_chunks)], filter=filter_train_pts)\n",
    "    feas1_test_dataloader = get_dataloaders(feas1_ecg_data_test)\n",
    "    feas1_val_dataloader = get_dataloaders(feas1_ecg_data_val)\n",
    "\n",
    "    feas1_test_dataloader_clean = get_dataloaders(feas1_ecg_data_test_clean)\n",
    "    feas1_val_dataloader_clean = get_dataloaders(feas1_ecg_data_val_clean)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error occured in stage 0\")\n",
    "    print(e)\n",
    "\n",
    "print(\"Stage 1: Training final model with noisy samples removed\")\n",
    "try:\n",
    "    raise Exception\n",
    "    model.load_state_dict(torch.load(\"TrainedModels/Transformer_20_May_cinc_train_attention_pooling_augmentation_smoothing.pt\", map_location=device))\n",
    "    model, losses = train(model, feas1_train_dataloader, feas1_test_dataloader_clean)\n",
    "\n",
    "    losses_np = np.array(losses)\n",
    "    np.save(\"TrainedModels/Transformer_27_May_feas1_train_attention_pooling_augmentation_smoothing_training_curve_no_noisy_nk_beats\", losses_np)\n",
    "\n",
    "    torch.save(model.state_dict(), \"TrainedModels/Transformer_27_May_feas1_train_attention_pooling_augmentation_smoothing_no_noisy_nk_beats.pt\")\n",
    "except Exception as e:\n",
    "    print(\"Error occured in stage 1\")\n",
    "    print(e)\n",
    "\n",
    "print(\"Stage 2: Training final model with noisy samples included\")\n",
    "try:\n",
    "    import Models.SpectrogramTransformerAttentionPooling\n",
    "    importlib.reload(Models.SpectrogramTransformerAttentionPooling)\n",
    "    from Models.SpectrogramTransformerAttentionPooling import TransformerModel\n",
    "\n",
    "    model = TransformerModel(3, embed_dim, n_head, 512, 6, n_fft, n_inp_rri, device=device).to(device)\n",
    "    model.load_state_dict(torch.load(\"TrainedModels/Transformer_20_May_cinc_train_attention_pooling_augmentation_smoothing.pt\", map_location=device))\n",
    "\n",
    "    # recreate the filters without removing noise\n",
    "    def filter_train_pts(ecg_data):\n",
    "        print(f\"filtering {(feas1_noise_predictions[ecg_data.index] > 0).sum()} ECGs out\")\n",
    "        return ecg_data[(ecg_data[\"ptID\"].isin(train_pts[\"ptID\"]))]\n",
    "\n",
    "    def filter_test_pts(ecg_data):\n",
    "        return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"])]\n",
    "\n",
    "    def filter_val_pts(ecg_data):\n",
    "        return ecg_data[ecg_data[\"ptID\"].isin(test_pts[\"ptID\"])]\n",
    "\n",
    "    feas1_train_dataloader = DatasetSequenceIterator(loading_functions, [64 for n in range(num_chunks)], filter=filter_train_pts)\n",
    "\n",
    "    class_counts = torch.tensor(ecg_data[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "    class_weights = (1/class_counts)\n",
    "    class_weights /= torch.sum(class_weights)\n",
    "\n",
    "    loss_func = focal_loss(class_weights, gamma=2, label_smoothing=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "    number_warmup_batches = 600\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "    model, losses = train(model, feas1_train_dataloader, feas1_test_dataloader)\n",
    "\n",
    "    losses_np = np.array(losses)\n",
    "    np.save(\"TrainedModels/Transformer_27_May_feas1_train_attention_pooling_augmentation_smoothing_training_curve_nk_beats_retrain\", losses_np)\n",
    "\n",
    "    torch.save(model.state_dict(), \"TrainedModels/Transformer_27_May_feas1_train_attention_pooling_augmentation_smoothing_nk_beats_retrain.pt\")\n",
    "except Exception as e:\n",
    "    print(\"Error occured in stage 2\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Stage 3: Training the initial model with noisy samples included\")\n",
    "try:\n",
    "    import Models.SpectrogramTransformer\n",
    "    importlib.reload(Models.SpectrogramTransformer)\n",
    "    from Models.SpectrogramTransformer import TransformerModel\n",
    "\n",
    "    model = TransformerModel(3, embed_dim, n_head, 512, 6, n_fft, n_inp_rri, device=device, enable_rri=False).to(device)\n",
    "    model.load_state_dict(torch.load(\"TrainedModels/Transformer_12_May_cinc_trained_initial.pt\", map_location=device))\n",
    "\n",
    "    class_counts = torch.tensor(ecg_data[\"class_index\"].value_counts().sort_index().values.astype(np.float32))\n",
    "    class_weights = (1/class_counts)\n",
    "    class_weights /= torch.sum(class_weights)\n",
    "\n",
    "    loss_func = focal_loss(class_weights, gamma=2, label_smoothing=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "    number_warmup_batches = 600\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "    model, losses = train(model, feas1_train_dataloader, feas1_test_dataloader)\n",
    "\n",
    "    losses_np = np.array(losses)\n",
    "    np.save(\"TrainedModels/Transformer_27_May_feas1_train_initial_training_curve_nk_beats\", losses_np)\n",
    "\n",
    "    torch.save(model.state_dict(), \"TrainedModels/Transformer_27_May_feas1_train_initial_nk_beats.pt\")\n",
    "except Exception as e:\n",
    "    print(\"Error occured in stage 3\")\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "def get_predictions(model, dataloader, dataset):\n",
    "\n",
    "    attentions = []\n",
    "\n",
    "    \"\"\"\n",
    "    def hook(module, x, y):\n",
    "        for a in y[1]:\n",
    "            attentions.append(a.detach().cpu().numpy())\n",
    "\n",
    "    attention_hook = model.attention_pooling.attn.register_forward_hook(hook)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    outputs = []\n",
    "    inds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signal = signals[0].to(device).float()\n",
    "            rris = signals[1].to(device).float()\n",
    "            rri_len = signals[2].to(device).float()\n",
    "\n",
    "            labels = labels.long().detach().numpy()\n",
    "            true_labels.append(labels)\n",
    "\n",
    "            output = model(signal, rris, rri_len).detach().to(\"cpu\").numpy() # rris).detach().to(\"cpu\").numpy()\n",
    "\n",
    "            prediction = output # np.argmax(output, axis=-1)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            for i, o in zip(ind, output):\n",
    "                outputs.append(o)\n",
    "                if isinstance(i, str):\n",
    "                    inds.append(i)\n",
    "                else:\n",
    "                    inds.append(i.item())\n",
    "\n",
    "    dataset[\"prediction\"] = pd.Series(data=outputs, index=inds)\n",
    "    # dataset[\"attention\"] = pd.Series(data=attentions, index=inds)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    true_labels = np.concatenate(true_labels)\n",
    "\n",
    "    # attention_hook.remove()\n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "predictions, true_labels = get_predictions(model, feas1_val_dataloader, feas1_ecg_data_val)\n",
    "conf_mat = confusion_matrix(true_labels, np.argmax(predictions, axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "feas1_ecg_data_test[\"noise_probs\"] = feas1_ecg_data_test[\"noise_prediction\"]\n",
    "feas1_ecg_data_val[\"noise_probs\"] = feas1_ecg_data_val[\"noise_prediction\"]\n",
    "\n",
    "def get_noise_free_conf_mat(dataset):\n",
    "   return confusion_matrix(dataset[dataset[\"noise_probs\"] < 0][\"class_index\"], dataset[dataset[\"noise_probs\"] < 0][\"prediction\"].map(np.argmax))\n",
    "\n",
    "noise_free_conf_mat = get_noise_free_conf_mat(feas1_ecg_data_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[10169   298 12395]\n",
      " [   65    61     0]\n",
      " [  357    56    39]]\n",
      "Sensitivity: 0.484\n",
      "Specificity: 0.985\n",
      "\n",
      "Normal F1: 0.608\n",
      "AF F1: 0.226\n",
      "Other F1: 0.006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def F1_ind(conf_mat, ind):\n",
    "    return (2 * conf_mat[ind, ind])/(np.sum(conf_mat[ind]) + np.sum(conf_mat[:, ind]))\n",
    "\n",
    "def print_results(conf_mat):\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(conf_mat)\n",
    "\n",
    "    print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1]):0.3f}\")\n",
    "    print(f\"Specificity: {(conf_mat[0, 0] + conf_mat[0, 2] + conf_mat[2, 0] + conf_mat[2, 2])/(np.sum(conf_mat[0]) + np.sum(conf_mat[2])):0.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"Normal F1: {F1_ind(conf_mat, 0):0.3f}\")\n",
    "    print(f\"AF F1: {F1_ind(conf_mat, 1):0.3f}\")\n",
    "    print(f\"Other F1: {F1_ind(conf_mat, 2):0.3f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print_results(conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 7545   112 11494]\n",
      " [   56    34     0]\n",
      " [  305    48    38]]\n",
      "Sensitivity: 0.378\n",
      "Specificity: 0.992\n",
      "\n",
      "Normal F1: 0.558\n",
      "AF F1: 0.239\n",
      "Other F1: 0.006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print noise free conf mats\n",
    "print_results(noise_free_conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "misclassified_inds = feas1_ecg_data_test[feas1_ecg_data_test[\"prediction\"].map(np.argmax) != feas1_ecg_data_test[\"class_index\"]].index.values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "feas1_test_interesting = feas1_ecg_data_test[feas1_ecg_data_test[\"class_index\"] != 0]\n",
    "feas1_test_interesting_dataloader = get_dataloaders(feas1_test_interesting, 64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.special import softmax\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=250)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "\n",
    "labels = [\"Normal\", \"AF\", \"Other\"]\n",
    "\n",
    "for i in range(3):\n",
    "    p, r, d = precision_recall_curve((feas1_ecg_data_val_clean[\"class_index\"] == i),  feas1_ecg_data_val_clean[\"prediction\"].map(lambda x: softmax(x)[i]))\n",
    "    ax.plot(r, p, label=labels[i])\n",
    "    # plt.xlim((0, 1.1))\n",
    "    # plt.ylim((0, 1.1))\n",
    "\n",
    "    # closest_point_to_0_final = np.argmin(np.abs(d))\n",
    "    # ax.plot(r[closest_point_to_0_final], p[closest_point_to_0_final], \"o\", color=\"#ff7f0e\", label=r\"$p(AF) = 0.5$\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# fig.savefig(\"FinalReportFigs/CNN_NoiseDetect_precision_recall.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "importlib.reload(Utilities.Plotting)\n",
    "from Utilities.Plotting import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74351\n",
      "measDiag                                 DiagEnum.AF\n",
      "prediction     [-0.28620133, -0.32692084, 0.7239914]\n",
      "class_index                                        1\n",
      "Name: 74351, dtype: object\n",
      "74431\n",
      "measDiag                                DiagEnum.AF\n",
      "prediction     [-0.7460612, 0.22486098, 0.49314404]\n",
      "class_index                                       1\n",
      "Name: 74431, dtype: object\n",
      "74435\n",
      "measDiag                                 DiagEnum.AF\n",
      "prediction     [-0.017127324, -0.4723771, 0.8629926]\n",
      "class_index                                        1\n",
      "Name: 74435, dtype: object\n",
      "74385\n",
      "measDiag                                 DiagEnum.AF\n",
      "prediction     [-0.76330775, 0.16884518, 0.48620653]\n",
      "class_index                                        1\n",
      "Name: 74385, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [210], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m plot_ecg(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m300\u001B[39m, n_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, r_peaks\u001B[38;5;241m=\u001B[39mecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr_peaks\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     16\u001B[0m plot_ecg_spectrogram(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m300\u001B[39m, n_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, cut_range\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m18\u001B[39m], figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m2.5\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 17\u001B[0m \u001B[43mplot_ecg_drr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mecg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrri_feature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mecg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrri_len\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_quality\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\Utilities\\Plotting.py:57\u001B[0m, in \u001B[0;36mplot_ecg_drr\u001B[1;34m(rri, rri_len, figsize, export_quality)\u001B[0m\n\u001B[0;32m     54\u001B[0m ax\u001B[38;5;241m.\u001B[39mspines[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mset_visible(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     56\u001B[0m fig\u001B[38;5;241m.\u001B[39mtight_layout()\n\u001B[1;32m---> 57\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "dataset = feas1_ecg_data_val_clean\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(lambda x: np.argmax(x))\n",
    "#  & (dataset[\"noise_probs\"] < 0)\n",
    "\n",
    "selection = dataset[(dataset[\"class_prediction\"] == 2) & (dataset[\"class_index\"] == 1)]\n",
    "\n",
    "\n",
    "for ecg_ind, ecg in selection.sample(frac=1).iterrows():\n",
    "    print(ecg_ind)\n",
    "    print(ecg[[\"measDiag\", \"prediction\", \"class_index\"]])\n",
    "    # filtered_ecg = scipy.signal.sosfiltfilt(sos, ecg[\"data\"], padlen=150)\n",
    "\n",
    "    plot_ecg(ecg[\"data\"], 300, n_split=3, r_peaks=ecg[\"r_peaks\"])\n",
    "    plot_ecg_spectrogram(ecg[\"data\"], 300, n_split=3, cut_range=[2, 18], figsize=(6, 2.5), export_quality=True)\n",
    "    plot_ecg_drr(ecg[\"rri_feature\"], ecg[\"rri_len\"], export_quality=True)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "conf_mat_initial_transformer = np.array([[[ 717,   41,  226],\n",
    " [  88,  686,  129],\n",
    " [ 456,  184, 1407]], # CinC2020\n",
    "[[   0, 2237, 1457],\n",
    " [   0,  437,   67],\n",
    " [   0, 1094,  555]],  # CinC2017\n",
    "[[7384, 2281, 9848],\n",
    " [   0,    8,    8],\n",
    " [ 249,  151,  357]],  # Safer Feas2\n",
    "[[ 8502,  2400, 11362],\n",
    " [    2,    50,    67],\n",
    " [  223,    75,   256]]])  # safer feas1\n",
    "\n",
    "conf_mat_fine_tuned = np.array([[[  0, 320, 419],\n",
    " [  0,  91,  10],\n",
    " [  0, 171, 159]],  # CinC 2017\n",
    "[[14918,  1543,  3052],\n",
    " [    3,     7,     6],\n",
    " [  562,    79,   116]],  # SAFER feas 2\n",
    "[[17558,  2061,  2829],\n",
    " [    7,    92,    34],\n",
    " [  343,    60,   159]]])  # SAFER feas 1\n",
    "\n",
    "\n",
    "conf_mat_final = np.array([[[20413,   376,  1475],\n",
    " [   13,    82,    38],\n",
    " [  203,    73,   282]],  # SAFER feas1\n",
    "[[19113,    19,   381],\n",
    " [    7,     5,     4],\n",
    " [  428,    67,   262]]]) # SAFER feas2\n",
    "\n",
    "\n",
    "for c in conf_mat_final:\n",
    "    plot_confusion_matrix_2(c, [\"Normal\", \"AF\", \"Other Rhythm\"], colour=\"Blues\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_79224\\2077217174.py:6: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101033\n",
      "measDiag                            DiagEnum.NoAF\n",
      "prediction     [-2.428257, 0.87048155, 0.7332832]\n",
      "class_index                                     2\n",
      "Name: 101033, dtype: object\n",
      "Plotting attention\n",
      "54862\n",
      "measDiag                             DiagEnum.NoAF\n",
      "prediction     [-2.3245063, 0.9324973, 0.23180105]\n",
      "class_index                                      2\n",
      "Name: 54862, dtype: object\n",
      "Plotting attention\n",
      "15492\n",
      "measDiag                             DiagEnum.NoAF\n",
      "prediction     [-2.7731287, 1.894449, -0.38886702]\n",
      "class_index                                      2\n",
      "Name: 15492, dtype: object\n",
      "Plotting attention\n",
      "54477\n",
      "measDiag                              DiagEnum.NoAF\n",
      "prediction     [-3.2570727, 2.3353019, -0.08587719]\n",
      "class_index                                       2\n",
      "Name: 54477, dtype: object\n",
      "Plotting attention\n",
      "1445\n",
      "measDiag                               DiagEnum.NoAF\n",
      "prediction     [-0.45606652, 0.4858007, -0.10426653]\n",
      "class_index                                        2\n",
      "Name: 1445, dtype: object\n",
      "Plotting attention\n",
      "54951\n",
      "measDiag                              DiagEnum.NoAF\n",
      "prediction     [-1.0993279, 0.39522126, 0.24903332]\n",
      "class_index                                       2\n",
      "Name: 54951, dtype: object\n",
      "Plotting attention\n",
      "54893\n",
      "measDiag                           DiagEnum.NoAF\n",
      "prediction     [-1.79672, 0.9511066, 0.06217341]\n",
      "class_index                                    2\n",
      "Name: 54893, dtype: object\n",
      "Plotting attention\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [228], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m plot_ecg_with_attention(ecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m3000\u001B[39m], \u001B[38;5;241m300\u001B[39m, n_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, attention\u001B[38;5;241m=\u001B[39mecg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][:, :\u001B[38;5;241m96\u001B[39m], figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m4\u001B[39m), export_quality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# plot_ecg_spectrogram(ecg[\"data\"], 300, n_split=3, cut_range=[2, 18])\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# plot_ecg_poincare(ecg[\"rri_feature\"], ecg[\"rri_len\"])\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\pyplot.py:409\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03mDisplay all open figures.\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;124;03mexplicitly there.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    408\u001B[0m _warn_if_gui_out_of_main_thread()\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_backend_mod()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backend_bases.py:3546\u001B[0m, in \u001B[0;36m_Backend.show\u001B[1;34m(cls, block)\u001B[0m\n\u001B[0;32m   3544\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m ipython_pylab \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive()\n\u001B[0;32m   3545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m-> 3546\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:1032\u001B[0m, in \u001B[0;36m_BackendTk.mainloop\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1030\u001B[0m manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1032\u001B[0m     \u001B[43mfirst_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1034\u001B[0m     manager_class\u001B[38;5;241m.\u001B[39m_owns_mainloop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001B[0m, in \u001B[0;36mMisc.mainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmainloop\u001B[39m(\u001B[38;5;28mself\u001B[39m, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1458\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Plot with attention maps\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "dataset = feas1_test_interesting\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(lambda x: np.argmax(x))\n",
    "#  & (dataset[\"noise_probs\"] < 0)\n",
    "\n",
    "selection = dataset[(dataset[\"class_prediction\"] == 1) & (dataset[\"class_index\"] == 2)]\n",
    "\n",
    "\n",
    "\n",
    "for ecg_ind, ecg in selection.dropna(subset=[\"attention\"]).sample(frac=1).iterrows():\n",
    "    print(ecg_ind)\n",
    "    print(ecg[[\"measDiag\", \"prediction\", \"class_index\"]])\n",
    "    # filtered_ecg = scipy.signal.sosfiltfilt(sos, ecg[\"data\"], padlen=150)\n",
    "    plot_ecg_with_attention(ecg[\"data\"][:3000], 300, n_split=1, attention=ecg[\"attention\"][0][:, :96], figsize=(6, 4), export_quality=True)\n",
    "    # plot_ecg_spectrogram(ecg[\"data\"], 300, n_split=3, cut_range=[2, 18])\n",
    "    # plot_ecg_poincare(ecg[\"rri_feature\"], ecg[\"rri_len\"])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    225\n",
      "True      94\n",
      "Name: pred_af, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_50484\\1509770050.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"class_prediction\"] = dataset[\"prediction\"].map(np.argmax)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_50484\\1509770050.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"pred_af\"] = dataset[\"class_prediction\"] == 1\n"
     ]
    }
   ],
   "source": [
    "# Compute patient wise performance\n",
    "\n",
    "dataset = feas1_ecg_data_test_clean\n",
    "\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(np.argmax)\n",
    "dataset[\"pred_af\"] = dataset[\"class_prediction\"] == 1\n",
    "pt_diagnoses = dataset.groupby(\"ptID\")[\"pred_af\"].any()\n",
    "print(pt_diagnoses.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "4    2027\n2      65\n3      38\n5       9\n1       2\nName: ptDiag, dtype: int64"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_data[\"ptDiag\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "feas1_pt_data = pt_data[pt_data[\"ptDiag\"].isin([4, 2, 1])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_50484\\3214977234.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  feas1_pt_data.loc[:, \"pt_prediction_af\"] = pt_diagnoses\n"
     ]
    }
   ],
   "source": [
    "feas1_pt_data.loc[:, \"pt_prediction_af\"] = pt_diagnoses\n",
    "\n",
    "val_patients = feas1_pt_data.dropna(subset=[\"pt_prediction_af\"])\n",
    "pt_conf_mat = confusion_matrix((val_patients[\"ptDiag\"] == 2).values, val_patients[\"pt_prediction_af\"].astype(bool))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[219  80]\n",
      " [  2  10]]\n",
      "Sensitivity: 0.833\n",
      "Specificity: 0.732\n",
      "\n",
      "Normal F1: 0.842\n",
      "AF F1: 0.196\n"
     ]
    }
   ],
   "source": [
    "def print_binary_results(conf_mat):\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(conf_mat)\n",
    "\n",
    "    print(f\"Sensitivity: {conf_mat[1, 1]/np.sum(conf_mat[1]):0.3f}\")\n",
    "    print(f\"Specificity: {(conf_mat[0, 0])/np.sum(conf_mat[0]):0.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(f\"Normal F1: {F1_ind(conf_mat, 0):0.3f}\")\n",
    "    print(f\"AF F1: {F1_ind(conf_mat, 1):0.3f}\")\n",
    "\n",
    "print_binary_results(pt_conf_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "1330"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the worst case how many ECGs would be reviewed\n",
    "\n",
    "val_patients[val_patients[\"pt_prediction_af\"] == 1][\"noHQrecs\"].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspect the attention mechanism (not very useful yet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "# model.transformer_encoder.layers.\n",
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "def patch_attention(m):\n",
    "    forward_orig = m.forward\n",
    "\n",
    "    def wrap(*args, **kwargs):\n",
    "        kwargs['need_weights'] = True\n",
    "        kwargs['average_attn_weights'] = False\n",
    "\n",
    "        return forward_orig(*args, **kwargs)\n",
    "\n",
    "    m.forward = wrap\n",
    "\n",
    "attentions = []\n",
    "inds = []\n",
    "\n",
    "def save_outputs(module, x, y):\n",
    "    for att in y[1]:\n",
    "        attentions.append(att.cpu().numpy())\n",
    "\n",
    "patch_attention(model.transformer_encoder.layers[0].self_attn)\n",
    "attention_hook = model.transformer_encoder.layers[0].self_attn.register_forward_hook(save_outputs)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels, ind) in enumerate(feas1_test_interesting_dataloader):\n",
    "        signal = signals[0].to(device).float()\n",
    "        rris = signals[1].to(device).float()\n",
    "        rri_len = signals[2].to(device).float()\n",
    "\n",
    "        labels = labels.long().detach().numpy()\n",
    "        for i in ind:\n",
    "            if isinstance(i, str):\n",
    "                inds.append(i)\n",
    "            else:\n",
    "                inds.append(i.item())\n",
    "\n",
    "        output = model(signal, rris, rri_len) # rris).detach().to(\"cpu\").numpy()\n",
    "        # plot_ecg(signal[0].cpu().numpy())\n",
    "        # plt.show()\n",
    "\n",
    "attention_hook.remove()\n",
    "# attentions = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "attentions = attentions[::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_79224\\3165289508.py:1: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feas1_test_interesting[\"attention\"] = pd.Series(data=attentions, index=inds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "attention_hook.remove()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect the attention pooling weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 9120])\n",
      "hook\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [170], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# fft = torch.abs(torch.fft.fft(signals))\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# signals = torch.cat([signals, fft], dim=1)\u001B[39;00m\n\u001B[0;32m     20\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mlong()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m---> 21\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msignals\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(attentions\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [140], line 114\u001B[0m, in \u001B[0;36mTransformerModel.forward\u001B[1;34m(self, src)\u001B[0m\n\u001B[0;32m    111\u001B[0m output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtranspose(output, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# output = torch.flatten(output, 1)\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_pooling\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder1(output)\n\u001B[0;32m    117\u001B[0m output \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(output)\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [139], line 58\u001B[0m, in \u001B[0;36mLearnedAggregation.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x:Tensor):\n\u001B[1;32m---> 58\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_q \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma_1 \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls_q\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma_2 \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mffn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x))\n",
      "File \u001B[1;32m~\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1215\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[0;32m   1214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m-> 1215\u001B[0m         hook_result \u001B[38;5;241m=\u001B[39m \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1216\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m hook_result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1217\u001B[0m             result \u001B[38;5;241m=\u001B[39m hook_result\n",
      "Cell \u001B[1;32mIn [166], line 10\u001B[0m, in \u001B[0;36mhook\u001B[1;34m(module, x, y)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m attentions\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhook\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mattentions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m(y[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# model.transformer_encoder.layers.\n",
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "attentions = None\n",
    "\n",
    "def hook(module, x, y):\n",
    "    global attentions\n",
    "    print(\"hook\")\n",
    "    attentions = y[1].detach().to(\"cpu\").numpy()\n",
    "\n",
    "attention_hook = model.attention_pooling.attn.register_forward_hook(hook)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (signals, labels, ind) in enumerate(test_dataloader_safer):\n",
    "        print(signals.shape)\n",
    "        signals = torch.transpose(signals.to(device), 0, 1).float()\n",
    "        # fft = torch.abs(torch.fft.fft(signals))\n",
    "        # signals = torch.cat([signals, fft], dim=1)\n",
    "        labels = labels.long().detach().numpy()\n",
    "        output = model(signals).detach().to(\"cpu\").numpy()\n",
    "\n",
    "        if labels[0] == 0:\n",
    "            print(attentions.shape)\n",
    "            fig = make_subplots(2, 1)\n",
    "            fig.add_trace(go.Scatter(y=signals[:, 0].detach().to(\"cpu\").numpy()), row=1, col=1)\n",
    "            for j in range(attentions.shape[-2]):\n",
    "                fig.add_trace(go.Scatter(y=attentions[0, j, :]), row=2, col=1)\n",
    "            fig.show()\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "attention_hook.remove()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect the final classification layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "fc1_weight = model.decoder1.weight.data\n",
    "fc2_weight = model.decoder2.weight.data\n",
    "\n",
    "plt.imshow(fc1_weight.cpu().numpy())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "dataset = test_dataset_safer\n",
    "dataset[\"class_prediction\"] = dataset[\"prediction\"].map(lambda x: np.argmax(x))\n",
    "selection = dataset[(dataset[\"class_prediction\"] == 1) & (dataset[\"class_index\"] == 1) & (dataset[\"noise_probs\"]< 0)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "np_signal = np.vstack(selection[\"data\"].values)\n",
    "np_rri = np.vstack(selection[\"rri_feature\"].values)\n",
    "rri_len = selection[\"rri_len\"].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook\n",
      "torch.Size([17, 192])\n",
      "torch.Size([17, 128, 1])\n",
      "torch.Size([17, 128, 1])\n",
      "torch.Size([17, 3, 1])\n",
      "torch.Size([17, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "signal = torch.tensor(np_signal, dtype=torch.float, device=device)\n",
    "rri = torch.tensor(np_rri, dtype=torch.float, device=device)\n",
    "rri_lens = torch.tensor(rri_len, device=device)\n",
    "\n",
    "encoder_out = None\n",
    "\n",
    "def get_encoding(module, x, y):\n",
    "    global encoder_out\n",
    "    print(\"hook\")\n",
    "    print(x[0].shape)\n",
    "    encoder_out = x[0].detach().to(\"cpu\")\n",
    "\n",
    "encoding_hook = model.decoder1.register_forward_hook(get_encoding)\n",
    "\n",
    "output = model(signal, rri, rri_lens)\n",
    "\n",
    "encoding_hook.remove()\n",
    "\n",
    "# Now recreate the output from just the RRI or ECG and see which makes the biggest impact\n",
    "fc1_weight = model.decoder1.weight.data.to(\"cpu\")\n",
    "fc2_weight = model.decoder2.weight.data.to(\"cpu\")\n",
    "\n",
    "\n",
    "ecg_out = fc1_weight[:, :128] @ torch.unsqueeze(encoder_out[:, :128], dim=-1)\n",
    "rri_out = fc1_weight[:, 128:] @ torch.unsqueeze(encoder_out[:, 128:], dim=-1)\n",
    "\n",
    "print(ecg_out.shape)\n",
    "print(ecg_out.shape)\n",
    "\n",
    "ecg_out = nn.functional.relu(ecg_out)\n",
    "rri_out = nn.functional.relu(rri_out)\n",
    "\n",
    "ecg_out = fc2_weight @ ecg_out\n",
    "rri_out = fc2_weight @ rri_out\n",
    "\n",
    "print(ecg_out.shape)\n",
    "print(ecg_out.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"ECG Signal Outputs\")\n",
    "plt.imshow(ecg_out)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"RRI Sequence Outputs\")\n",
    "plt.imshow(rri_out)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TSNE the encoder outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4078, 2)\n"
     ]
    }
   ],
   "source": [
    "encoder_out = []\n",
    "class_indexes = []\n",
    "inds = []\n",
    "\n",
    "def get_encoding(module, x, y):\n",
    "    encoder_out.append(x[0].detach().to(\"cpu\").numpy())\n",
    "\n",
    "encoding_hook = model.decoder1.register_forward_hook(get_encoding)\n",
    "\n",
    "dataloader = test_dataloader_safer\n",
    "\n",
    "with torch.no_grad():\n",
    "        for i, (signals, labels, ind) in enumerate(dataloader):\n",
    "            signal = signals[0].to(device).float()\n",
    "            rris = signals[1].to(device).float()\n",
    "            rri_len = signals[2].to(device).float()\n",
    "\n",
    "            labels = labels.long().detach().numpy()\n",
    "            class_indexes.append(labels)\n",
    "\n",
    "            output = model(signal, rris, rri_len).detach().to(\"cpu\").numpy()\n",
    "            inds.append(ind.cpu().detach().numpy())\n",
    "\n",
    "encoding_hook.remove()\n",
    "\n",
    "encoder_out = np.concatenate(encoder_out, axis=0)\n",
    "class_indexes = np.concatenate(class_indexes, axis=0)\n",
    "inds = np.concatenate(inds, axis=0)\n",
    "\n",
    "test_dataset_safer[\"encodings\"] = pd.Series(data=[encoder_out[i] for i in range(encoder_out.shape[0])], index=inds)\n",
    "\n",
    "tsne = TSNE(perplexity=30)\n",
    "embeddings = tsne.fit_transform(encoder_out)\n",
    "\n",
    "print(embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\daniel\\Documents\\CambridgeSoftwareProjects\\ecg-signal-quality\\projectEnv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(perplexity=10)\n",
    "embeddings = tsne.fit_transform(encoder_out)\n",
    "\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c=class_indexes, marker=\"x\")\n",
    "plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
